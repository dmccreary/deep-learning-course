
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="An example of an intelligent textbook for teaching deep learning. All content licensed Creative Commons ShareAlike Attribution Noncommercial">
      
      
        <meta name="author" content="Dan McCreary">
      
      
        <link rel="canonical" href="https://dmccreary.github.io/deep-learning-course/prompts/07-equations/">
      
      
        <link rel="prev" href="../06-glossary-of-terms/">
      
      
        <link rel="next" href="../../sims/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.45">
    
    
      
        <title>Equations - Deep Learning Course with AI</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#generating-commonly-used-deep-learning-equations" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Deep Learning Course with AI" class="md-header__button md-logo" aria-label="Deep Learning Course with AI" data-md-component="logo">
      
  <img src="../../img/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Deep Learning Course with AI
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Equations
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/dmccreary/deep-learning-course" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub Repo
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Deep Learning Course with AI" class="md-nav__button md-logo" aria-label="Deep Learning Course with AI" data-md-component="logo">
      
  <img src="../../img/logo.png" alt="logo">

    </a>
    Deep Learning Course with AI
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/dmccreary/deep-learning-course" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub Repo
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course-description/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Course Description
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Prompts
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Prompts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-bloom-taxonomy-description/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Course Description
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-concept-enumeration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Concept Enumeration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-concept-dependencies/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Concept Dependencies
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-concept-taxonomy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Concept Taxonomy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-glossary-of-terms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Glossary of Terms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Equations
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Equations
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slope-and-intercept" class="md-nav__link">
    <span class="md-ellipsis">
      Slope and Intercept
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#area-of-a-circle" class="md-nav__link">
    <span class="md-ellipsis">
      Area of a Circle
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#response" class="md-nav__link">
    <span class="md-ellipsis">
      Response
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../sims/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    MicroSims
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../checklist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Checklist
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code-highlight-test.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code Highlight Test
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../glossary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Glossary
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../how-we-built-this-site/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How We Built This Site
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../feedback/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Feedback
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../license/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    License
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    References
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contact/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contact
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slope-and-intercept" class="md-nav__link">
    <span class="md-ellipsis">
      Slope and Intercept
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#area-of-a-circle" class="md-nav__link">
    <span class="md-ellipsis">
      Area of a Circle
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#response" class="md-nav__link">
    <span class="md-ellipsis">
      Response
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/dmccreary/deep-learning-course/blob/master/docs/prompts/07-equations.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  


<h1 id="generating-commonly-used-deep-learning-equations">Generating Commonly Used Deep Learning Equations</h1>
<div class="admonition prompt">
<p class="admonition-title">Prompt</p>
<p>Create a list of the 100 most commonly used
equations in a college level course in deep learning.</p>
<p>For each equation, return the following:</p>
<ol>
<li>The name of the equation in markdown level 4.</li>
<li>Within the body, return the equation in LaTeX format</li>
<li>Surround the equation with dollar signs</li>
<li>For each variable in the equation, add a "where:" that defines each variable</li>
<li>Put blank lines around the list of variables</li>
<li>Wrap the variables names in dollar signs</li>
</ol>
<p>Examples:</p>
<h4 id="slope-and-intercept">Slope and Intercept</h4>
<p>$y = m * x + b$</p>
<p>where:</p>
<ul>
<li>$y$ is the vertical axis</li>
<li>$x$ is the horizontal axis</li>
<li>$m$ is the slope</li>
<li>$b$ is the intercept</li>
</ul>
<h4 id="area-of-a-circle">Area of a Circle</h4>
<p>$A = 2\pi r^2$
where:</p>
<ul>
<li>$A$ is the area of the circle</li>
<li>$r$ is the radius</li>
</ul>
</div>
<h2 id="response">Response</h2>
<p>Below are 100 commonly encountered equations in a college-level deep learning course, presented in alphabetical order. Each entry has:</p>
<ol>
<li>A heading in <strong>Markdown level 4</strong>.</li>
<li>The equation in <strong>LaTeX</strong>, surrounded by dollar signs.</li>
<li>A <strong>where:</strong> section defining each variable, with each variable in dollar signs and blank lines around the list.</li>
</ol>
<h1 id="100-common-equations-in-deep-learning">100 Common Equations in Deep Learning</h1>
<p>Below is a curated list of 100 frequently encountered equations in a college-level Deep Learning course. Each entry includes:</p>
<ul>
<li><strong>A name</strong> (in level 4 markdown).</li>
<li><strong>The equation</strong> (in LaTeX, surrounded by single dollar signs).</li>
<li>A <strong>“where:”</strong> section explaining each variable.</li>
</ul>
<p>The equations are grouped loosely by theme (basic algebra, loss functions, optimization, activations, convolution, RNNs, attention mechanisms, etc.) but may appear in any Deep Learning curriculum.</p>
<hr />
<h4 id="1-mean-squared-error-mse">1. Mean Squared Error (MSE)</h4>
<p>$L_{\text{MSE}} = \frac{1}{N}\sum_{i=1}^{N}(y^{(i)} - \hat{y}^{(i)})^2$</p>
<p>where:</p>
<ul>
<li>$N$ is the number of samples</li>
<li>$y^{(i)}$ is the true label for the (i)-th sample</li>
<li>$\hat{y}^{(i)}$ is the predicted value for the (i)-th sample</li>
</ul>
<hr />
<h4 id="2-cross-entropy-loss-binary">2. Cross-Entropy Loss (Binary)</h4>
<p>$L_{\text{CE}} = -\frac{1}{N}\sum_{i=1}^{N}\Big[y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)})\Big]$</p>
<p>where:</p>
<ul>
<li>$N$ is the number of samples</li>
<li>$y^{(i)} \in {0, 1}$ is the true binary label</li>
<li>$\hat{y}^{(i)}$ is the predicted probability of the positive class</li>
</ul>
<hr />
<h4 id="3-cross-entropy-loss-multi-class">3. Cross-Entropy Loss (Multi-Class)</h4>
<p>$L_{\text{CE}} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{c}^{(i)} \log(\hat{y}_{c}^{(i)})$</p>
<p>where:</p>
<ul>
<li>$N$ is the number of samples</li>
<li>$C$ is the number of classes</li>
<li>$y_{c}^{(i)} \in {0,1}$ indicates the true class of sample (i)</li>
<li>$\hat{y}_{c}^{(i)}$ is the predicted probability for class (c)</li>
</ul>
<hr />
<h4 id="4-softmax-function">4. Softmax Function</h4>
<p>$\hat{y}<em k="1">j = \frac{\exp(z_j)}{\sum</em>$}^{C}\exp(z_k)</p>
<p>where:</p>
<ul>
<li>$z_j$ is the logit for class (j)</li>
<li>$C$ is the total number of classes</li>
<li>$\hat{y}_j$ is the probability assigned to class (j)</li>
</ul>
<hr />
<h4 id="5-sigmoid-logistic-function">5. Sigmoid (Logistic) Function</h4>
<p>$\sigma(z) = \frac{1}{1 + e^{-z}}$</p>
<p>where:</p>
<ul>
<li>$z$ is the input or logit</li>
<li>$\sigma(z)$ is the output between 0 and 1</li>
</ul>
<hr />
<h4 id="6-tanh-function">6. Tanh Function</h4>
<p>$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$</p>
<p>where:</p>
<ul>
<li>$z$ is the input</li>
<li>$\tanh(z)$ outputs values in the range ([-1, 1])</li>
</ul>
<hr />
<h4 id="7-rectified-linear-unit-relu">7. Rectified Linear Unit (ReLU)</h4>
<p>$\text{ReLU}(z) = \max(0, z)$</p>
<p>where:</p>
<ul>
<li>$z$ is the input</li>
<li>$\text{ReLU}(z)$ is zero for negative (z) and (z) itself if (z &gt; 0)</li>
</ul>
<hr />
<h4 id="8-leaky-relu">8. Leaky ReLU</h4>
<p>$\text{LeakyReLU}(z) = 
\begin{cases}
z &amp; \text{if } z \ge 0\
\alpha \, z &amp; \text{if } z &lt; 0
\end{cases}$</p>
<p>where:</p>
<ul>
<li>$z$ is the input</li>
<li>$\alpha$ is a small positive slope (e.g., 0.01) for negative (z)</li>
</ul>
<hr />
<h4 id="9-weighted-sum-of-inputs">9. Weighted Sum of Inputs</h4>
<p>$z = \sum_{i=1}^{d} w_i x_i + b$</p>
<p>where:</p>
<ul>
<li>$d$ is the number of input features</li>
<li>$w_i$ is the weight for feature (x_i)</li>
<li>$x_i$ is the (i)-th input</li>
<li>$b$ is the bias term</li>
</ul>
<hr />
<h4 id="10-neural-network-output-one-layer">10. Neural Network Output (One Layer)</h4>
<p>$\hat{y} = f\bigg(\sum_{i=1}^{d} w_i x_i + b\bigg)$</p>
<p>where:</p>
<ul>
<li>$f(\cdot)$ is an activation function</li>
<li>$w_i$, $x_i$, and $b$ are defined as above</li>
</ul>
<hr />
<h4 id="11-gradient-descent-parameter-update">11. Gradient Descent (Parameter Update)</h4>
<p>$\theta \leftarrow \theta - \eta \,\nabla_\theta L(\theta)$</p>
<p>where:</p>
<ul>
<li>$\theta$ represents parameters (weights, biases)</li>
<li>$\eta$ is the learning rate</li>
<li>$L(\theta)$ is the loss function</li>
<li>$\nabla_\theta L(\theta)$ is the gradient of the loss w.r.t. parameters</li>
</ul>
<hr />
<h4 id="12-chain-rule-single-variable">12. Chain Rule (Single-Variable)</h4>
<p>$\frac{d}{dx} f(g(x)) = f'(g(x)) \cdot g'(x)$</p>
<p>where:</p>
<ul>
<li>$f$ and $g$ are functions</li>
<li>$f'$ and $g'$ are their derivatives</li>
</ul>
<hr />
<h4 id="13-chain-rule-multivariable">13. Chain Rule (Multivariable)</h4>
<p>$\frac{\partial}{\partial x_i} L = \sum_{j} \frac{\partial L}{\partial z_j} \frac{\partial z_j}{\partial x_i}$</p>
<p>where:</p>
<ul>
<li>$L$ is the loss function</li>
<li>$z_j$ are intermediate variables dependent on $x_i$</li>
</ul>
<hr />
<h4 id="14-momentum-update">14. Momentum Update</h4>
<p>[
\begin{aligned}
v &amp;\leftarrow \beta v + (1-\beta)\nabla_\theta L(\theta), \
\theta &amp;\leftarrow \theta - \eta\, v
\end{aligned}
]</p>
<p>where:</p>
<ul>
<li>$v$ is the velocity term</li>
<li>$\beta$ is the momentum hyperparameter (e.g., 0.9)</li>
<li>$\eta$ is the learning rate</li>
<li>$\nabla_\theta L(\theta)$ is the gradient</li>
</ul>
<hr />
<h4 id="15-nesterov-accelerated-gradient-nag">15. Nesterov Accelerated Gradient (NAG)</h4>
<p>[
\begin{aligned}
v &amp;\leftarrow \beta v + (1-\beta)\nabla_\theta L(\theta - \beta v),\
\theta &amp;\leftarrow \theta - \eta\, v
\end{aligned}
]</p>
<p>where:</p>
<ul>
<li>$v$ is the velocity term</li>
<li>$\beta$ is the momentum factor</li>
<li>$\theta$ are the parameters</li>
<li>$\eta$ is the learning rate</li>
</ul>
<hr />
<h4 id="16-rmsprop-update">16. RMSProp Update</h4>
<p>[
\begin{aligned}
E[g^2] &amp;\leftarrow \rho\, E[g^2] + (1-\rho) \, (\nabla_\theta L(\theta))^2, \
\theta &amp;\leftarrow \theta - \frac{\eta}{\sqrt{E[g^2] + \epsilon}} \nabla_\theta L(\theta)
\end{aligned}
]</p>
<p>where:</p>
<ul>
<li>$E[g^2]$ is the running average of squared gradients</li>
<li>$\rho$ is the decay rate (e.g., 0.9)</li>
<li>$\eta$ is the learning rate</li>
<li>$\epsilon$ is a small constant for numerical stability</li>
</ul>
<hr />
<h4 id="17-adam-optimizer">17. Adam Optimizer</h4>
<p>[
\begin{aligned}
m &amp;\leftarrow \beta_1 m + (1 - \beta_1)\nabla_\theta L(\theta), \
v &amp;\leftarrow \beta_2 v + (1 - \beta_2)(\nabla_\theta L(\theta))^2, \
\hat{m} &amp;\leftarrow \frac{m}{1 - \beta_1^t}, \quad \hat{v} \leftarrow \frac{v}{1 - \beta_2^t}, \
\theta &amp;\leftarrow \theta - \eta \frac{\hat{m}}{\sqrt{\hat{v}} + \epsilon}
\end{aligned}
]</p>
<p>where:</p>
<ul>
<li>$m$ is the first moment estimate</li>
<li>$v$ is the second moment estimate</li>
<li>$\beta_1$ and $\beta_2$ are exponential decay rates</li>
<li>$\eta$ is the learning rate</li>
<li>$t$ is the current time step</li>
<li>$\epsilon$ is a small constant</li>
</ul>
<hr />
<h4 id="18-l1-regularization">18. L1 Regularization</h4>
<p>$R_{L1} = \lambda \sum_{j} |w_j|$</p>
<p>where:</p>
<ul>
<li>$\lambda$ is the regularization coefficient</li>
<li>$w_j$ are the weights</li>
</ul>
<hr />
<h4 id="19-l2-regularization">19. L2 Regularization</h4>
<p>$R_{L2} = \frac{\lambda}{2} \sum_{j} w_j^2$</p>
<p>where:</p>
<ul>
<li>$\lambda$ is the regularization strength</li>
<li>$w_j$ are the model’s weights</li>
<li>The factor $\tfrac{1}{2}$ is often included by convention</li>
</ul>
<hr />
<h4 id="20-weighted-cross-entropy-class-imbalance">20. Weighted Cross-Entropy (Class Imbalance)</h4>
<p>$L_{\text{WCE}} = -\frac{1}{N} \sum_{i=1}^N \Big[\alpha \, y^{(i)} \log(\hat{y}^{(i)}) + (1-\alpha)\,(1-y^{(i)})\log(1-\hat{y}^{(i)})\Big]$</p>
<p>where:</p>
<ul>
<li>$\alpha$ is a weight factor for the positive class</li>
<li>$y^{(i)} \in {0,1}$ is the true label</li>
<li>$\hat{y}^{(i)}$ is the predicted probability</li>
</ul>
<hr />
<h4 id="21-negative-log-likelihood-nll">21. Negative Log-Likelihood (NLL)</h4>
<p>$L_{\text{NLL}} = -\sum_{i=1}^N \log \, p(y^{(i)} | x^{(i)})$</p>
<p>where:</p>
<ul>
<li>$p(y|x)$ is the predicted probability of label (y) given input (x)</li>
<li>$N$ is the number of samples</li>
</ul>
<hr />
<h4 id="22-normal-equation-linear-regression">22. Normal Equation (Linear Regression)</h4>
<p>$\theta = (X^\top X)^{-1} X^\top y$</p>
<p>where:</p>
<ul>
<li>$X$ is the design matrix</li>
<li>$y$ is the vector of targets</li>
<li>$\theta$ is the vector of parameters</li>
</ul>
<hr />
<h4 id="23-coefficient-of-determination-r2">23. Coefficient of Determination ($R^2$)</h4>
<p>$R^2 = 1 - \frac{\sum_{i=1}^N (y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^N (y^{(i)} - \bar{y})^2}$</p>
<p>where:</p>
<ul>
<li>$y^{(i)}$ is the actual value</li>
<li>$\hat{y}^{(i)}$ is the predicted value</li>
<li>$\bar{y}$ is the mean of $y^{(i)}$</li>
</ul>
<hr />
<h4 id="24-batch-normalization-mean">24. Batch Normalization (Mean)</h4>
<p>$\mu_B = \frac{1}{m}\sum_{i=1}^{m} x_i$</p>
<p>where:</p>
<ul>
<li>$m$ is the number of samples in a mini-batch</li>
<li>$x_i$ is the (i)-th activation in the batch</li>
</ul>
<hr />
<h4 id="25-batch-normalization-variance">25. Batch Normalization (Variance)</h4>
<p>$\sigma_B^2 = \frac{1}{m}\sum_{i=1}^{m} (x_i - \mu_B)^2$</p>
<p>where:</p>
<ul>
<li>$x_i$ is the (i)-th activation in the batch</li>
<li>$\mu_B$ is the batch mean</li>
<li>$\sigma_B^2$ is the batch variance</li>
</ul>
<hr />
<h4 id="26-batch-normalization-forward-pass">26. Batch Normalization (Forward Pass)</h4>
<p>$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y_i = \gamma \, \hat{x}_i + \beta$</p>
<p>where:</p>
<ul>
<li>$x_i$ is the input activation</li>
<li>$\mu_B$, $\sigma_B^2$ are batch statistics</li>
<li>$\epsilon$ is a small constant</li>
<li>$\gamma$, $\beta$ are learnable scale and shift parameters</li>
</ul>
<hr />
<h4 id="27-dropout-forward-pass">27. Dropout (Forward Pass)</h4>
<p>$\tilde{h}^{(l)} = \mathbf{m}^{(l)} \odot h^{(l)}$</p>
<p>where:</p>
<ul>
<li>$h^{(l)}$ is the activation vector at layer (l)</li>
<li>$\mathbf{m}^{(l)}$ is a random binary mask (each entry ~ Bernoulli(p))</li>
<li>$\odot$ denotes elementwise multiplication</li>
</ul>
<hr />
<h4 id="28-2d-convolution-without-stridepadding">28. 2D Convolution (Without Stride/Padding)</h4>
<p>$(I * K)(x,y) = \sum_{u=-r}^{r} \sum_{v=-s}^{s} I(x+u, y+v)\, K(u,v)$</p>
<p>where:</p>
<ul>
<li>$I$ is the input image</li>
<li>$K$ is the kernel</li>
<li>$r$, $s$ define kernel size offsets</li>
</ul>
<hr />
<h4 id="29-transposed-convolution">29. Transposed Convolution</h4>
<p>$\text{ConvTranspose}(z, K) = \sum_{u}\sum_{v} z(u,v)\, \text{Upsample}(K, \text{stride})$</p>
<p>where:</p>
<ul>
<li>$z(u,v)$ is an activation map</li>
<li>$K$ is the kernel</li>
<li>$\text{stride}$ is the factor by which the output is upsampled</li>
</ul>
<hr />
<h4 id="30-max-pooling-2d">30. Max Pooling (2D)</h4>
<p>$\text{MaxPool}(x,y) = \max_{(p,q)\in \,\mathcal{R}_{x,y}} I(p,q)$</p>
<p>where:</p>
<ul>
<li>$I$ is the input feature map</li>
<li>$\mathcal{R}_{x,y}$ is the receptive field around location $(x,y)$</li>
</ul>
<hr />
<h4 id="31-average-pooling-2d">31. Average Pooling (2D)</h4>
<p>$\text{AvgPool}(x,y) = \frac{1}{|\mathcal{R}<em _mathcal_R="\,\mathcal{R" _p_q_in="(p,q)\in">{x,y}|}\sum</em> I(p,q)$}_{x,y}</p>
<p>where:</p>
<ul>
<li>$\mathcal{R}_{x,y}$ is the region of pooling</li>
<li>$|\mathcal{R}_{x,y}|$ is the size of that region</li>
</ul>
<hr />
<h4 id="32-rnn-hidden-state-update">32. RNN Hidden State Update</h4>
<p>$h_t = f(W_{hh} \, h_{t-1} + W_{xh} \, x_t + b_h)$</p>
<p>where:</p>
<ul>
<li>$h_t$ is the hidden state at time (t)</li>
<li>$x_t$ is the input at time (t)</li>
<li>$W_{hh}, W_{xh}$ are weight matrices</li>
<li>$b_h$ is a bias vector</li>
<li>$f(\cdot)$ is an activation (e.g., tanh)</li>
</ul>
<hr />
<h4 id="33-lstm-input-gate">33. LSTM Input Gate</h4>
<p>$i_t = \sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)$</p>
<p>where:</p>
<ul>
<li>$i_t$ is the input gate</li>
<li>$x_t$ is the current input</li>
<li>$h_{t-1}$ is the previous hidden state</li>
<li>$W_{xi}, W_{hi}$ are weight matrices</li>
<li>$b_i$ is the bias</li>
<li>$\sigma$ is the sigmoid function</li>
</ul>
<hr />
<h4 id="34-lstm-forget-gate">34. LSTM Forget Gate</h4>
<p>$f_t = \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)$</p>
<p>where:</p>
<ul>
<li>$f_t$ is the forget gate</li>
<li>$W_{xf}, W_{hf}$ are weight matrices</li>
<li>$b_f$ is the bias</li>
</ul>
<hr />
<h4 id="35-lstm-output-gate">35. LSTM Output Gate</h4>
<p>$o_t = \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)$</p>
<p>where:</p>
<ul>
<li>$o_t$ is the output gate</li>
<li>$W_{xo}, W_{ho}$ are weight matrices</li>
<li>$b_o$ is the bias</li>
</ul>
<hr />
<h4 id="36-lstm-cell-state">36. LSTM Cell State</h4>
<p>$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}<em xc="xc">t, \quad \tilde{c}_t = \tanh(W</em> + b_c)$} x_t + W_{hc} h_{t-1</p>
<p>where:</p>
<ul>
<li>$c_t$ is the cell state</li>
<li>$f_t, i_t$ are forget and input gates</li>
<li>$\tilde{c}_t$ is the candidate cell state</li>
<li>$\odot$ is elementwise multiplication</li>
</ul>
<hr />
<h4 id="37-gru-update-gate">37. GRU Update Gate</h4>
<p>$z_t = \sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z)$</p>
<p>where:</p>
<ul>
<li>$z_t$ is the update gate</li>
<li>$x_t$ is the input at time (t)</li>
<li>$h_{t-1}$ is the previous hidden state</li>
<li>$W_{xz}, W_{hz}$ are weight matrices</li>
<li>$b_z$ is the bias</li>
</ul>
<hr />
<h4 id="38-gru-reset-gate">38. GRU Reset Gate</h4>
<p>$r_t = \sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r)$</p>
<p>where:</p>
<ul>
<li>$r_t$ is the reset gate</li>
<li>$W_{xr}, W_{hr}$ are weight matrices</li>
<li>$b_r$ is the bias</li>
</ul>
<hr />
<h4 id="39-gru-hidden-state">39. GRU Hidden State</h4>
<p>$h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h}<em xh="xh">t, \quad \tilde{h}_t = \tanh(W</em>))$} x_t + r_t \odot (W_{hh} h_{t-1</p>
<p>where:</p>
<ul>
<li>$h_t$ is the updated hidden state</li>
<li>$z_t, r_t$ are the update and reset gates</li>
<li>$\tilde{h}_t$ is the candidate hidden state</li>
</ul>
<hr />
<h4 id="40-attention-score-dot-product">40. Attention Score (Dot Product)</h4>
<p>$e_{t,s} = h_t^\top \, s_s$</p>
<p>where:</p>
<ul>
<li>$h_t$ is the query vector</li>
<li>$s_s$ is the key vector</li>
<li>$e_{t,s}$ is the scalar score measuring alignment</li>
</ul>
<hr />
<h4 id="41-attention-weights-softmax">41. Attention Weights (Softmax)</h4>
<p>$\alpha_{t,s} = \frac{\exp(e_{t,s})}{\sum_{k=1}^{S} \exp(e_{t,k})}$</p>
<p>where:</p>
<ul>
<li>$e_{t,s}$ is the attention score for query (t) and key (s)</li>
<li>$S$ is the total number of key vectors</li>
</ul>
<hr />
<h4 id="42-context-vector">42. Context Vector</h4>
<p>$c_t = \sum_{s=1}^{S} \alpha_{t,s}\, v_s$</p>
<p>where:</p>
<ul>
<li>$\alpha_{t,s}$ are attention weights</li>
<li>$v_s$ is the value vector associated with key (s)</li>
<li>$c_t$ is the resulting weighted sum</li>
</ul>
<hr />
<h4 id="43-scaled-dot-product-attention">43. Scaled Dot-Product Attention</h4>
<p>$\text{Attention}(Q,K,V) = \text{softmax}\bigg(\frac{QK^\top}{\sqrt{d_k}}\bigg)V$</p>
<p>where:</p>
<ul>
<li>$Q$ is the matrix of query vectors</li>
<li>$K$ is the matrix of key vectors</li>
<li>$V$ is the matrix of value vectors</li>
<li>$d_k$ is the dimensionality of keys/queries</li>
</ul>
<hr />
<h4 id="44-multi-head-attention">44. Multi-Head Attention</h4>
<p>$\text{MHA}(Q,K,V) = \big[\text{head}_1, \dots, \text{head}_h\big]W^O$</p>
<p>where:</p>
<ul>
<li>$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</li>
<li>$W^O$ is the output projection matrix</li>
<li>$h$ is the number of heads</li>
</ul>
<hr />
<h4 id="45-feed-forward-transformer">45. Feed-Forward (Transformer)</h4>
<p>$\text{FFN}(x) = \max(0, xW_1 + b_1) \, W_2 + b_2$</p>
<p>where:</p>
<ul>
<li>$x$ is the input</li>
<li>$W_1, b_1, W_2, b_2$ are learned parameters</li>
<li>$\max(0,\cdot)$ indicates ReLU (or another activation)</li>
</ul>
<hr />
<h4 id="46-positional-encoding-sinusoidal">46. Positional Encoding (Sinusoidal)</h4>
<p>$\text{PE}(pos, 2i) = \sin\big(pos / 10000^{2i/d_{\text{model}}}\big), \quad \text{PE}(pos, 2i+1) = \cos\big(pos / 10000^{2i/d_{\text{model}}}\big)$</p>
<p>where:</p>
<ul>
<li>$pos$ is the position index</li>
<li>$i$ is the dimension index</li>
<li>$d_{\text{model}}$ is the model dimensionality</li>
</ul>
<hr />
<h4 id="47-perceptron-rule">47. Perceptron Rule</h4>
<p>$\hat{y} = 
\begin{cases}
1 &amp; \text{if } w^\top x + b \ge 0\
0 &amp; \text{otherwise}
\end{cases}$</p>
<p>where:</p>
<ul>
<li>$w$ is the weight vector</li>
<li>$x$ is the input vector</li>
<li>$b$ is the bias term</li>
</ul>
<hr />
<h4 id="48-general-activation-forward">48. General Activation Forward</h4>
<p>$a = f(z)$</p>
<p>where:</p>
<ul>
<li>$z$ is the pre-activation (weighted sum)</li>
<li>$f(\cdot)$ is an activation function (e.g., sigmoid, ReLU)</li>
</ul>
<hr />
<h4 id="49-derivative-of-relu">49. Derivative of ReLU</h4>
<p>$\frac{d}{dz}\text{ReLU}(z) =
\begin{cases}
1 &amp; z &gt; 0\
0 &amp; z \le 0
\end{cases}$</p>
<p>where:</p>
<ul>
<li>$z$ is the input</li>
</ul>
<hr />
<h4 id="50-derivative-of-sigmoid">50. Derivative of Sigmoid</h4>
<p>$\sigma'(z) = \sigma(z)\big(1-\sigma(z)\big)$</p>
<p>where:</p>
<ul>
<li>$\sigma(z)$ is the sigmoid function</li>
</ul>
<hr />
<h4 id="51-derivative-of-tanh">51. Derivative of Tanh</h4>
<p>$\frac{d}{dz}\tanh(z) = 1 - \tanh^2(z)$</p>
<p>where:</p>
<ul>
<li>$\tanh(z)$ is the hyperbolic tangent function</li>
</ul>
<hr />
<h4 id="52-derivative-of-softmax-vector-form">52. Derivative of Softmax (Vector Form)</h4>
<p>$\frac{\partial \hat{y}<em jk="jk">j}{\partial z_k} = \hat{y}_j \big(\delta</em>_k\big)$} - \hat{y</p>
<p>where:</p>
<ul>
<li>$\hat{y}_j$ is the softmax output for class (j)</li>
<li>$z_k$ is the logit for class (k)</li>
<li>$\delta_{jk}$ is the Kronecker delta (1 if (j=k), 0 otherwise)</li>
</ul>
<hr />
<h4 id="53-derivative-of-cross-entropy-wrt-logits-softmax">53. Derivative of Cross-Entropy w.r.t. Logits (Softmax)</h4>
<p>$\frac{\partial L_{\text{CE}}}{\partial z_j} = \hat{y}_j - y_j$</p>
<p>where:</p>
<ul>
<li>$\hat{y}_j$ is the predicted probability (softmax output)</li>
<li>$y_j$ is the one-hot target</li>
</ul>
<hr />
<h4 id="54-l2-distance-euclidean">54. L2 Distance (Euclidean)</h4>
<p>$d_{\text{Euclidean}}(x,y) = \sqrt{\sum_{i=1}^{d}(x_i - y_i)^2}$</p>
<p>where:</p>
<ul>
<li>$x, y \in \mathbb{R}^d$ are vectors</li>
<li>$d$ is the number of dimensions</li>
</ul>
<hr />
<h4 id="55-manhattan-distance-l1">55. Manhattan Distance (L1)</h4>
<p>$d_{\text{Manhattan}}(x,y) = \sum_{i=1}^d |x_i - y_i|$</p>
<p>where:</p>
<ul>
<li>$x, y$ are vectors in (\mathbb{R}^d)</li>
<li>$|\,\cdot\,|$ is the absolute value</li>
</ul>
<hr />
<h4 id="56-cosine-similarity">56. Cosine Similarity</h4>
<p>$\text{cos_sim}(x, y) = \frac{x \cdot y}{|x| \, |y|}$</p>
<p>where:</p>
<ul>
<li>$x \cdot y$ is the dot product</li>
<li>$|x|$ is the norm of $x$</li>
</ul>
<hr />
<h4 id="57-dot-product">57. Dot Product</h4>
<p>$x \cdot y = \sum_{i=1}^{d} x_i \, y_i$</p>
<p>where:</p>
<ul>
<li>$x, y$ are vectors in (\mathbb{R}^d)</li>
</ul>
<hr />
<h4 id="58-matrix-multiplication">58. Matrix Multiplication</h4>
<p>$(AB)<em k="1">{ij} = \sum</em>$}^{n} A_{ik}\, B_{kj</p>
<p>where:</p>
<ul>
<li>$A$ is a ((m \times n)) matrix</li>
<li>$B$ is a ((n \times p)) matrix</li>
<li>$(AB)$ is a ((m \times p)) matrix</li>
</ul>
<hr />
<h4 id="59-determinant-of-a-22-matrix">59. Determinant of a 2×2 Matrix</h4>
<p>$\det\begin{pmatrix}
a &amp; b\
c &amp; d
\end{pmatrix} = ad - bc$</p>
<p>where:</p>
<ul>
<li>$a, b, c, d$ are elements of the matrix</li>
</ul>
<hr />
<h4 id="60-matrix-inverse-22">60. Matrix Inverse (2×2)</h4>
<p>$\begin{pmatrix} a &amp; b \ c &amp; d \end{pmatrix}^{-1} 
= \frac{1}{ad - bc} \begin{pmatrix} d &amp; -b \ -c &amp; a\end{pmatrix}$</p>
<p>where:</p>
<ul>
<li>$a, b, c, d$ are elements</li>
<li>$ad - bc \neq 0$</li>
</ul>
<hr />
<h4 id="61-frobenius-norm">61. Frobenius Norm</h4>
<p>$|A|<em i="1">F = \sqrt{\sum</em>$}^{m}\sum_{j=1}^{n} |A_{ij}|^2</p>
<p>where:</p>
<ul>
<li>$A$ is an (m \times n) matrix</li>
<li>$A_{ij}$ is the element in row (i), column (j)</li>
</ul>
<hr />
<h4 id="62-kl-divergence">62. KL Divergence</h4>
<p>$D_{\text{KL}}(P | Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}$</p>
<p>where:</p>
<ul>
<li>$P$ and $Q$ are discrete probability distributions</li>
<li>The sum is over all possible outcomes (x)</li>
</ul>
<hr />
<h4 id="63-jensen-shannon-divergence">63. Jensen-Shannon Divergence</h4>
<p>$D_{\text{JS}}(P|Q) = \frac{1}{2} D_{\text{KL}}\big(P | M\big) + \frac{1}{2} D_{\text{KL}}\big(Q | M\big)$</p>
<p>where:</p>
<ul>
<li>$M = \frac{1}{2}(P + Q)$</li>
<li>$P$ and $Q$ are probability distributions</li>
</ul>
<hr />
<h4 id="64-bayes-rule">64. Bayes’ Rule</h4>
<p>$P(A|B) = \frac{P(B|A)\,P(A)}{P(B)}$</p>
<p>where:</p>
<ul>
<li>$A$ and $B$ are events</li>
<li>$P(A)$, $P(B)$, $P(B|A)$ are known or can be inferred</li>
</ul>
<hr />
<h4 id="65-covariance">65. Covariance</h4>
<p>$\text{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])]$</p>
<p>where:</p>
<ul>
<li>$E[\cdot]$ denotes the expectation</li>
<li>$X, Y$ are random variables</li>
</ul>
<hr />
<h4 id="66-pearson-correlation">66. Pearson Correlation</h4>
<p>$\rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$</p>
<p>where:</p>
<ul>
<li>$\sigma_X$ and $\sigma_Y$ are standard deviations of (X) and (Y)</li>
</ul>
<hr />
<h4 id="67-pca-covariance-eigen-decomposition">67. PCA Covariance Eigen-Decomposition</h4>
<p>$C = U \Lambda U^\top$</p>
<p>where:</p>
<ul>
<li>$C$ is the covariance matrix</li>
<li>$U$ is the matrix of eigenvectors</li>
<li>$\Lambda$ is the diagonal matrix of eigenvalues</li>
</ul>
<hr />
<h4 id="68-singular-value-decomposition-svd">68. Singular Value Decomposition (SVD)</h4>
<p>$A = U \Sigma V^\top$</p>
<p>where:</p>
<ul>
<li>$A$ is an ((m \times n)) matrix</li>
<li>$U$ and $V$ are orthonormal matrices</li>
<li>$\Sigma$ is the diagonal matrix of singular values</li>
</ul>
<hr />
<h4 id="69-gaussian-distribution-1d">69. Gaussian Distribution (1D)</h4>
<p>$\mathcal{N}(x \mid \mu,\sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\Big(-\frac{(x-\mu)^2}{2\sigma^2}\Big)$</p>
<p>where:</p>
<ul>
<li>$\mu$ is the mean</li>
<li>$\sigma^2$ is the variance</li>
</ul>
<hr />
<h4 id="70-probability-integral-discrete">70. Probability Integral (Discrete)</h4>
<p>$\sum_{x} P(x) = 1$</p>
<p>where:</p>
<ul>
<li>$x$ ranges over all possible outcomes</li>
<li>$P(x)$ is a probability distribution</li>
</ul>
<hr />
<h4 id="71-probability-integral-continuous">71. Probability Integral (Continuous)</h4>
<p>$\int_{-\infty}^{\infty} p(x)\, dx = 1$</p>
<p>where:</p>
<ul>
<li>$p(x)$ is a probability density function</li>
</ul>
<hr />
<h4 id="72-exponential-family-general-form">72. Exponential Family (General Form)</h4>
<p>$p(x|\theta) = h(x) \exp\big(\eta(\theta)^\top T(x) - A(\theta)\big)$</p>
<p>where:</p>
<ul>
<li>$h(x)$, $T(x)$ are known functions</li>
<li>$\eta(\theta)$ is the natural parameter</li>
<li>$A(\theta)$ is the log-partition function</li>
</ul>
<hr />
<h4 id="73-weighted-adjacency-graph-neural-network">73. Weighted Adjacency (Graph Neural Network)</h4>
<p>$h_v^{(l+1)} = \sigma\Bigg(\sum_{u \in \mathcal{N}(v)} \alpha_{vu} \, W^{(l)} h_u^{(l)}\Bigg)$</p>
<p>where:</p>
<ul>
<li>$h_v^{(l)}$ is the hidden representation of node (v) at layer (l)</li>
<li>$\alpha_{vu}$ is the attention or adjacency weight from node (u) to (v)</li>
<li>$W^{(l)}$ is a trainable weight matrix</li>
<li>$\sigma$ is an activation function</li>
</ul>
<hr />
<h4 id="74-gnn-aggregation">74. GNN Aggregation</h4>
<p>$h_v^{(l+1)} = \text{AGGREGATE}\big({\,h_u^{(l)} : u \in \mathcal{N}(v)}\big)$</p>
<p>where:</p>
<ul>
<li>$\mathcal{N}(v)$ is the set of neighbors of node (v)</li>
<li>$\text{AGGREGATE}(\cdot)$ could be sum, mean, max, or an attention-based mechanism</li>
</ul>
<hr />
<h4 id="75-rmsprop-repeated-for-emphasis">75. RMSProp (Repeated for Emphasis)</h4>
<p>$\theta \leftarrow \theta - \frac{\eta}{\sqrt{E[g^2] + \epsilon}} \nabla_\theta L(\theta)$</p>
<p>where:</p>
<ul>
<li>$\theta$ is the parameter vector</li>
<li>$\eta$ is the learning rate</li>
<li>$E[g^2]$ is the running average of squared gradients</li>
<li>$\epsilon$ prevents division by zero</li>
</ul>
<hr />
<h4 id="76-weighted-combination-for-multi-head">76. Weighted Combination for Multi-Head</h4>
<p>$\text{head}_i = \text{Attention}(QW_i^Q, \; KW_i^K, \; VW_i^V)$</p>
<p>where:</p>
<ul>
<li>$Q, K, V$ are query, key, and value matrices</li>
<li>$W_i^Q, W_i^K, W_i^V$ are parameter matrices for head (i)</li>
</ul>
<hr />
<h4 id="77-gradient-of-mse-wrt-weights">77. Gradient of MSE w.r.t. Weights</h4>
<p>$\frac{\partial L_{\text{MSE}}}{\partial w_j} = \frac{1}{N}\sum_{i=1}^N 2 \,(\hat{y}^{(i)} - y^{(i)}) \, x_j^{(i)}$</p>
<p>where:</p>
<ul>
<li>$w_j$ is weight (j)</li>
<li>$x_j^{(i)}$ is the (j)-th feature of sample (i)</li>
</ul>
<hr />
<h4 id="78-gradient-of-binary-cross-entropy-wrt-weights">78. Gradient of Binary Cross-Entropy w.r.t. Weights</h4>
<p>$\frac{\partial L_{\text{CE}}}{\partial w_j} = \frac{1}{N} \sum_{i=1}^N (\hat{y}^{(i)} - y^{(i)})\, x_j^{(i)}$</p>
<p>where:</p>
<ul>
<li>$\hat{y}^{(i)}$ is the predicted probability</li>
<li>$y^{(i)}$ is the true label</li>
</ul>
<hr />
<h4 id="79-logistic-regression-decision-boundary">79. Logistic Regression Decision Boundary</h4>
<p>$\hat{y} = \sigma(w^\top x + b) \implies 
\text{Decision} = \begin{cases}
1 &amp; \text{if } w^\top x + b \ge 0\
0 &amp; \text{otherwise}
\end{cases}$</p>
<p>where:</p>
<ul>
<li>$w, b$ are parameters</li>
<li>$x$ is the input</li>
<li>$\sigma$ is the sigmoid function</li>
</ul>
<hr />
<h4 id="80-balanced-cross-entropy">80. Balanced Cross-Entropy</h4>
<p>$L_{\text{bal}} = -\frac{1}{N} \sum_{i=1}^N \Big[\beta \, y^{(i)} \log(\hat{y}^{(i)}) + (1-\beta)(1-y^{(i)}) \log(1-\hat{y}^{(i)})\Big]$</p>
<p>where:</p>
<ul>
<li>$\beta$ is a factor weighting the positive class</li>
<li>$y^{(i)}, \hat{y}^{(i)}$ as before</li>
</ul>
<hr />
<h4 id="81-f1-score">81. F1 Score</h4>
<p>$\text{F1} = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}$</p>
<p>where:</p>
<ul>
<li>$\text{precision} = \frac{TP}{TP+FP}$</li>
<li>$\text{recall} = \frac{TP}{TP+FN}$</li>
</ul>
<hr />
<h4 id="82-precision">82. Precision</h4>
<p>$\text{precision} = \frac{TP}{TP + FP}$</p>
<p>where:</p>
<ul>
<li>$TP$ = true positives</li>
<li>$FP$ = false positives</li>
</ul>
<hr />
<h4 id="83-recall">83. Recall</h4>
<p>$\text{recall} = \frac{TP}{TP + FN}$</p>
<p>where:</p>
<ul>
<li>$TP$ = true positives</li>
<li>$FN$ = false negatives</li>
</ul>
<hr />
<h4 id="84-auc-area-under-roc-curve">84. AUC (Area Under ROC Curve)</h4>
<p>$\text{AUC} = \int_0^1 TPR(\text{FPR}^{-1}(x))\, dx$</p>
<p>where:</p>
<ul>
<li>$TPR$ is the true positive rate</li>
<li>$FPR$ is the false positive rate</li>
<li>The integral is conceptually the area under the ROC curve</li>
</ul>
<hr />
<h4 id="85-step-decay-learning-rate-scheduling">85. Step Decay (Learning Rate Scheduling)</h4>
<p>$\eta_{\text{new}} = \eta_{\text{old}} \cdot \gamma^{\lfloor \frac{\text{epoch}}{k}\rfloor}$</p>
<p>where:</p>
<ul>
<li>$\eta_{\text{old}}$ is the current learning rate</li>
<li>$\eta_{\text{new}}$ is the updated learning rate</li>
<li>$\gamma \in (0,1)$ is the decay factor</li>
<li>$k$ is the step size in epochs</li>
</ul>
<hr />
<h4 id="86-polynomial-kernel-svm">86. Polynomial Kernel (SVM)</h4>
<p>$K(x, x') = (x^\top x' + c)^p$</p>
<p>where:</p>
<ul>
<li>$x, x'$ are feature vectors</li>
<li>$c$ is a constant (often 1)</li>
<li>$p$ is the polynomial degree</li>
</ul>
<hr />
<h4 id="87-radial-basis-function-kernel-rbf">87. Radial Basis Function Kernel (RBF)</h4>
<p>$K(x, x') = \exp\Big(-\frac{|x - x'|^2}{2\sigma^2}\Big)$</p>
<p>where:</p>
<ul>
<li>$x, x'$ are feature vectors</li>
<li>$\sigma$ is the kernel width</li>
</ul>
<hr />
<h4 id="88-hinge-loss-svm">88. Hinge Loss (SVM)</h4>
<p>$L_{\text{hinge}} = \max\big(0, 1 - y_i (w^\top x_i + b)\big)$</p>
<p>where:</p>
<ul>
<li>$y_i \in {-1, +1}$ is the true label</li>
<li>$w$ is the weight vector</li>
<li>$x_i$ is the input</li>
<li>$b$ is the bias</li>
</ul>
<hr />
<h4 id="89-margin-hard-svm">89. Margin (Hard-SVM)</h4>
<p>$\text{Margin} = \frac{2}{|w|}$</p>
<p>where:</p>
<ul>
<li>$w$ is the normal vector to the decision boundary</li>
<li>$|w|$ is the Euclidean norm of $w$</li>
</ul>
<hr />
<h4 id="90-q-learning-update-rl">90. Q-Learning Update (RL)</h4>
<p>$Q(s,a) \leftarrow Q(s,a) + \alpha \Big[r + \gamma \max_{a'} Q(s', a') - Q(s,a)\Big]$</p>
<p>where:</p>
<ul>
<li>$Q(s,a)$ is the current Q-value</li>
<li>$\alpha$ is the learning rate</li>
<li>$r$ is the reward</li>
<li>$\gamma$ is the discount factor</li>
<li>$s'$ is the next state</li>
<li>$a'$ is the next action</li>
</ul>
<hr />
<h4 id="91-bellman-equation-value-function">91. Bellman Equation (Value Function)</h4>
<p>$V^\pi(s) = \mathbb{E}_{a\sim \pi}\Big[r(s,a) + \gamma V^\pi(s')\Big]$</p>
<p>where:</p>
<ul>
<li>$V^\pi(s)$ is the value of state (s) under policy (\pi)</li>
<li>$r(s,a)$ is the immediate reward</li>
<li>$\gamma$ is the discount factor</li>
<li>$s'$ is the next state</li>
</ul>
<hr />
<h4 id="92-actor-critic-gradient">92. Actor-Critic Gradient</h4>
<p>$\nabla_\theta J(\theta) = \mathbb{E}\big[\nabla_\theta \log \pi_\theta(a|s)\, (Q_w(s,a) - b(s))\big]$</p>
<p>where:</p>
<ul>
<li>$\pi_\theta$ is the policy (actor)</li>
<li>$Q_w(s,a)$ is the critic’s estimate of action-value</li>
<li>$b(s)$ is a baseline to reduce variance</li>
</ul>
<hr />
<h4 id="93-softplus-activation">93. Softplus Activation</h4>
<p>$\text{Softplus}(z) = \log(1 + e^z)$</p>
<p>where:</p>
<ul>
<li>$z$ is the input</li>
</ul>
<hr />
<h4 id="94-elu-exponential-linear-unit">94. ELU (Exponential Linear Unit)</h4>
<p>$\text{ELU}(z) =
\begin{cases}
z &amp; z \ge 0,\
\alpha(e^z - 1) &amp; z &lt; 0
\end{cases}$</p>
<p>where:</p>
<ul>
<li>$\alpha$ is a positive constant</li>
</ul>
<hr />
<h4 id="95-selu-scaled-elu">95. SELU (Scaled ELU)</h4>
<p>$\text{SELU}(z) = \lambda \begin{cases}
z &amp; z \ge 0\
\alpha(e^z - 1) &amp; z &lt; 0
\end{cases}$</p>
<p>where:</p>
<ul>
<li>$\alpha$ and $\lambda$ are constants (e.g., (\alpha \approx 1.673), (\lambda \approx 1.051))</li>
</ul>
<hr />
<h4 id="96-knowledge-distillation">96. Knowledge Distillation</h4>
<p>$L_{\text{KD}} = \tau^2 \cdot \text{KL}\big(p_\text{teacher}(x;\tau)\,|\,p_\text{student}(x;\tau)\big)$</p>
<p>where:</p>
<ul>
<li>$\tau$ is the temperature</li>
<li>$p_\text{teacher}$ and $p_\text{student}$ are softmax outputs of teacher and student networks, respectively</li>
</ul>
<hr />
<h4 id="97-threshold-function-binary-step">97. Threshold Function (Binary Step)</h4>
<p>$\theta(z) =
\begin{cases}
1 &amp; z \ge 0\
0 &amp; z &lt; 0
\end{cases}$</p>
<p>where:</p>
<ul>
<li>$z$ is the input</li>
</ul>
<hr />
<h4 id="98-weighted-hinge-loss">98. Weighted Hinge Loss</h4>
<p>$L_{\text{whinge}} = w_i \,\max\big(0, 1 - y_i (w^\top x_i + b)\big)$</p>
<p>where:</p>
<ul>
<li>$w_i$ is the sample weight or class weight</li>
<li>$y_i \in {-1, +1}$ is the true label</li>
</ul>
<hr />
<h4 id="99-perplexity-language-modeling">99. Perplexity (Language Modeling)</h4>
<p>$\text{PPL} = \exp\Big(\frac{1}{N}\sum_{i=1}^{N} -\log p(w_i)\Big)$</p>
<p>where:</p>
<ul>
<li>$w_i$ are tokens in the sequence</li>
<li>$p(w_i)$ is the predicted probability of token (w_i)</li>
<li>$N$ is the total number of tokens</li>
</ul>
<hr />
<h4 id="100-cycle-consistency-loss-cyclegan">100. Cycle Consistency Loss (CycleGAN)</h4>
<p>$L_{\text{cyc}}(G,F) = \mathbb{E}<em _sim="\sim" p_y_="p(y)" y="y">{x \sim p(x)}\big[|F(G(x)) - x|\big] + \mathbb{E}</em>\big[|G(F(y)) - y|\big]$</p>
<p>where:</p>
<ul>
<li>$G$ is the generator mapping domain (X \to Y)</li>
<li>$F$ is the generator mapping domain (Y \to X)</li>
<li>$x, y$ are samples from domains (X, Y)</li>
<li>$|\cdot|$ is a distance measure (e.g., L1 norm)</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../06-glossary-of-terms/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Glossary of Terms">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Glossary of Terms
              </div>
            </div>
          </a>
        
        
          
          <a href="../../sims/" class="md-footer__link md-footer__link--next" aria-label="Next: Intro">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Intro
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "navigation.expand", "navigation.path", "navigation.prune", "navigation.indexes", "toc.follow", "navigation.top", "navigation.footer", "content.action.edit"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>