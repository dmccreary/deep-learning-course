
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="An example of an intelligent textbook for teaching deep learning. All content licensed Creative Commons ShareAlike Attribution Noncommercial">
      
      
        <meta name="author" content="Dan McCreary">
      
      
        <link rel="canonical" href="https://dmccreary.github.io/deep-learning-course/prompts/07-equations/">
      
      
        <link rel="prev" href="../06-glossary-of-terms/">
      
      
        <link rel="next" href="../08-word-cloud/">
      
      
      <link rel="icon" href="../../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.45">
    
    
      
        <title>Equations - Deep Learning Course with AI</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Equations - Deep Learning Course with AI" >
      
        <meta  property="og:description"  content="An example of an intelligent textbook for teaching deep learning. All content licensed Creative Commons ShareAlike Attribution Noncommercial" >
      
        <meta  property="og:image"  content="https://dmccreary.github.io/deep-learning-course/assets/images/social/prompts/07-equations.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://dmccreary.github.io/deep-learning-course/prompts/07-equations/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Equations - Deep Learning Course with AI" >
      
        <meta  name="twitter:description"  content="An example of an intelligent textbook for teaching deep learning. All content licensed Creative Commons ShareAlike Attribution Noncommercial" >
      
        <meta  name="twitter:image"  content="https://dmccreary.github.io/deep-learning-course/assets/images/social/prompts/07-equations.png" >
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="gold">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#generating-commonly-used-deep-learning-equations" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Deep Learning Course with AI" class="md-header__button md-logo" aria-label="Deep Learning Course with AI" data-md-component="logo">
      
  <img src="../../img/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Deep Learning Course with AI
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Equations
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/dmccreary/deep-learning-course" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub Repo
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Deep Learning Course with AI" class="md-nav__button md-logo" aria-label="Deep Learning Course with AI" data-md-component="logo">
      
  <img src="../../img/logo.png" alt="logo">

    </a>
    Deep Learning Course with AI
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/dmccreary/deep-learning-course" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub Repo
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course-description/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Course Description
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../chapters/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Chapters
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Prompts
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Prompts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-bloom-taxonomy-description/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Course Description
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-concept-enumeration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Concept Enumeration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-concept-dependencies/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Concept Dependencies
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-concept-taxonomy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Concept Taxonomy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-glossary-of-terms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Glossary of Terms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Equations
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Equations
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slope-and-intercept" class="md-nav__link">
    <span class="md-ellipsis">
      Slope and Intercept
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#area-of-a-circle" class="md-nav__link">
    <span class="md-ellipsis">
      Area of a Circle
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#response" class="md-nav__link">
    <span class="md-ellipsis">
      Response
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08-word-cloud/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Word Cloud
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-suggested-simulations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Suggested Simulations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11-tensorflow-playground/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TensorFlow Playground
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10-activation-functions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Activation Functions
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../sims/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    MicroSims
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../checklist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Checklist
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../glossary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Glossary
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../how-we-built-this-site/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How We Built This Site
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../feedback/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Feedback
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../license/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    License
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    References
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contact/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contact
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slope-and-intercept" class="md-nav__link">
    <span class="md-ellipsis">
      Slope and Intercept
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#area-of-a-circle" class="md-nav__link">
    <span class="md-ellipsis">
      Area of a Circle
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#response" class="md-nav__link">
    <span class="md-ellipsis">
      Response
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/dmccreary/deep-learning-course/blob/master/docs/prompts/07-equations.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  


<h1 id="generating-commonly-used-deep-learning-equations">Generating Commonly Used Deep Learning Equations</h1>
<div class="admonition prompt">
<p class="admonition-title">Prompt</p>
<p>Create a list of the 100 most commonly used
equations in a college level course in deep learning.</p>
<p>For each equation, return the following:</p>
<ol>
<li>The name of the equation in markdown level 4.</li>
<li>Within the body, return the equation in LaTeX format</li>
<li>Surround the equation with dollar signs</li>
<li>For each variable in the equation, add a "where:" that defines each variable</li>
<li>Put blank lines around the list of variables</li>
<li>Wrap the variables names in dollar signs</li>
</ol>
<p>Examples:</p>
<h4 id="slope-and-intercept">Slope and Intercept</h4>
<p><span class="arithmatex">\(y = m * x + b\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(y\)</span> is the vertical axis</li>
<li><span class="arithmatex">\(x\)</span> is the horizontal axis</li>
<li><span class="arithmatex">\(m\)</span> is the slope</li>
<li><span class="arithmatex">\(b\)</span> is the intercept</li>
</ul>
<h4 id="area-of-a-circle">Area of a Circle</h4>
<p><span class="arithmatex">\(A = 2\pi r^2\)</span>
where:</p>
<ul>
<li><span class="arithmatex">\(A\)</span> is the area of the circle</li>
<li><span class="arithmatex">\(r\)</span> is the radius</li>
</ul>
</div>
<h2 id="response">Response</h2>
<p>Below are 100 commonly encountered equations in a college-level deep learning course, presented in alphabetical order. Each entry has:</p>
<ol>
<li>A heading in <strong>Markdown level 4</strong>.</li>
<li>The equation in <strong>LaTeX</strong>, surrounded by dollar signs.</li>
<li>A <strong>where:</strong> section defining each variable, with each variable in dollar signs and blank lines around the list.</li>
</ol>
<h1 id="100-common-equations-in-deep-learning">100 Common Equations in Deep Learning</h1>
<p>Below is a curated list of 100 frequently encountered equations in a college-level Deep Learning course. Each entry includes:</p>
<ul>
<li><strong>A name</strong> (in level 4 markdown).</li>
<li><strong>The equation</strong> (in LaTeX, surrounded by single dollar signs).</li>
<li>A <strong>“where:”</strong> section explaining each variable.</li>
</ul>
<p>The equations are grouped loosely by theme (basic algebra, loss functions, optimization, activations, convolution, RNNs, attention mechanisms, etc.) but may appear in any Deep Learning curriculum.</p>
<hr />
<h4 id="1-mean-squared-error-mse">1. Mean Squared Error (MSE)</h4>
<p><span class="arithmatex">\(L_{\text{MSE}} = \frac{1}{N}\sum_{i=1}^{N}(y^{(i)} - \hat{y}^{(i)})^2\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(N\)</span> is the number of samples</li>
<li><span class="arithmatex">\(y^{(i)}\)</span> is the true label for the <span class="arithmatex">\(i\)</span>-th sample</li>
<li><span class="arithmatex">\(\hat{y}^{(i)}\)</span> is the predicted value for the <span class="arithmatex">\(i\)</span>-th sample</li>
</ul>
<hr />
<h4 id="2-cross-entropy-loss-binary">2. Cross-Entropy Loss (Binary)</h4>
<p><span class="arithmatex">\(L_{\text{CE}} = -\frac{1}{N}\sum_{i=1}^{N}\Big[y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)})\Big]\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(N\)</span> is the number of samples</li>
<li><span class="arithmatex">\(y^{(i)} \in \{0, 1\}\)</span> is the true binary label</li>
<li><span class="arithmatex">\(\hat{y}^{(i)}\)</span> is the predicted probability of the positive class</li>
</ul>
<hr />
<h4 id="3-cross-entropy-loss-multi-class">3. Cross-Entropy Loss (Multi-Class)</h4>
<p><span class="arithmatex">\(L_{\text{CE}} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{c}^{(i)} \log(\hat{y}_{c}^{(i)})\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(N\)</span> is the number of samples</li>
<li><span class="arithmatex">\(C\)</span> is the number of classes</li>
<li><span class="arithmatex">\(y_{c}^{(i)} \in \{0,1\}\)</span> indicates the true class of sample <span class="arithmatex">\(i\)</span></li>
<li><span class="arithmatex">\(\hat{y}_{c}^{(i)}\)</span> is the predicted probability for class <span class="arithmatex">\(c\)</span></li>
</ul>
<hr />
<h4 id="4-softmax-function">4. Softmax Function</h4>
<p><span class="arithmatex">\(\hat{y}_j = \frac{\exp(z_j)}{\sum_{k=1}^{C}\exp(z_k)}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z_j\)</span> is the logit for class <span class="arithmatex">\(j\)</span></li>
<li><span class="arithmatex">\(C\)</span> is the total number of classes</li>
<li><span class="arithmatex">\(\hat{y}_j\)</span> is the probability assigned to class <span class="arithmatex">\(j\)</span></li>
</ul>
<hr />
<h4 id="5-sigmoid-logistic-function">5. Sigmoid (Logistic) Function</h4>
<p><span class="arithmatex">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z\)</span> is the input or logit</li>
<li><span class="arithmatex">\(\sigma(z)\)</span> is the output between 0 and 1</li>
</ul>
<hr />
<h4 id="6-tanh-function">6. Tanh Function</h4>
<p><span class="arithmatex">\(\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z\)</span> is the input</li>
<li><span class="arithmatex">\(\tanh(z)\)</span> outputs values in the range <span class="arithmatex">\([-1, 1]\)</span></li>
</ul>
<hr />
<h4 id="7-rectified-linear-unit-relu">7. Rectified Linear Unit (ReLU)</h4>
<p><span class="arithmatex">\(\text{ReLU}(z) = \max(0, z)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z\)</span> is the input</li>
<li><span class="arithmatex">\(\text{ReLU}(z)\)</span> is zero for negative <span class="arithmatex">\(z\)</span> and <span class="arithmatex">\(z\)</span> itself if <span class="arithmatex">\(z &gt; 0\)</span></li>
</ul>
<hr />
<h4 id="8-leaky-relu">8. Leaky ReLU</h4>
<p><span class="arithmatex">\(\text{LeakyReLU}(z) = 
\begin{cases}
z &amp; \text{if } z \ge 0\\
\alpha \, z &amp; \text{if } z &lt; 0
\end{cases}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z\)</span> is the input</li>
<li><span class="arithmatex">\(\alpha\)</span> is a small positive slope (e.g., 0.01) for negative <span class="arithmatex">\(z\)</span></li>
</ul>
<hr />
<h4 id="9-weighted-sum-of-inputs">9. Weighted Sum of Inputs</h4>
<p><span class="arithmatex">\(z = \sum_{i=1}^{d} w_i x_i + b\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(d\)</span> is the number of input features</li>
<li><span class="arithmatex">\(w_i\)</span> is the weight for feature <span class="arithmatex">\(x_i\)</span></li>
<li><span class="arithmatex">\(x_i\)</span> is the <span class="arithmatex">\(i\)</span>-th input</li>
<li><span class="arithmatex">\(b\)</span> is the bias term</li>
</ul>
<hr />
<h4 id="10-neural-network-output-one-layer">10. Neural Network Output (One Layer)</h4>
<p><span class="arithmatex">\(\hat{y} = f\bigg(\sum_{i=1}^{d} w_i x_i + b\bigg)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(f(\cdot)\)</span> is an activation function</li>
<li><span class="arithmatex">\(w_i\)</span>, <span class="arithmatex">\(x_i\)</span>, and <span class="arithmatex">\(b\)</span> are defined as above</li>
</ul>
<hr />
<h4 id="11-gradient-descent-parameter-update">11. Gradient Descent (Parameter Update)</h4>
<p><span class="arithmatex">\(\theta \leftarrow \theta - \eta \,\nabla_\theta L(\theta)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\theta\)</span> represents parameters (weights, biases)</li>
<li><span class="arithmatex">\(\eta\)</span> is the learning rate</li>
<li><span class="arithmatex">\(L(\theta)\)</span> is the loss function</li>
<li><span class="arithmatex">\(\nabla_\theta L(\theta)\)</span> is the gradient of the loss w.r.t. parameters</li>
</ul>
<hr />
<h4 id="12-chain-rule-single-variable">12. Chain Rule (Single-Variable)</h4>
<p><span class="arithmatex">\(\frac{d}{dx} f(g(x)) = f'(g(x)) \cdot g'(x)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span> are functions</li>
<li><span class="arithmatex">\(f'\)</span> and <span class="arithmatex">\(g'\)</span> are their derivatives</li>
</ul>
<hr />
<h4 id="13-chain-rule-multivariable">13. Chain Rule (Multivariable)</h4>
<p><span class="arithmatex">\(\frac{\partial}{\partial x_i} L = \sum_{j} \frac{\partial L}{\partial z_j} \frac{\partial z_j}{\partial x_i}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(L\)</span> is the loss function</li>
<li><span class="arithmatex">\(z_j\)</span> are intermediate variables dependent on <span class="arithmatex">\(x_i\)</span></li>
</ul>
<hr />
<h4 id="14-momentum-update">14. Momentum Update</h4>
<div class="arithmatex">\[
\begin{aligned}
v &amp;\leftarrow \beta v + (1-\beta)\nabla_\theta L(\theta), \\
\theta &amp;\leftarrow \theta - \eta\, v
\end{aligned}
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(v\)</span> is the velocity term</li>
<li><span class="arithmatex">\(\beta\)</span> is the momentum hyperparameter (e.g., 0.9)</li>
<li><span class="arithmatex">\(\eta\)</span> is the learning rate</li>
<li><span class="arithmatex">\(\nabla_\theta L(\theta)\)</span> is the gradient</li>
</ul>
<hr />
<h4 id="15-nesterov-accelerated-gradient-nag">15. Nesterov Accelerated Gradient (NAG)</h4>
<div class="arithmatex">\[
\begin{aligned}
v &amp;\leftarrow \beta v + (1-\beta)\nabla_\theta L(\theta - \beta v),\\
\theta &amp;\leftarrow \theta - \eta\, v
\end{aligned}
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(v\)</span> is the velocity term</li>
<li><span class="arithmatex">\(\beta\)</span> is the momentum factor</li>
<li><span class="arithmatex">\(\theta\)</span> are the parameters</li>
<li><span class="arithmatex">\(\eta\)</span> is the learning rate</li>
</ul>
<hr />
<h4 id="16-rmsprop-update">16. RMSProp Update</h4>
<div class="arithmatex">\[
\begin{aligned}
E[g^2] &amp;\leftarrow \rho\, E[g^2] + (1-\rho) \, (\nabla_\theta L(\theta))^2, \\
\theta &amp;\leftarrow \theta - \frac{\eta}{\sqrt{E[g^2] + \epsilon}} \nabla_\theta L(\theta)
\end{aligned}
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(E[g^2]\)</span> is the running average of squared gradients</li>
<li><span class="arithmatex">\(\rho\)</span> is the decay rate (e.g., 0.9)</li>
<li><span class="arithmatex">\(\eta\)</span> is the learning rate</li>
<li><span class="arithmatex">\(\epsilon\)</span> is a small constant for numerical stability</li>
</ul>
<hr />
<h4 id="17-adam-optimizer">17. Adam Optimizer</h4>
<div class="arithmatex">\[
\begin{aligned}
m &amp;\leftarrow \beta_1 m + (1 - \beta_1)\nabla_\theta L(\theta), \\
v &amp;\leftarrow \beta_2 v + (1 - \beta_2)(\nabla_\theta L(\theta))^2, \\
\hat{m} &amp;\leftarrow \frac{m}{1 - \beta_1^t}, \quad \hat{v} \leftarrow \frac{v}{1 - \beta_2^t}, \\
\theta &amp;\leftarrow \theta - \eta \frac{\hat{m}}{\sqrt{\hat{v}} + \epsilon}
\end{aligned}
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(m\)</span> is the first moment estimate</li>
<li><span class="arithmatex">\(v\)</span> is the second moment estimate</li>
<li><span class="arithmatex">\(\beta_1\)</span> and <span class="arithmatex">\(\beta_2\)</span> are exponential decay rates</li>
<li><span class="arithmatex">\(\eta\)</span> is the learning rate</li>
<li><span class="arithmatex">\(t\)</span> is the current time step</li>
<li><span class="arithmatex">\(\epsilon\)</span> is a small constant</li>
</ul>
<hr />
<h4 id="18-l1-regularization">18. L1 Regularization</h4>
<p><span class="arithmatex">\(R_{L1} = \lambda \sum_{j} |w_j|\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\lambda\)</span> is the regularization coefficient</li>
<li><span class="arithmatex">\(w_j\)</span> are the weights</li>
</ul>
<hr />
<h4 id="19-l2-regularization">19. L2 Regularization</h4>
<p><span class="arithmatex">\(R_{L2} = \frac{\lambda}{2} \sum_{j} w_j^2\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\lambda\)</span> is the regularization strength</li>
<li><span class="arithmatex">\(w_j\)</span> are the model’s weights</li>
<li>The factor <span class="arithmatex">\(\tfrac{1}{2}\)</span> is often included by convention</li>
</ul>
<hr />
<h4 id="20-weighted-cross-entropy-class-imbalance">20. Weighted Cross-Entropy (Class Imbalance)</h4>
<p><span class="arithmatex">\(L_{\text{WCE}} = -\frac{1}{N} \sum_{i=1}^N \Big[\alpha \, y^{(i)} \log(\hat{y}^{(i)}) + (1-\alpha)\,(1-y^{(i)})\log(1-\hat{y}^{(i)})\Big]\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\alpha\)</span> is a weight factor for the positive class</li>
<li><span class="arithmatex">\(y^{(i)} \in \{0,1\}\)</span> is the true label</li>
<li><span class="arithmatex">\(\hat{y}^{(i)}\)</span> is the predicted probability</li>
</ul>
<hr />
<h4 id="21-negative-log-likelihood-nll">21. Negative Log-Likelihood (NLL)</h4>
<p><span class="arithmatex">\(L_{\text{NLL}} = -\sum_{i=1}^N \log \, p(y^{(i)} | x^{(i)})\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(p(y|x)\)</span> is the predicted probability of label <span class="arithmatex">\(y\)</span> given input <span class="arithmatex">\(x\)</span></li>
<li><span class="arithmatex">\(N\)</span> is the number of samples</li>
</ul>
<hr />
<h4 id="22-normal-equation-linear-regression">22. Normal Equation (Linear Regression)</h4>
<p><span class="arithmatex">\(\theta = (X^\top X)^{-1} X^\top y\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(X\)</span> is the design matrix</li>
<li><span class="arithmatex">\(y\)</span> is the vector of targets</li>
<li><span class="arithmatex">\(\theta\)</span> is the vector of parameters</li>
</ul>
<hr />
<h4 id="23-coefficient-of-determination-r2">23. Coefficient of Determination (<span class="arithmatex">\(R^2\)</span>)</h4>
<p><span class="arithmatex">\(R^2 = 1 - \frac{\sum_{i=1}^N (y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^N (y^{(i)} - \bar{y})^2}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(y^{(i)}\)</span> is the actual value</li>
<li><span class="arithmatex">\(\hat{y}^{(i)}\)</span> is the predicted value</li>
<li><span class="arithmatex">\(\bar{y}\)</span> is the mean of <span class="arithmatex">\(y^{(i)}\)</span></li>
</ul>
<hr />
<h4 id="24-batch-normalization-mean">24. Batch Normalization (Mean)</h4>
<p><span class="arithmatex">\(\mu_B = \frac{1}{m}\sum_{i=1}^{m} x_i\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(m\)</span> is the number of samples in a mini-batch</li>
<li><span class="arithmatex">\(x_i\)</span> is the <span class="arithmatex">\(i\)</span>-th activation in the batch</li>
</ul>
<hr />
<h4 id="25-batch-normalization-variance">25. Batch Normalization (Variance)</h4>
<p><span class="arithmatex">\(\sigma_B^2 = \frac{1}{m}\sum_{i=1}^{m} (x_i - \mu_B)^2\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(x_i\)</span> is the <span class="arithmatex">\(i\)</span>-th activation in the batch</li>
<li><span class="arithmatex">\(\mu_B\)</span> is the batch mean</li>
<li><span class="arithmatex">\(\sigma_B^2\)</span> is the batch variance</li>
</ul>
<hr />
<h4 id="26-batch-normalization-forward-pass">26. Batch Normalization (Forward Pass)</h4>
<p><span class="arithmatex">\(\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y_i = \gamma \, \hat{x}_i + \beta\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(x_i\)</span> is the input activation</li>
<li><span class="arithmatex">\(\mu_B\)</span>, <span class="arithmatex">\(\sigma_B^2\)</span> are batch statistics</li>
<li><span class="arithmatex">\(\epsilon\)</span> is a small constant</li>
<li><span class="arithmatex">\(\gamma\)</span>, <span class="arithmatex">\(\beta\)</span> are learnable scale and shift parameters</li>
</ul>
<hr />
<h4 id="27-dropout-forward-pass">27. Dropout (Forward Pass)</h4>
<p><span class="arithmatex">\(\tilde{h}^{(l)} = \mathbf{m}^{(l)} \odot h^{(l)}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(h^{(l)}\)</span> is the activation vector at layer <span class="arithmatex">\(l\)</span></li>
<li><span class="arithmatex">\(\mathbf{m}^{(l)}\)</span> is a random binary mask (each entry ~ Bernoulli(p))</li>
<li><span class="arithmatex">\(\odot\)</span> denotes elementwise multiplication</li>
</ul>
<hr />
<h4 id="28-2d-convolution-without-stridepadding">28. 2D Convolution (Without Stride/Padding)</h4>
<p><span class="arithmatex">\((I * K)(x,y) = \sum_{u=-r}^{r} \sum_{v=-s}^{s} I(x+u, y+v)\, K(u,v)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(I\)</span> is the input image</li>
<li><span class="arithmatex">\(K\)</span> is the kernel</li>
<li><span class="arithmatex">\(r\)</span>, <span class="arithmatex">\(s\)</span> define kernel size offsets</li>
</ul>
<hr />
<h4 id="29-transposed-convolution">29. Transposed Convolution</h4>
<p><span class="arithmatex">\(\text{ConvTranspose}(z, K) = \sum_{u}\sum_{v} z(u,v)\, \text{Upsample}(K, \text{stride})\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z(u,v)\)</span> is an activation map</li>
<li><span class="arithmatex">\(K\)</span> is the kernel</li>
<li><span class="arithmatex">\(\text{stride}\)</span> is the factor by which the output is upsampled</li>
</ul>
<hr />
<h4 id="30-max-pooling-2d">30. Max Pooling (2D)</h4>
<p><span class="arithmatex">\(\text{MaxPool}(x,y) = \max_{(p,q)\in \,\mathcal{R}_{x,y}} I(p,q)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(I\)</span> is the input feature map</li>
<li><span class="arithmatex">\(\mathcal{R}_{x,y}\)</span> is the receptive field around location <span class="arithmatex">\((x,y)\)</span></li>
</ul>
<hr />
<h4 id="31-average-pooling-2d">31. Average Pooling (2D)</h4>
<p><span class="arithmatex">\(\text{AvgPool}(x,y) = \frac{1}{|\mathcal{R}_{x,y}|}\sum_{(p,q)\in \,\mathcal{R}_{x,y}} I(p,q)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\mathcal{R}_{x,y}\)</span> is the region of pooling</li>
<li><span class="arithmatex">\(|\mathcal{R}_{x,y}|\)</span> is the size of that region</li>
</ul>
<hr />
<h4 id="32-rnn-hidden-state-update">32. RNN Hidden State Update</h4>
<p><span class="arithmatex">\(h_t = f(W_{hh} \, h_{t-1} + W_{xh} \, x_t + b_h)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(h_t\)</span> is the hidden state at time <span class="arithmatex">\(t\)</span></li>
<li><span class="arithmatex">\(x_t\)</span> is the input at time <span class="arithmatex">\(t\)</span></li>
<li><span class="arithmatex">\(W_{hh}, W_{xh}\)</span> are weight matrices</li>
<li><span class="arithmatex">\(b_h\)</span> is a bias vector</li>
<li><span class="arithmatex">\(f(\cdot)\)</span> is an activation (e.g., tanh)</li>
</ul>
<hr />
<h4 id="33-lstm-input-gate">33. LSTM Input Gate</h4>
<p><span class="arithmatex">\(i_t = \sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(i_t\)</span> is the input gate</li>
<li><span class="arithmatex">\(x_t\)</span> is the current input</li>
<li><span class="arithmatex">\(h_{t-1}\)</span> is the previous hidden state</li>
<li><span class="arithmatex">\(W_{xi}, W_{hi}\)</span> are weight matrices</li>
<li><span class="arithmatex">\(b_i\)</span> is the bias</li>
<li><span class="arithmatex">\(\sigma\)</span> is the sigmoid function</li>
</ul>
<hr />
<h4 id="34-lstm-forget-gate">34. LSTM Forget Gate</h4>
<p><span class="arithmatex">\(f_t = \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(f_t\)</span> is the forget gate</li>
<li><span class="arithmatex">\(W_{xf}, W_{hf}\)</span> are weight matrices</li>
<li><span class="arithmatex">\(b_f\)</span> is the bias</li>
</ul>
<hr />
<h4 id="35-lstm-output-gate">35. LSTM Output Gate</h4>
<p><span class="arithmatex">\(o_t = \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(o_t\)</span> is the output gate</li>
<li><span class="arithmatex">\(W_{xo}, W_{ho}\)</span> are weight matrices</li>
<li><span class="arithmatex">\(b_o\)</span> is the bias</li>
</ul>
<hr />
<h4 id="36-lstm-cell-state">36. LSTM Cell State</h4>
<p><span class="arithmatex">\(c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t, \quad \tilde{c}_t = \tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(c_t\)</span> is the cell state</li>
<li><span class="arithmatex">\(f_t, i_t\)</span> are forget and input gates</li>
<li><span class="arithmatex">\(\tilde{c}_t\)</span> is the candidate cell state</li>
<li><span class="arithmatex">\(\odot\)</span> is elementwise multiplication</li>
</ul>
<hr />
<h4 id="37-gru-update-gate">37. GRU Update Gate</h4>
<p><span class="arithmatex">\(z_t = \sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z_t\)</span> is the update gate</li>
<li><span class="arithmatex">\(x_t\)</span> is the input at time <span class="arithmatex">\(t\)</span></li>
<li><span class="arithmatex">\(h_{t-1}\)</span> is the previous hidden state</li>
<li><span class="arithmatex">\(W_{xz}, W_{hz}\)</span> are weight matrices</li>
<li><span class="arithmatex">\(b_z\)</span> is the bias</li>
</ul>
<hr />
<h4 id="38-gru-reset-gate">38. GRU Reset Gate</h4>
<p><span class="arithmatex">\(r_t = \sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(r_t\)</span> is the reset gate</li>
<li><span class="arithmatex">\(W_{xr}, W_{hr}\)</span> are weight matrices</li>
<li><span class="arithmatex">\(b_r\)</span> is the bias</li>
</ul>
<hr />
<h4 id="39-gru-hidden-state">39. GRU Hidden State</h4>
<p><span class="arithmatex">\(h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h}_t, \quad \tilde{h}_t = \tanh(W_{xh} x_t + r_t \odot (W_{hh} h_{t-1}))\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(h_t\)</span> is the updated hidden state</li>
<li><span class="arithmatex">\(z_t, r_t\)</span> are the update and reset gates</li>
<li><span class="arithmatex">\(\tilde{h}_t\)</span> is the candidate hidden state</li>
</ul>
<hr />
<h4 id="40-attention-score-dot-product">40. Attention Score (Dot Product)</h4>
<p><span class="arithmatex">\(e_{t,s} = h_t^\top \, s_s\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(h_t\)</span> is the query vector</li>
<li><span class="arithmatex">\(s_s\)</span> is the key vector</li>
<li><span class="arithmatex">\(e_{t,s}\)</span> is the scalar score measuring alignment</li>
</ul>
<hr />
<h4 id="41-attention-weights-softmax">41. Attention Weights (Softmax)</h4>
<p><span class="arithmatex">\(\alpha_{t,s} = \frac{\exp(e_{t,s})}{\sum_{k=1}^{S} \exp(e_{t,k})}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(e_{t,s}\)</span> is the attention score for query <span class="arithmatex">\(t\)</span> and key <span class="arithmatex">\(s\)</span></li>
<li><span class="arithmatex">\(S\)</span> is the total number of key vectors</li>
</ul>
<hr />
<h4 id="42-context-vector">42. Context Vector</h4>
<p><span class="arithmatex">\(c_t = \sum_{s=1}^{S} \alpha_{t,s}\, v_s\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\alpha_{t,s}\)</span> are attention weights</li>
<li><span class="arithmatex">\(v_s\)</span> is the value vector associated with key <span class="arithmatex">\(s\)</span></li>
<li><span class="arithmatex">\(c_t\)</span> is the resulting weighted sum</li>
</ul>
<hr />
<h4 id="43-scaled-dot-product-attention">43. Scaled Dot-Product Attention</h4>
<p><span class="arithmatex">\(\text{Attention}(Q,K,V) = \text{softmax}\bigg(\frac{QK^\top}{\sqrt{d_k}}\bigg)V\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(Q\)</span> is the matrix of query vectors</li>
<li><span class="arithmatex">\(K\)</span> is the matrix of key vectors</li>
<li><span class="arithmatex">\(V\)</span> is the matrix of value vectors</li>
<li><span class="arithmatex">\(d_k\)</span> is the dimensionality of keys/queries</li>
</ul>
<hr />
<h4 id="44-multi-head-attention">44. Multi-Head Attention</h4>
<p><span class="arithmatex">\(\text{MHA}(Q,K,V) = \big[\text{head}_1, \dots, \text{head}_h\big]W^O\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span></li>
<li><span class="arithmatex">\(W^O\)</span> is the output projection matrix</li>
<li><span class="arithmatex">\(h\)</span> is the number of heads</li>
</ul>
<hr />
<h4 id="45-feed-forward-transformer">45. Feed-Forward (Transformer)</h4>
<p><span class="arithmatex">\(\text{FFN}(x) = \max(0, xW_1 + b_1) \, W_2 + b_2\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(x\)</span> is the input</li>
<li><span class="arithmatex">\(W_1, b_1, W_2, b_2\)</span> are learned parameters</li>
<li><span class="arithmatex">\(\max(0,\cdot)\)</span> indicates ReLU (or another activation)</li>
</ul>
<hr />
<h4 id="46-positional-encoding-sinusoidal">46. Positional Encoding (Sinusoidal)</h4>
<p><span class="arithmatex">\(\text{PE}(pos, 2i) = \sin\big(pos / 10000^{2i/d_{\text{model}}}\big), \quad \text{PE}(pos, 2i+1) = \cos\big(pos / 10000^{2i/d_{\text{model}}}\big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(pos\)</span> is the position index</li>
<li><span class="arithmatex">\(i\)</span> is the dimension index</li>
<li><span class="arithmatex">\(d_{\text{model}}\)</span> is the model dimensionality</li>
</ul>
<hr />
<h4 id="47-perceptron-rule">47. Perceptron Rule</h4>
<p><span class="arithmatex">\(\hat{y} = 
\begin{cases}
1 &amp; \text{if } w^\top x + b \ge 0\\
0 &amp; \text{otherwise}
\end{cases}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(w\)</span> is the weight vector</li>
<li><span class="arithmatex">\(x\)</span> is the input vector</li>
<li><span class="arithmatex">\(b\)</span> is the bias term</li>
</ul>
<hr />
<h4 id="48-general-activation-forward">48. General Activation Forward</h4>
<p><span class="arithmatex">\(a = f(z)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z\)</span> is the pre-activation (weighted sum)</li>
<li><span class="arithmatex">\(f(\cdot)\)</span> is an activation function (e.g., sigmoid, ReLU)</li>
</ul>
<hr />
<h4 id="49-derivative-of-relu">49. Derivative of ReLU</h4>
<p><span class="arithmatex">\(\frac{d}{dz}\text{ReLU}(z) =
\begin{cases}
1 &amp; z &gt; 0\\
0 &amp; z \le 0
\end{cases}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z\)</span> is the input</li>
</ul>
<hr />
<h4 id="50-derivative-of-sigmoid">50. Derivative of Sigmoid</h4>
<p><span class="arithmatex">\(\sigma'(z) = \sigma(z)\big(1-\sigma(z)\big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\sigma(z)\)</span> is the sigmoid function</li>
</ul>
<hr />
<h4 id="51-derivative-of-tanh">51. Derivative of Tanh</h4>
<p><span class="arithmatex">\(\frac{d}{dz}\tanh(z) = 1 - \tanh^2(z)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\tanh(z)\)</span> is the hyperbolic tangent function</li>
</ul>
<hr />
<h4 id="52-derivative-of-softmax-vector-form">52. Derivative of Softmax (Vector Form)</h4>
<p><span class="arithmatex">\(\frac{\partial \hat{y}_j}{\partial z_k} = \hat{y}_j \big(\delta_{jk} - \hat{y}_k\big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\hat{y}_j\)</span> is the softmax output for class <span class="arithmatex">\(j\)</span></li>
<li><span class="arithmatex">\(z_k\)</span> is the logit for class <span class="arithmatex">\(k\)</span></li>
<li><span class="arithmatex">\(\delta_{jk}\)</span> is the Kronecker delta (1 if <span class="arithmatex">\(j=k\)</span>, 0 otherwise)</li>
</ul>
<hr />
<h4 id="53-derivative-of-cross-entropy-wrt-logits-softmax">53. Derivative of Cross-Entropy w.r.t. Logits (Softmax)</h4>
<p><span class="arithmatex">\(\frac{\partial L_{\text{CE}}}{\partial z_j} = \hat{y}_j - y_j\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\hat{y}_j\)</span> is the predicted probability (softmax output)</li>
<li><span class="arithmatex">\(y_j\)</span> is the one-hot target</li>
</ul>
<hr />
<h4 id="54-l2-distance-euclidean">54. L2 Distance (Euclidean)</h4>
<p><span class="arithmatex">\(d_{\text{Euclidean}}(x,y) = \sqrt{\sum_{i=1}^{d}(x_i - y_i)^2}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(x, y \in \mathbb{R}^d\)</span> are vectors</li>
<li><span class="arithmatex">\(d\)</span> is the number of dimensions</li>
</ul>
<hr />
<h4 id="55-manhattan-distance-l1">55. Manhattan Distance (L1)</h4>
<p><span class="arithmatex">\(d_{\text{Manhattan}}(x,y) = \sum_{i=1}^d |x_i - y_i|\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(x, y\)</span> are vectors in <span class="arithmatex">\(\mathbb{R}^d\)</span></li>
<li><span class="arithmatex">\(|\,\cdot\,|\)</span> is the absolute value</li>
</ul>
<hr />
<h4 id="56-cosine-similarity">56. Cosine Similarity</h4>
<p><span class="arithmatex">\(\text{cos\_sim}(x, y) = \frac{x \cdot y}{\|x\| \, \|y\|}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(x \cdot y\)</span> is the dot product</li>
<li><span class="arithmatex">\(\|x\|\)</span> is the norm of <span class="arithmatex">\(x\)</span></li>
</ul>
<hr />
<h4 id="57-dot-product">57. Dot Product</h4>
<p><span class="arithmatex">\(x \cdot y = \sum_{i=1}^{d} x_i \, y_i\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(x, y\)</span> are vectors in <span class="arithmatex">\(\mathbb{R}^d\)</span></li>
</ul>
<hr />
<h4 id="58-matrix-multiplication">58. Matrix Multiplication</h4>
<p><span class="arithmatex">\((AB)_{ij} = \sum_{k=1}^{n} A_{ik}\, B_{kj}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(A\)</span> is a <span class="arithmatex">\((m \times n)\)</span> matrix</li>
<li><span class="arithmatex">\(B\)</span> is a <span class="arithmatex">\((n \times p)\)</span> matrix</li>
<li><span class="arithmatex">\((AB)\)</span> is a <span class="arithmatex">\((m \times p)\)</span> matrix</li>
</ul>
<hr />
<h4 id="59-determinant-of-a-22-matrix">59. Determinant of a 2×2 Matrix</h4>
<p><span class="arithmatex">\(\det\begin{pmatrix}
a &amp; b\\
c &amp; d
\end{pmatrix} = ad - bc\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(a, b, c, d\)</span> are elements of the matrix</li>
</ul>
<hr />
<h4 id="60-matrix-inverse-22">60. Matrix Inverse (2×2)</h4>
<p><span class="arithmatex">\(\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}^{-1} 
= \frac{1}{ad - bc} \begin{pmatrix} d &amp; -b \\ -c &amp; a\end{pmatrix}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(a, b, c, d\)</span> are elements</li>
<li><span class="arithmatex">\(ad - bc \neq 0\)</span></li>
</ul>
<hr />
<h4 id="61-frobenius-norm">61. Frobenius Norm</h4>
<p><span class="arithmatex">\(\|A\|_F = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n} |A_{ij}|^2}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(A\)</span> is an <span class="arithmatex">\(m \times n\)</span> matrix</li>
<li><span class="arithmatex">\(A_{ij}\)</span> is the element in row <span class="arithmatex">\(i\)</span>, column <span class="arithmatex">\(j\)</span></li>
</ul>
<hr />
<h4 id="62-kl-divergence">62. KL Divergence</h4>
<p><span class="arithmatex">\(D_{\text{KL}}(P \| Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(Q\)</span> are discrete probability distributions</li>
<li>The sum is over all possible outcomes <span class="arithmatex">\(x\)</span></li>
</ul>
<hr />
<h4 id="63-jensen-shannon-divergence">63. Jensen-Shannon Divergence</h4>
<p><span class="arithmatex">\(D_{\text{JS}}(P\|Q) = \frac{1}{2} D_{\text{KL}}\big(P \| M\big) + \frac{1}{2} D_{\text{KL}}\big(Q \| M\big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(M = \frac{1}{2}(P + Q)\)</span></li>
<li><span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(Q\)</span> are probability distributions</li>
</ul>
<hr />
<h4 id="64-bayes-rule">64. Bayes’ Rule</h4>
<p><span class="arithmatex">\(P(A|B) = \frac{P(B|A)\,P(A)}{P(B)}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span> are events</li>
<li><span class="arithmatex">\(P(A)\)</span>, <span class="arithmatex">\(P(B)\)</span>, <span class="arithmatex">\(P(B|A)\)</span> are known or can be inferred</li>
</ul>
<hr />
<h4 id="65-covariance">65. Covariance</h4>
<p><span class="arithmatex">\(\text{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])]\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(E[\cdot]\)</span> denotes the expectation</li>
<li><span class="arithmatex">\(X, Y\)</span> are random variables</li>
</ul>
<hr />
<h4 id="66-pearson-correlation">66. Pearson Correlation</h4>
<p><span class="arithmatex">\(\rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\sigma_X\)</span> and <span class="arithmatex">\(\sigma_Y\)</span> are standard deviations of <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span></li>
</ul>
<hr />
<h4 id="67-pca-covariance-eigen-decomposition">67. PCA Covariance Eigen-Decomposition</h4>
<p><span class="arithmatex">\(C = U \Lambda U^\top\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(C\)</span> is the covariance matrix</li>
<li><span class="arithmatex">\(U\)</span> is the matrix of eigenvectors</li>
<li><span class="arithmatex">\(\Lambda\)</span> is the diagonal matrix of eigenvalues</li>
</ul>
<hr />
<h4 id="68-singular-value-decomposition-svd">68. Singular Value Decomposition (SVD)</h4>
<p><span class="arithmatex">\(A = U \Sigma V^\top\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(A\)</span> is an <span class="arithmatex">\((m \times n)\)</span> matrix</li>
<li><span class="arithmatex">\(U\)</span> and <span class="arithmatex">\(V\)</span> are orthonormal matrices</li>
<li><span class="arithmatex">\(\Sigma\)</span> is the diagonal matrix of singular values</li>
</ul>
<hr />
<h4 id="69-gaussian-distribution-1d">69. Gaussian Distribution (1D)</h4>
<p><span class="arithmatex">\(\mathcal{N}(x \mid \mu,\sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\Big(-\frac{(x-\mu)^2}{2\sigma^2}\Big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\mu\)</span> is the mean</li>
<li><span class="arithmatex">\(\sigma^2\)</span> is the variance</li>
</ul>
<hr />
<h4 id="70-probability-integral-discrete">70. Probability Integral (Discrete)</h4>
<p><span class="arithmatex">\(\sum_{x} P(x) = 1\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(x\)</span> ranges over all possible outcomes</li>
<li><span class="arithmatex">\(P(x)\)</span> is a probability distribution</li>
</ul>
<hr />
<h4 id="71-probability-integral-continuous">71. Probability Integral (Continuous)</h4>
<p><span class="arithmatex">\(\int_{-\infty}^{\infty} p(x)\, dx = 1\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(p(x)\)</span> is a probability density function</li>
</ul>
<hr />
<h4 id="72-exponential-family-general-form">72. Exponential Family (General Form)</h4>
<p><span class="arithmatex">\(p(x|\theta) = h(x) \exp\big(\eta(\theta)^\top T(x) - A(\theta)\big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(h(x)\)</span>, <span class="arithmatex">\(T(x)\)</span> are known functions</li>
<li><span class="arithmatex">\(\eta(\theta)\)</span> is the natural parameter</li>
<li><span class="arithmatex">\(A(\theta)\)</span> is the log-partition function</li>
</ul>
<hr />
<h4 id="73-weighted-adjacency-graph-neural-network">73. Weighted Adjacency (Graph Neural Network)</h4>
<p><span class="arithmatex">\(h_v^{(l+1)} = \sigma\Bigg(\sum_{u \in \mathcal{N}(v)} \alpha_{vu} \, W^{(l)} h_u^{(l)}\Bigg)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(h_v^{(l)}\)</span> is the hidden representation of node <span class="arithmatex">\(v\)</span> at layer <span class="arithmatex">\(l\)</span></li>
<li><span class="arithmatex">\(\alpha_{vu}\)</span> is the attention or adjacency weight from node <span class="arithmatex">\(u\)</span> to <span class="arithmatex">\(v\)</span></li>
<li><span class="arithmatex">\(W^{(l)}\)</span> is a trainable weight matrix</li>
<li><span class="arithmatex">\(\sigma\)</span> is an activation function</li>
</ul>
<hr />
<h4 id="74-gnn-aggregation">74. GNN Aggregation</h4>
<p><span class="arithmatex">\(h_v^{(l+1)} = \text{AGGREGATE}\big(\{\,h_u^{(l)} : u \in \mathcal{N}(v)\}\big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\mathcal{N}(v)\)</span> is the set of neighbors of node <span class="arithmatex">\(v\)</span></li>
<li><span class="arithmatex">\(\text{AGGREGATE}(\cdot)\)</span> could be sum, mean, max, or an attention-based mechanism</li>
</ul>
<hr />
<h4 id="75-rmsprop-repeated-for-emphasis">75. RMSProp (Repeated for Emphasis)</h4>
<p><span class="arithmatex">\(\theta \leftarrow \theta - \frac{\eta}{\sqrt{E[g^2] + \epsilon}} \nabla_\theta L(\theta)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\theta\)</span> is the parameter vector</li>
<li><span class="arithmatex">\(\eta\)</span> is the learning rate</li>
<li><span class="arithmatex">\(E[g^2]\)</span> is the running average of squared gradients</li>
<li><span class="arithmatex">\(\epsilon\)</span> prevents division by zero</li>
</ul>
<hr />
<h4 id="76-weighted-combination-for-multi-head">76. Weighted Combination for Multi-Head</h4>
<p><span class="arithmatex">\(\text{head}_i = \text{Attention}(QW_i^Q, \; KW_i^K, \; VW_i^V)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(Q, K, V\)</span> are query, key, and value matrices</li>
<li><span class="arithmatex">\(W_i^Q, W_i^K, W_i^V\)</span> are parameter matrices for head <span class="arithmatex">\(i\)</span></li>
</ul>
<hr />
<h4 id="77-gradient-of-mse-wrt-weights">77. Gradient of MSE w.r.t. Weights</h4>
<p><span class="arithmatex">\(\frac{\partial L_{\text{MSE}}}{\partial w_j} = \frac{1}{N}\sum_{i=1}^N 2 \,(\hat{y}^{(i)} - y^{(i)}) \, x_j^{(i)}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(w_j\)</span> is weight <span class="arithmatex">\(j\)</span></li>
<li><span class="arithmatex">\(x_j^{(i)}\)</span> is the <span class="arithmatex">\(j\)</span>-th feature of sample <span class="arithmatex">\(i\)</span></li>
</ul>
<hr />
<h4 id="78-gradient-of-binary-cross-entropy-wrt-weights">78. Gradient of Binary Cross-Entropy w.r.t. Weights</h4>
<p><span class="arithmatex">\(\frac{\partial L_{\text{CE}}}{\partial w_j} = \frac{1}{N} \sum_{i=1}^N (\hat{y}^{(i)} - y^{(i)})\, x_j^{(i)}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\hat{y}^{(i)}\)</span> is the predicted probability</li>
<li><span class="arithmatex">\(y^{(i)}\)</span> is the true label</li>
</ul>
<hr />
<h4 id="79-logistic-regression-decision-boundary">79. Logistic Regression Decision Boundary</h4>
<p><span class="arithmatex">\(\hat{y} = \sigma(w^\top x + b) \implies 
\text{Decision} = \begin{cases}
1 &amp; \text{if } w^\top x + b \ge 0\\
0 &amp; \text{otherwise}
\end{cases}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(w, b\)</span> are parameters</li>
<li><span class="arithmatex">\(x\)</span> is the input</li>
<li><span class="arithmatex">\(\sigma\)</span> is the sigmoid function</li>
</ul>
<hr />
<h4 id="80-balanced-cross-entropy">80. Balanced Cross-Entropy</h4>
<p><span class="arithmatex">\(L_{\text{bal}} = -\frac{1}{N} \sum_{i=1}^N \Big[\beta \, y^{(i)} \log(\hat{y}^{(i)}) + (1-\beta)(1-y^{(i)}) \log(1-\hat{y}^{(i)})\Big]\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\beta\)</span> is a factor weighting the positive class</li>
<li><span class="arithmatex">\(y^{(i)}, \hat{y}^{(i)}\)</span> as before</li>
</ul>
<hr />
<h4 id="81-f1-score">81. F1 Score</h4>
<p><span class="arithmatex">\(\text{F1} = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\text{precision} = \frac{TP}{TP+FP}\)</span></li>
<li><span class="arithmatex">\(\text{recall} = \frac{TP}{TP+FN}\)</span></li>
</ul>
<hr />
<h4 id="82-precision">82. Precision</h4>
<p><span class="arithmatex">\(\text{precision} = \frac{TP}{TP + FP}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(TP\)</span> = true positives</li>
<li><span class="arithmatex">\(FP\)</span> = false positives</li>
</ul>
<hr />
<h4 id="83-recall">83. Recall</h4>
<p><span class="arithmatex">\(\text{recall} = \frac{TP}{TP + FN}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(TP\)</span> = true positives</li>
<li><span class="arithmatex">\(FN\)</span> = false negatives</li>
</ul>
<hr />
<h4 id="84-auc-area-under-roc-curve">84. AUC (Area Under ROC Curve)</h4>
<p><span class="arithmatex">\(\text{AUC} = \int_0^1 TPR(\text{FPR}^{-1}(x))\, dx\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(TPR\)</span> is the true positive rate</li>
<li><span class="arithmatex">\(FPR\)</span> is the false positive rate</li>
<li>The integral is conceptually the area under the ROC curve</li>
</ul>
<hr />
<h4 id="85-step-decay-learning-rate-scheduling">85. Step Decay (Learning Rate Scheduling)</h4>
<p><span class="arithmatex">\(\eta_{\text{new}} = \eta_{\text{old}} \cdot \gamma^{\lfloor \frac{\text{epoch}}{k}\rfloor}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\eta_{\text{old}}\)</span> is the current learning rate</li>
<li><span class="arithmatex">\(\eta_{\text{new}}\)</span> is the updated learning rate</li>
<li><span class="arithmatex">\(\gamma \in (0,1)\)</span> is the decay factor</li>
<li><span class="arithmatex">\(k\)</span> is the step size in epochs</li>
</ul>
<hr />
<h4 id="86-polynomial-kernel-svm">86. Polynomial Kernel (SVM)</h4>
<p><span class="arithmatex">\(K(x, x') = (x^\top x' + c)^p\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(x, x'\)</span> are feature vectors</li>
<li><span class="arithmatex">\(c\)</span> is a constant (often 1)</li>
<li><span class="arithmatex">\(p\)</span> is the polynomial degree</li>
</ul>
<hr />
<h4 id="87-radial-basis-function-kernel-rbf">87. Radial Basis Function Kernel (RBF)</h4>
<p><span class="arithmatex">\(K(x, x') = \exp\Big(-\frac{\|x - x'\|^2}{2\sigma^2}\Big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(x, x'\)</span> are feature vectors</li>
<li><span class="arithmatex">\(\sigma\)</span> is the kernel width</li>
</ul>
<hr />
<h4 id="88-hinge-loss-svm">88. Hinge Loss (SVM)</h4>
<p><span class="arithmatex">\(L_{\text{hinge}} = \max\big(0, 1 - y_i (w^\top x_i + b)\big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(y_i \in \{-1, +1\}\)</span> is the true label</li>
<li><span class="arithmatex">\(w\)</span> is the weight vector</li>
<li><span class="arithmatex">\(x_i\)</span> is the input</li>
<li><span class="arithmatex">\(b\)</span> is the bias</li>
</ul>
<hr />
<h4 id="89-margin-hard-svm">89. Margin (Hard-SVM)</h4>
<p><span class="arithmatex">\(\text{Margin} = \frac{2}{\|w\|}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(w\)</span> is the normal vector to the decision boundary</li>
<li><span class="arithmatex">\(\|w\|\)</span> is the Euclidean norm of <span class="arithmatex">\(w\)</span></li>
</ul>
<hr />
<h4 id="90-q-learning-update-rl">90. Q-Learning Update (RL)</h4>
<p><span class="arithmatex">\(Q(s,a) \leftarrow Q(s,a) + \alpha \Big[r + \gamma \max_{a'} Q(s', a') - Q(s,a)\Big]\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(Q(s,a)\)</span> is the current Q-value</li>
<li><span class="arithmatex">\(\alpha\)</span> is the learning rate</li>
<li><span class="arithmatex">\(r\)</span> is the reward</li>
<li><span class="arithmatex">\(\gamma\)</span> is the discount factor</li>
<li><span class="arithmatex">\(s'\)</span> is the next state</li>
<li><span class="arithmatex">\(a'\)</span> is the next action</li>
</ul>
<hr />
<h4 id="91-bellman-equation-value-function">91. Bellman Equation (Value Function)</h4>
<p><span class="arithmatex">\(V^\pi(s) = \mathbb{E}_{a\sim \pi}\Big[r(s,a) + \gamma V^\pi(s')\Big]\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(V^\pi(s)\)</span> is the value of state <span class="arithmatex">\(s\)</span> under policy <span class="arithmatex">\(\pi\)</span></li>
<li><span class="arithmatex">\(r(s,a)\)</span> is the immediate reward</li>
<li><span class="arithmatex">\(\gamma\)</span> is the discount factor</li>
<li><span class="arithmatex">\(s'\)</span> is the next state</li>
</ul>
<hr />
<h4 id="92-actor-critic-gradient">92. Actor-Critic Gradient</h4>
<p><span class="arithmatex">\(\nabla_\theta J(\theta) = \mathbb{E}\big[\nabla_\theta \log \pi_\theta(a|s)\, (Q_w(s,a) - b(s))\big]\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\pi_\theta\)</span> is the policy (actor)</li>
<li><span class="arithmatex">\(Q_w(s,a)\)</span> is the critic’s estimate of action-value</li>
<li><span class="arithmatex">\(b(s)\)</span> is a baseline to reduce variance</li>
</ul>
<hr />
<h4 id="93-softplus-activation">93. Softplus Activation</h4>
<p><span class="arithmatex">\(\text{Softplus}(z) = \log(1 + e^z)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z\)</span> is the input</li>
</ul>
<hr />
<h4 id="94-elu-exponential-linear-unit">94. ELU (Exponential Linear Unit)</h4>
<p><span class="arithmatex">\(\text{ELU}(z) =
\begin{cases}
z &amp; z \ge 0,\\
\alpha(e^z - 1) &amp; z &lt; 0
\end{cases}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\alpha\)</span> is a positive constant</li>
</ul>
<hr />
<h4 id="95-selu-scaled-elu">95. SELU (Scaled ELU)</h4>
<p><span class="arithmatex">\(\text{SELU}(z) = \lambda \begin{cases}
z &amp; z \ge 0\\
\alpha(e^z - 1) &amp; z &lt; 0
\end{cases}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\alpha\)</span> and <span class="arithmatex">\(\lambda\)</span> are constants (e.g., <span class="arithmatex">\(\alpha \approx 1.673\)</span>, <span class="arithmatex">\(\lambda \approx 1.051\)</span>)</li>
</ul>
<hr />
<h4 id="96-knowledge-distillation">96. Knowledge Distillation</h4>
<p><span class="arithmatex">\(L_{\text{KD}} = \tau^2 \cdot \text{KL}\big(p_\text{teacher}(x;\tau)\,\|\,p_\text{student}(x;\tau)\big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\tau\)</span> is the temperature</li>
<li><span class="arithmatex">\(p_\text{teacher}\)</span> and <span class="arithmatex">\(p_\text{student}\)</span> are softmax outputs of teacher and student networks, respectively</li>
</ul>
<hr />
<h4 id="97-threshold-function-binary-step">97. Threshold Function (Binary Step)</h4>
<p><span class="arithmatex">\(\theta(z) =
\begin{cases}
1 &amp; z \ge 0\\
0 &amp; z &lt; 0
\end{cases}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z\)</span> is the input</li>
</ul>
<hr />
<h4 id="98-weighted-hinge-loss">98. Weighted Hinge Loss</h4>
<p><span class="arithmatex">\(L_{\text{whinge}} = w_i \,\max\big(0, 1 - y_i (w^\top x_i + b)\big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(w_i\)</span> is the sample weight or class weight</li>
<li><span class="arithmatex">\(y_i \in \{-1, +1\}\)</span> is the true label</li>
</ul>
<hr />
<h4 id="99-perplexity-language-modeling">99. Perplexity (Language Modeling)</h4>
<p><span class="arithmatex">\(\text{PPL} = \exp\Big(\frac{1}{N}\sum_{i=1}^{N} -\log p(w_i)\Big)\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(w_i\)</span> are tokens in the sequence</li>
<li><span class="arithmatex">\(p(w_i)\)</span> is the predicted probability of token <span class="arithmatex">\(w_i\)</span></li>
<li><span class="arithmatex">\(N\)</span> is the total number of tokens</li>
</ul>
<hr />
<h4 id="100-cycle-consistency-loss-cyclegan">100. Cycle Consistency Loss (CycleGAN)</h4>
<p><span class="arithmatex">\(L_{\text{cyc}}(G,F) = \mathbb{E}_{x \sim p(x)}\big[\|F(G(x)) - x\|\big] + \mathbb{E}_{y \sim p(y)}\big[\|G(F(y)) - y\|\big]\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(G\)</span> is the generator mapping domain <span class="arithmatex">\(X \to Y\)</span></li>
<li><span class="arithmatex">\(F\)</span> is the generator mapping domain <span class="arithmatex">\(Y \to X\)</span></li>
<li><span class="arithmatex">\(x, y\)</span> are samples from domains <span class="arithmatex">\(X, Y\)</span></li>
<li><span class="arithmatex">\(\|\cdot\|\)</span> is a distance measure (e.g., L1 norm)</li>
</ul>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../06-glossary-of-terms/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Glossary of Terms">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Glossary of Terms
              </div>
            </div>
          </a>
        
        
          
          <a href="../08-word-cloud/" class="md-footer__link md-footer__link--next" aria-label="Next: Word Cloud">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Word Cloud
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "navigation.expand", "navigation.path", "navigation.prune", "navigation.indexes", "toc.follow", "navigation.top", "navigation.footer", "content.action.edit"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>