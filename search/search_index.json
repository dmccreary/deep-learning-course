{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Deep Learning with AI","text":"<p>This website contains resources for teaching a college-level course on Deep Learning with AI.  We assume that both instructors and students have access to generative AI tools such as ChatGPT, Anthropic Claude or Ollama.</p> <p>Please contact me on LinkedIn if you have any questions about the course.</p>"},{"location":"about/","title":"About this Website","text":"<p>This website contains resources for teaching a college-level course on Deep Learning.  We assume that both instructors and students have access to generative AI tools such as ChatGPT, Anthropic Claude or Ollama.  Understanding prompt engineering is key to generating high quality Python code, test cases and acceptance test plans.</p>"},{"location":"about/#why-this-site-why-now","title":"Why This Site?  Why Now?","text":"<p>Deep learning has become an indispensable pillar of modern computer science and electrical engineering, fundamentally transforming how we approach complex problems across virtually every technical domain. For CS and EE students, understanding deep learning is no longer optional \u2013 it's a critical skill that will shape their future careers and their ability to innovate in their fields.</p> <p>Consider how deep learning has revolutionized traditional EE and CS domains: In electrical engineering, deep learning has dramatically improved signal processing, circuit design optimization, and power systems management. EE students who understand deep learning can develop more efficient hardware architectures, design better communication systems, and create more sophisticated embedded systems that can adapt to real-world conditions.</p> <p>For computer science students, deep learning has become essential for solving complex problems in computer vision, natural language processing, and autonomous systems. The ability to design and implement neural networks is now as fundamental as understanding data structures and algorithms.</p> <p>This intelligent textbook is an excellent resource for instructors, as it presents a comprehensive taxonomy of deep learning concepts. Based on the structure I can see in the data (with ConceptIDs, concept labels, and dependencies), it seems to offer a well-organized progression through the material that helps students build their knowledge systematically.</p> <p>What makes deep learning particularly crucial for today's students is its rapid evolution and widespread adoption across industries. From healthcare to autonomous vehicles, from financial systems to renewable energy optimization \u2013 virtually every sector now leverages deep learning. Students who graduate without this knowledge risk falling behind in an increasingly AI-driven job market.</p> <p>Moreover, deep learning represents a fundamental shift in how we approach problem-solving in computing. Rather than explicitly programming solutions, students learn to design systems that can learn from data \u2013 a paradigm shift that will only become more important as the complexity of our technical challenges grows.</p>"},{"location":"checklist/","title":"Site Checklist","text":"<ol> <li>Customize the fields in your mkdocs.yml file</li> <li>Configure Google Analytics to use the right site ID</li> <li>Make sure that your .gitignore file includes the <code>site</code> directory</li> <li>Test the build</li> <li>Make sure the Edit button appears</li> <li>Make sure that code color heightening renders correctly</li> <li>run <code>git config advice.addIgnoredFile false</code></li> </ol>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Course Description","text":"<p>Organization: University of Minnesota Department of Electrical and Computer Engineering (ECE) Course Identifier: EE 4940 Sequence Name: Special Topics in Electrical and Computer Engineering (1-4 Cr.) Offering Course Title: : Introduction to Deep Learning Offering Date: Spring 2025</p>"},{"location":"course-description/#course-instructor-ta-and-format","title":"Course Instructor, TA and Format","text":"<p>Sang-Hyun Oh, Professor of ECE, graduate faculty in neuroscience, physics, biomedical eng. Email: sang@umn.edu Office: 5-119 Keller Hall Teaching Assistant: Ike Goetzman (PhD candidate in ECE; email: goetz219@umn.edu) Format: Lecture (Jan 22 \u2013 May 5, 2024): Monday/Wednesday 11:15am-12:30pm @ Appleby Hall 3</p>"},{"location":"course-description/#course-summary","title":"Course Summary","text":"<p>Machine learning, especially deep learning based on neural networks, has shown transformative impacts across life sciences, engineering, and medicine, as recently highlighted by the 2024 Nobel Prizes: </p> <ul> <li>The Physics Nobel was awarded to John Hopfield and Geoffrey Hinton for pioneering contributions to neural networks that enable machine learning</li> <li>Demis Hassabis, John Jumper, and David Baker received the Chemistry Nobel for advancing computational protein design.</li> </ul>"},{"location":"course-description/#course-overview","title":"Course Overview","text":"<p>EE 4940: Introduction to Deep Learning aims to provide students with both fundamental and applied perspectives on these advances. Beginning with an overview of AI, the course introduces deep neural networks (DNNs) through practical, hands-on examples and guides students through specialized neural network architectures, including</p> <ul> <li>convolutional neural networks (CNNs) for diagnostic applications and </li> <li>recurrent neural networks (RNNs) for time series analysis.</li> </ul> <p>Emphasis is placed on applying neural networks to tackle real-world problems in classification, regression, image analysis, and time-series prediction. Scientific applications are demonstrated as well, such as using DNNs to solve equations.</p> <p>The course concludes with dimensionality reduction techniques, followed by advanced topics such as generative AI, and an introduction to reinforcement learning. </p> <p>Emphasis will be on practical experience with Python-based coding over mathematical theory.</p> <p>By the end of the course, students will have gained familiarity with Python-driven deep learning, hands-on skills, and a working knowledge of deep learning\u2019s potential for a wide range of problems.</p>"},{"location":"course-description/#course-prerequisites","title":"Course Prerequisites","text":"<p>EE 4940 - Special Topics in Electrical and Computer Engineering is recommended but not required.</p> <p>This course is designed to be accessible and introductory, with essential concepts and coding skills introduced as needed.</p>"},{"location":"course-description/#expected-workload","title":"Expected Workload","text":"<p>This course (EE 4940) is a 3-credit, lecture format course. The expected workload for this course this semester is, on average, 45 hours per credit, or just over 3 hours per credit hour per week (including lecture). The total workload of 9 hours per week.</p>"},{"location":"course-description/#course-goals-and-objectives","title":"Course Goals and Objectives","text":"<p>EE 4940 aims to equip students with a foundation in deep learning, focusing on core architectures like deep neural networks (DNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).</p> <p>Through hands-on applications, students will learn to apply these models to solve practical problems using real-world datasets, such as cancer diagnostics and time-series analysis. The primary goal of the course is to develop students's ability to implement and troubleshoot neural network models, gaining some practical experiences, and be aware of potential pitfalls/artifacts while using DNNs.</p> <p>By the course's end, students will have a working knowledge of AI\u2019s role in scientific problem-solving, dimensionality reduction techniques, and a simple introduction to generative models and reinforcement learning (RL).</p>"},{"location":"course-description/#topics-to-be-covered","title":"Topics To Be Covered","text":""},{"location":"course-description/#1-introduction-to-ai-and-deep-learning","title":"1. Introduction to AI and deep learning","text":"<ul> <li>Brief history of AI, neural networks and recent Nobel-winning breakthroughs in physics and chemistry</li> <li>Course setup, tools, and brief introduction to Python for deep learning</li> <li>The universality theorem and the manifold hypothesis</li> </ul>"},{"location":"course-description/#2-fundamentals-of-deep-neural-networks-dnns","title":"2. Fundamentals of Deep Neural Networks (DNNs)","text":"<ul> <li>Architecture of DNNs: layers, activation functions, and backpropagation</li> <li>Training, validation, and testing sets;</li> <li>Loss functions, gradient descent, mini-batches</li> <li>Hands-on coding</li> </ul>"},{"location":"course-description/#3-convolutional-neural-networks-cnns-applications-in-diagnostic-imaging","title":"3. Convolutional Neural Networks (CNNs) - applications in diagnostic imaging","text":"<ul> <li>CNN architecture</li> <li>Applying CNNs, with MHIST as a case study for cancer diagnosis</li> </ul>"},{"location":"course-description/#4-recurrent-neural-networks-rnns-and-time-series-analysis","title":"4. Recurrent Neural Networks (RNNs) and time series analysis","text":"<ul> <li>Introduction to RNNs, LSTMs, for sequency data analysis.</li> <li>Hands-on practice with real-world data.</li> </ul>"},{"location":"course-description/#5-chips-and-ai","title":"5. Chips and AI","text":"<p>Briefly explore AI chips, neuromorphic devices, and future directions.</p>"},{"location":"course-description/#6-ai-for-scientific-problem-solving-using-neural-networks-to-approximate-solutions-for","title":"6. AI for scientific problem solving: Using neural networks to approximate solutions for","text":"<p>Maxwell\u2019s equations</p>"},{"location":"course-description/#7-dimensionality-reduction-and-autoencoders","title":"7. Dimensionality reduction and autoencoders","text":""},{"location":"course-description/#8-advanced-topics","title":"8. Advanced topics","text":""},{"location":"course-description/#course-format","title":"Course Format","text":"<ul> <li>Classroom lectures only</li> <li>Canvas LMS link will be provided</li> <li>Textbook and Required Materials: none</li> <li>Based on lecture notes (uploaded in canvas 1-2 days before class)</li> </ul>"},{"location":"course-description/#homework-assignments-and-mini-projects","title":"Homework Assignments and \u201cMini-projects\u201d:","text":"<ul> <li>Regularly-scheduled weekly or biweekly homework assignments and 4 multi-week \u201cmini-projects.\u201d The former will be designed to exercise basic concepts and modest extensions of the technical or computational tools introduced in class; the latter will be crafted to allow students to take a \u201cdeeper dive\u201d by applying techniques developed in the course to data sets arising in various aspects of ECE and life sciences.</li> <li>You are allowed (in fact, encouraged) to work with others when solving the homework and mini project problems. However, solutions submitted for grading should be formulated by each student independently! Late homework and mini-project submissions will be accepted, but without a legitimate (e.g., illness-based) justification, will result in a maximum grade of 80% for that assignment. Specifically, unjustified late assignments will be graded according to the same rubric as on-time assignments, but the corresponding score will be multiplied by 0.8 when entered into the gradebook.</li> </ul>"},{"location":"course-description/#exams","title":"Exams","text":"<p>There will be one midterm exam and one final exam. The midterm exam will be open book/notes. The final exam will be open book/notes and will be due no earlier than the final exam time specified by the Registrar. Either or both may be take-home exams.</p>"},{"location":"course-description/#grading","title":"Grading","text":"<p>The grade distributions for the course variations are provided below.</p> <p>Note: distributions for homework assignments and mini-projects are total; individual submissions will be equally weighted.</p> Deliverable Weight Homework Assignments 40% Mini-projects 40% Midterm Exam 10% Final Exam 10%"},{"location":"course-description/#software-requirements","title":"Software Requirements","text":"<p>Through the homework assignments and mini-projects, students will be required to perform a number of computer exercises to become familiar with the computational and practical aspects of deep learning. This will typically entail utilization of Python, often in concert with existing toolboxes such as NumPy, SciPy, Matplotlib, Scikit-learn, and PyTorch (or TensorFlow), all of which are freely available for Windows, Mac, and Linux operating systems.</p> <p>As a perhaps preferred alternative, students may wish to perform all (Python) programming computations using a free cloud-based account (i.e., Google Colab), which will be discussed and established at the beginning of the semester. Large-scale deep learning computations necessary for the final \u201cmini-project\u201d will utilize these resources.</p> <p>Note</p> <p>The Required University Policy Statements were provided in the original PDF of the course description.  They have been removed to allow the generative AI tools to focus on the concept enumeration.</p>"},{"location":"feedback/","title":"Feedback on Graph Data Modeling","text":"<p>You are welcome to connect with me on anytime on LinkedIn or submit any issues to GitHub Issue Log.  All pull-requests with fixes to errors or additions are always welcome.</p> <p>If you would like to fill out a short survey and give us ideas on how we can create better tools for intelligent textbooks in the future.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":"<p>Below is an alphabetical glossary of 200 key concepts, each defined according to ISO 11179 guidelines. When relevant, an example of how the term is used in a Deep Learning context is provided.  A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"glossary/#accuracy","title":"Accuracy","text":"<p>A performance metric that measures the proportion of correct predictions out of all predictions made by a model. Example: In a binary classification problem, accuracy is the ratio of true positives and true negatives to the total sample size.</p>"},{"location":"glossary/#activation-functions","title":"Activation Functions","text":"<p>Mathematical functions applied to neuron outputs that introduce non-linearity into a neural network. Example: ReLU and Sigmoid are common activation functions used to help networks learn complex patterns.</p>"},{"location":"glossary/#actor-critic","title":"Actor-Critic","text":"<p>A class of reinforcement learning algorithms that maintain both a policy (actor) and a value function (critic) to guide action selection. Example: The actor-critic approach is used to stabilize training by combining policy gradient methods with value-based techniques.</p>"},{"location":"glossary/#adagrad","title":"AdaGrad","text":"<p>An adaptive gradient algorithm that individually scales learning rates based on the historical gradient magnitudes for each parameter. Example: AdaGrad can help train models more efficiently in scenarios where some parameters require more frequent updates than others.</p>"},{"location":"glossary/#adam-optimizer","title":"Adam Optimizer","text":"<p>An optimization algorithm that combines ideas from momentum (exponential moving averages of gradients) and AdaGrad (adaptive learning rates). Example: Adam is often the default choice for training deep neural networks due to its speed and stable convergence properties.</p>"},{"location":"glossary/#adversarial-examples","title":"Adversarial Examples","text":"<p>Inputs to a model that have been intentionally crafted to cause incorrect predictions, despite appearing normal to humans. Example: Slightly perturbed images that force a well-trained image classifier to misclassify a stop sign as a yield sign.</p>"},{"location":"glossary/#agile-methods","title":"Agile Methods","text":"<p>Project management practices emphasizing iterative development, collaboration, and adaptability. Example: In a machine learning project, sprints are used to rapidly prototype models and incorporate stakeholder feedback.</p>"},{"location":"glossary/#ai-branches","title":"AI Branches","text":"<p>Various subfields of Artificial Intelligence, including symbolic AI, data-driven AI, evolutionary computation, and others. Example: A course might compare symbolic reasoning systems with modern deep learning architectures.</p>"},{"location":"glossary/#ai-definition","title":"AI Definition","text":"<p>The study and development of machines capable of performing tasks that typically require human intelligence. Example: AI encompasses problem-solving, perception, language understanding, and learning processes.</p>"},{"location":"glossary/#attention","title":"Attention","text":"<p>The area of a subject that has a high weight when classifying the subject.</p> <p>The paper \"Attention is All You Need\" was the name of the landmark paper on Transformers.</p>"},{"location":"glossary/#automl","title":"AutoML","text":"<p>Automated Machine Learning techniques that handle tasks such as model selection, hyperparameter tuning, and feature engineering with minimal human intervention. Example: Tools like Google AutoML or AutoKeras can automatically try multiple pipelines and present the best-performing model.</p>"},{"location":"glossary/#autoencoder-dr","title":"Autoencoder DR","text":"<p>Dimensionality reduction technique that uses an autoencoder neural network to learn a compressed representation of data. Example: An autoencoder might reduce a 1000-dimensional dataset down to 50 dimensions for visualization.</p>"},{"location":"glossary/#autoencoders-ae","title":"Autoencoders (AE)","text":"<p>Neural network architectures designed to learn a compressed representation (encoding) of input data, then reconstruct it (decoding). Example: AEs are used to denoise images or perform feature extraction for anomaly detection.</p>"},{"location":"glossary/#autonomous-driving","title":"Autonomous Driving","text":"<p>The application of AI/ML methods to enable vehicles to sense their environment and operate without human intervention. Example: Deep neural networks can process camera and lidar data in real-time for self-driving car navigation.</p>"},{"location":"glossary/#auc-roc","title":"AUC-ROC","text":"<p>A performance metric summarizing the trade-off between true positive rate and false positive rate across different thresholds. Example: The area under the ROC curve (AUC) is often used to compare classifiers independent of a specific decision threshold.</p>"},{"location":"glossary/#backprop-through-time-bptt","title":"Backprop Through Time (BPTT)","text":"<p>A method for training recurrent neural networks by unrolling the network for a certain number of timesteps and applying backpropagation. Example: LSTM networks rely on BPTT to learn dependencies in sequential data such as text or time series.</p>"},{"location":"glossary/#backpropagation","title":"Backpropagation","text":"<p>The algorithm used to calculate gradients of error with respect to each weight in a neural network, enabling efficient training. Example: During backprop, partial derivatives of the loss function are propagated backward to update model parameters.</p>"},{"location":"glossary/#baseline-algorithms","title":"Baseline Algorithms","text":"<p>Simple or well-understood methods used as reference points for performance comparison. Example: A linear regression might serve as a baseline for a more complex deep model on a regression task.</p>"},{"location":"glossary/#batch-normalization","title":"Batch Normalization","text":"<p>A technique to normalize intermediate layer outputs to accelerate training and stabilize learning in neural networks. Example: By normalizing activations, batch norm helps reduce internal covariate shift.</p>"},{"location":"glossary/#batch-training","title":"Batch Training","text":"<p>A training paradigm where the model parameters are updated after processing an entire batch of training examples. Example: Large datasets are often split into batches to efficiently utilize GPU memory.</p>"},{"location":"glossary/#batchnorm","title":"BatchNorm","text":"<p>Another name for batch normalization, focusing on normalizing activations per mini-batch. Example: \u201cBatchNorm\u201d layers are common in architectures like ResNet and Inception.</p>"},{"location":"glossary/#bayesian-optimization","title":"Bayesian Optimization","text":"<p>A global optimization strategy using Bayesian inference to intelligently sample hyperparameters and find optimal configurations. Example: Given a function that\u2019s expensive to evaluate (e.g., a deep network training loop), Bayesian optimization tries to minimize the total evaluations required.</p>"},{"location":"glossary/#bert-overview","title":"BERT Overview","text":"<p>Description of Bidirectional Encoder Representations from Transformers, a popular transformer-based NLP model that learns context in both directions. Example: BERT is used to improve tasks like question-answering and sentence classification.</p>"},{"location":"glossary/#bias-mitigation","title":"Bias Mitigation","text":"<p>Techniques to reduce or remove unwanted bias in AI models, ensuring fair and equitable outcomes. Example: Reweighting training data to address underrepresentation of certain groups.</p>"},{"location":"glossary/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>The balance between the accuracy of a model (bias) and its sensitivity to small fluctuations in the training set (variance). Example: A highly complex model might overfit (low bias, high variance), while a simple model might underfit (high bias, low variance).</p>"},{"location":"glossary/#biological-inspiration","title":"Biological Inspiration","text":"<p>The design of AI techniques based on biological systems or processes, such as neural networks modeled after the human brain. Example: Convolutional neural networks are partially inspired by the visual cortex in mammals.</p>"},{"location":"glossary/#bleu-score","title":"BLEU Score","text":"<p>Bilingual Evaluation Understudy; a metric used to evaluate the quality of machine-translated text against reference translations. Example: BLEU is commonly reported in NLP tasks such as neural machine translation experiments.</p>"},{"location":"glossary/#capsule-networks","title":"Capsule Networks","text":"<p>Neural architectures that group neurons into \u201ccapsules\u201d to better capture positional and hierarchical relationships in data. Example: Capsule networks aim to preserve spatial relationships more effectively than traditional CNNs.</p>"},{"location":"glossary/#chatgpt-concept","title":"ChatGPT Concept","text":"<p>A high-level idea of OpenAI\u2019s ChatGPT, which uses large language models to generate context-aware text responses. Example: Students explore ChatGPT\u2019s approach to answer generation and how it leverages conversational context.</p>"},{"location":"glossary/#cloud-services","title":"Cloud Services","text":"<p>Remote computing resources (servers, storage, APIs) for hosting, training, and deploying AI models. Example: AWS, Azure, and Google Cloud offer GPU/TPU instances to accelerate model training.</p>"},{"location":"glossary/#cnn-architectures","title":"CNN Architectures","text":"<p>Convolutional Neural Network designs specialized for handling spatial data such as images. Example: Classic architectures include LeNet, AlexNet, VGG, ResNet, and DenseNet.</p>"},{"location":"glossary/#code-review","title":"Code Review","text":"<p>The process of examining and discussing code changes to improve code quality and share knowledge within a team. Example: Before merging new data preprocessing scripts, peers review for correctness and maintainability.</p>"},{"location":"glossary/#computer-vision","title":"Computer Vision","text":"<p>A field of AI focusing on enabling machines to interpret and understand visual information from the world. Example: Tasks include image classification, object detection, and image segmentation.</p>"},{"location":"glossary/#conditional-gan","title":"Conditional GAN","text":"<p>A GAN variant where both the generator and discriminator receive additional context information, such as class labels. Example: Conditional GANs can generate images conditioned on textual descriptions like \u201ccats\u201d or \u201cdogs.\u201d</p>"},{"location":"glossary/#confusion-matrix","title":"Confusion Matrix","text":"<p>A tabular layout displaying the number of correct and incorrect predictions grouped by class. Example: Rows typically represent actual classes, while columns represent predicted classes, making misclassifications visible.</p>"},{"location":"glossary/#conda-environment","title":"Conda Environment","text":"<p>A virtual environment manager provided by Anaconda, allowing users to isolate Python packages and dependencies. Example: Students create a separate conda environment to manage a specific ML project\u2019s libraries without interfering with system Python.</p>"},{"location":"glossary/#convolution-operation","title":"Convolution Operation","text":"<p>A mathematical operation where a filter (kernel) is applied to input data (e.g., an image) to detect features. Example: In CNNs, convolution layers learn filters that activate in response to relevant patterns.</p>"},{"location":"glossary/#cross-validation","title":"Cross-Validation","text":"<p>A technique for evaluating model performance by partitioning data into multiple folds, cycling through training and validation sets. Example: k-fold cross-validation helps assess how well the model generalizes to unseen data.</p>"},{"location":"glossary/#cyclegan","title":"CycleGAN","text":"<p>A GAN-based architecture for unpaired image-to-image translation. Example: CycleGAN can transform horse images into zebra images without needing paired training examples.</p>"},{"location":"glossary/#data-acquisition","title":"Data Acquisition","text":"<p>The process of gathering relevant datasets from various sources, such as APIs, sensors, or public repositories. Example: In a project, data acquisition might involve scraping websites or querying a medical imaging database.</p>"},{"location":"glossary/#data-annotation","title":"Data Annotation","text":"<p>Labeling or tagging raw data (images, text, audio) with meaningful information to facilitate supervised learning. Example: Crowdworkers often annotate bounding boxes on images to prepare object detection datasets.</p>"},{"location":"glossary/#data-augmentation","title":"Data Augmentation","text":"<p>Techniques used to increase the diversity of a training set by applying transformations (e.g., flips, rotations) to existing samples. Example: Randomly flipping or cropping images to help CNNs generalize better.</p>"},{"location":"glossary/#data-cleaning","title":"Data Cleaning","text":"<p>The process of identifying and correcting incomplete, inaccurate, or irrelevant parts of a dataset. Example: Removing duplicates or fixing inconsistent labels in a CSV file before model training.</p>"},{"location":"glossary/#data-driven-ai","title":"Data-Driven AI","text":"<p>AI methodologies that rely heavily on large datasets and statistical learning rather than explicit symbolic rules. Example: Deep learning is a classic form of data-driven AI, requiring abundant labeled data for training.</p>"},{"location":"glossary/#data-ethics","title":"Data Ethics","text":"<p>The standards and principles guiding the responsible collection, storage, and use of data. Example: Ensuring personal data is handled with user consent and compliance with legal frameworks such as GDPR.</p>"},{"location":"glossary/#data-imbalance","title":"Data Imbalance","text":"<p>A condition where classes or categories in a dataset are not equally represented. Example: In fraud detection, \u201cfraud\u201d cases are often far fewer than \u201cnon-fraud\u201d cases, impacting model training.</p>"},{"location":"glossary/#data-parallelism","title":"Data Parallelism","text":"<p>Distributing subsets of the input data across multiple processors or machines to train a model faster. Example: Large mini-batches are split across GPUs, and gradients are aggregated at each step.</p>"},{"location":"glossary/#data-preprocessing","title":"Data Preprocessing","text":"<p>Preparatory steps taken on raw data, such as cleaning, normalization, or encoding. Example: Converting categorical variables into one-hot vectors or normalizing images to zero mean and unit variance.</p>"},{"location":"glossary/#data-profile","title":"Data Profile","text":"<p>A summary of key statistical characteristics and quality metrics of a dataset, including distributions, missing values, cardinality, and data types for each feature.</p> <p>Example: An MNIST data profile would reveal that each image is 28x28 pixels (784 features), pixel values range from 0-255, there are 10 distinct label classes with roughly equal distribution (~6000 samples each), and the dataset contains no missing values.  See </p>"},{"location":"glossary/#data-splits","title":"Data Splits","text":"<p>Partitioning data into subsets (e.g., training, validation, test) for model development and evaluation. Example: 80% of the data for training, 10% for validation, 10% for testing.</p>"},{"location":"glossary/#data-visualization","title":"Data Visualization","text":"<p>Graphical representation of data to reveal patterns, trends, or outliers. Example: Creating scatter plots in Matplotlib to explore relationships between features.</p>"},{"location":"glossary/#data-visualization-tools","title":"Data Visualization Tools","text":"<p>Software libraries and platforms that facilitate the creation of charts, graphs, and interactive plots. Example: Matplotlib, Seaborn, and Plotly are used in notebooks to illustrate key data insights.</p>"},{"location":"glossary/#deep-learning","title":"Deep Learning","text":"<p>Neural network designs featuring multiple hidden layers, enabling hierarchical feature extraction.</p> <p>Deep Learning is a subset of machine learning that focus on complex models.  Deep learning is used for tasks such as image classification, object detection, speech recognition, natural language processing and other problems that can't be solved by simpler models.</p> <p>Example: A typical deep learning system might have a dozen or more layers in a CNN.</p>"},{"location":"glossary/#deep-q-network-dqn","title":"Deep Q-Network (DQN)","text":"<p>A reinforcement learning approach where a deep neural network approximates the Q-function for action selection. Example: DQN was famously applied by DeepMind to play Atari games at a superhuman level.</p>"},{"location":"glossary/#deepseek","title":"Deepseek","text":"<p>An open-source large-langauge model with reasoning announced in 2025 that has superior performance.</p> <p>The Deepseek-R1 7B model is used on a local GPU for testing the tokens-per-second lab in this course.</p>"},{"location":"glossary/#densenet","title":"DenseNet","text":"<p>A CNN architecture where each layer is connected to every other layer in a feed-forward manner. Example: DenseNet alleviates the vanishing gradient problem by encouraging feature reuse.</p>"},{"location":"glossary/#depth-vs-width","title":"Depth Vs Width","text":"<p>The tradeoff between making a network deeper (more layers) or wider (more neurons per layer). Example: Deeper networks can learn more abstract features, whereas wider networks can capture more detail at each level.</p>"},{"location":"glossary/#denoising-diffusion","title":"Denoising Diffusion","text":"<p>A class of generative models where noise is incrementally added to data, then learned in reverse to generate samples. Example: Used in image generation tasks to progressively refine noise into realistic images.</p>"},{"location":"glossary/#diffusion-models","title":"Diffusion Models","text":"<p>Probabilistic models that learn to reverse a gradual noising process to produce new samples. Example: DALL\u00b7E 2 incorporates diffusion methods for high-quality image synthesis.</p>"},{"location":"glossary/#dimensionality-concepts","title":"Dimensionality Concepts","text":"<p>Principles and considerations related to the number of features or variables in a dataset (dimensionality). Example: High-dimensional data can be more difficult to visualize and may require techniques like PCA.</p>"},{"location":"glossary/#documentation","title":"Documentation","text":"<p>Written guides, explanations, or references describing software or processes. Example: Thorough documentation ensures that future team members can understand the codebase and its usage.</p>"},{"location":"glossary/#drug-discovery","title":"Drug Discovery","text":"<p>The application of AI to identify potential new medications by analyzing large chemical and biological datasets. Example: ML models predict molecular binding affinities to reduce trial-and-error in drug design.</p>"},{"location":"glossary/#dropout","title":"Dropout","text":"<p>A regularization method that randomly \u201cdrops\u201d or sets some neurons to zero during training to reduce overfitting. Example: Setting a 50% dropout rate in a dense layer to encourage robust feature learning.</p>"},{"location":"glossary/#early-stopping","title":"Early Stopping","text":"<p>A regularization technique that halts training when validation performance stops improving. Example: Prevents overfitting by not allowing the model to train excessively on one dataset.</p>"},{"location":"glossary/#edge-deployment","title":"Edge Deployment","text":"<p>Running AI models directly on edge devices (smartphones, IoT devices) rather than on centralized servers. Example: A small CNN for object recognition deployed on a mobile phone for real-time inference.</p>"},{"location":"glossary/#energy-based-models","title":"Energy-Based Models","text":"<p>A class of probabilistic models that define an energy function over configurations, and learning involves shaping this energy landscape. Example: Boltzmann machines are one type of energy-based model used for representation learning.</p>"},{"location":"glossary/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Quantitative measures to assess model performance, guiding model selection and tuning. Example: Accuracy, precision, recall, and F1 score are common metrics for classification tasks.</p>"},{"location":"glossary/#explainable-ai-xai","title":"Explainable AI (XAI)","text":"<p>Methods designed to make AI system decisions interpretable by humans. Example: Feature attribution maps that highlight which parts of an image influenced a CNN\u2019s classification.</p>"},{"location":"glossary/#exploration-vs-exploitation","title":"Exploration Vs Exploitation","text":"<p>The balance in reinforcement learning between trying new actions (exploration) and using known rewarding actions (exploitation). Example: An RL agent might explore different states early on, then exploit the best actions discovered.</p>"},{"location":"glossary/#exploding-gradient-control","title":"Exploding Gradient Control","text":"<p>Techniques to prevent gradients from becoming excessively large during backpropagation. Example: Gradient clipping is a common way to avoid unstable updates in RNN training.</p>"},{"location":"glossary/#exploding-gradients","title":"Exploding Gradients","text":"<p>A situation where gradients grow uncontrollably during training, causing large parameter updates and potential instability. Example: Deep RNNs may experience exploding gradients unless special measures (like clipping) are applied.</p>"},{"location":"glossary/#f1-score","title":"F1 Score","text":"<p>The harmonic mean of precision and recall, providing a single measure of a test\u2019s accuracy. Example: Used when you care about both false positives and false negatives in an imbalanced classification.</p>"},{"location":"glossary/#fairness","title":"Fairness","text":"<p>The principle of designing AI systems to avoid discriminatory outcomes or biased treatment. Example: Ensuring loan approval models do not systematically disadvantage applicants from certain demographics.</p>"},{"location":"glossary/#feature-engineering","title":"Feature Engineering","text":"<p>The process of creating or transforming input features to improve model performance. Example: Combining multiple text columns into a single normalized \u201cbag of words\u201d vector.</p>"},{"location":"glossary/#feature-space","title":"Feature Space","text":"<p>The multidimensional space where each dimension corresponds to a feature of the data. Example: Visualizing points in feature space helps understand how the model separates classes.</p>"},{"location":"glossary/#filters-and-kernels","title":"Filters And Kernels","text":"<p>Learnable convolution operators that detect features in images or other spatial data. Example: Early layers might learn edge detectors, while deeper layers capture more complex patterns.</p>"},{"location":"glossary/#financial-forecasting","title":"Financial Forecasting","text":"<p>Using AI/ML to predict market movements, asset prices, or economic indicators. Example: LSTM networks analyzing historical stock data for next-day price predictions.</p>"},{"location":"glossary/#flow-based-models","title":"Flow-Based Models","text":"<p>Generative models that transform noise into data samples via a series of invertible transformations. Example: RealNVP or Glow architectures produce exact likelihood estimates and allow sampling.</p>"},{"location":"glossary/#forward-propagation","title":"Forward Propagation","text":"<p>The process of passing input data through a neural network to get an output prediction. Example: In a feedforward network, data flows from the input layer through hidden layers to the output.</p>"},{"location":"glossary/#frozen-in-time","title":"Frozen in Time","text":"<p>The term used to describe that the knowledge base that a LLM is trained on has a cutoff date which can often omit current events in the past year.</p>"},{"location":"glossary/#fully-connected-layers","title":"Fully Connected Layers","text":"<p>Layers where every neuron is connected to every neuron in the next layer, typically appearing after convolution blocks. Example: After convolution and pooling, the extracted features might go into a fully connected classifier.</p>"},{"location":"glossary/#gan-basics","title":"GAN Basics","text":"<p>The foundational idea of Generative Adversarial Networks, involving a generator and a discriminator in a minimax game. Example: A generator tries to create realistic images, while the discriminator attempts to distinguish them from real images.</p>"},{"location":"glossary/#generative-models","title":"Generative Models","text":"<p>Models that learn the joint probability distribution of data, enabling them to generate new, synthetic samples. Example: Models like GANs, VAEs, and diffusion models can create realistic images or text.</p>"},{"location":"glossary/#generator-vs-discriminator","title":"Generator Vs Discriminator","text":"<p>Two components of a GAN: the generator synthesizes data, and the discriminator classifies whether data is real or generated. Example: Training alternates between improving the generator\u2019s realism and tightening the discriminator\u2019s detection.</p>"},{"location":"glossary/#gpt-overview","title":"GPT Overview","text":"<p>Highlights of the Generative Pre-trained Transformer series, focusing on autoregressive language modeling at scale. Example: GPT-3 can generate coherent paragraphs of text based on a given prompt.</p>"},{"location":"glossary/#graph-neural-networks-gnns","title":"Graph Neural Networks (GNNs)","text":"<p>Networks designed to process graph-structured data, learning node or edge representations through message passing. Example: GNNs can predict molecular properties by treating atoms as nodes and bonds as edges.</p>"},{"location":"glossary/#gradient-descent","title":"Gradient Descent","text":"<p>An iterative optimization method that updates parameters in the opposite direction of the gradient of the loss function. Example: Simple gradient descent uses the entire dataset to compute gradients each iteration.</p>"},{"location":"glossary/#gru-units","title":"GRU Units","text":"<p>Gated Recurrent Unit cells that manage hidden state transitions without a separate cell state. Example: GRUs are often computationally simpler yet comparable to LSTMs for sequence tasks.</p>"},{"location":"glossary/#grid-search","title":"Grid Search","text":"<p>A hyperparameter tuning method that exhaustively tries every combination of a specified parameter grid. Example: Searching over different learning rates and regularization strengths for logistic regression.</p>"},{"location":"glossary/#gpu-acceleration","title":"GPU Acceleration","text":"<p>Utilizing graphics processing units to speed up parallelizable operations in neural network training. Example: Matrix multiplications in backprop are greatly accelerated on GPUs.</p>"},{"location":"glossary/#he-initialization","title":"He Initialization","text":"<p>Weight initialization method adapted for ReLU-like activation functions, aiming to maintain variance across layers. Example: Also known as Kaiming initialization, used to stabilize training in deep ReLU networks.</p>"},{"location":"glossary/#high-dimensional-data","title":"High-Dimensional Data","text":"<p>Data with many features, which can lead to the \u201ccurse of dimensionality\u201d and sparse observations in feature space. Example: Text data with thousands of unique terms is inherently high-dimensional.</p>"},{"location":"glossary/#history-of-ai","title":"History Of AI","text":"<p>The chronological development and milestones of AI, from symbolic systems to modern deep learning breakthroughs. Example: Tracing from early logic-based AI in the 1950s to recent achievements in large-scale neural models.</p>"},{"location":"glossary/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>The process of finding optimal values for parameters not directly learned during training, like learning rates or layer sizes. Example: Using cross-validation to compare multiple hyperparameter configurations.</p>"},{"location":"glossary/#imagenet","title":"ImageNet","text":"<p>A large-scale image dataset widely used as a benchmark for deep learning, especially in computer vision. Example: Models that excel on ImageNet often generalize well to a range of vision tasks.</p>"},{"location":"glossary/#inception-modules","title":"Inception Modules","text":"<p>Architectural blocks that perform convolutions of different sizes in parallel, then concatenate outputs. Example: GoogLeNet (Inception v1) introduced inception modules for more efficient resource usage.</p>"},{"location":"glossary/#initialization-methods","title":"Initialization Methods","text":"<p>Techniques for setting initial weight values in neural networks to aid stable convergence. Example: He or Xavier initialization is often chosen based on the activation function used.</p>"},{"location":"glossary/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Interactive web-based computational tools that combine code, visualizations, and text. Example: Commonly used in teaching ML, allowing students to experiment step by step and visualize results.</p>"},{"location":"glossary/#key-figures","title":"Key Figures","text":"<p>Influential researchers or pioneers in AI who contributed foundational theories or breakthroughs. Example: Alan Turing, Marvin Minsky, Geoffrey Hinton, Yoshua Bengio, and Yann LeCun.</p>"},{"location":"glossary/#language-modeling","title":"Language Modeling","text":"<p>Predicting the likelihood of a sequence of words, forming the basis for many NLP tasks. Example: A language model might predict the next word in a sentence or evaluate the fluency of generated text.</p>"},{"location":"glossary/#large-language-models-llms","title":"Large Language Models (LLMs)","text":"<p>Very large neural network-based models trained on massive text corpora to perform complex language tasks. Example: GPT-3, BERT, and PaLM can perform text generation, question answering, and zero-shot tasks.</p>"},{"location":"glossary/#latent-space","title":"Latent Space","text":"<p>The lower-dimensional internal representation of data learned by a model, especially in generative methods. Example: In a VAE, points in the latent space can be sampled and decoded into new images.</p>"},{"location":"glossary/#layer-stacking","title":"Layer Stacking","text":"<p>Arranging multiple layers in a network to build deeper models capable of complex feature extraction. Example: Adding layers in a CNN to capture higher-level abstractions of the input.</p>"},{"location":"glossary/#learning-rate","title":"Learning Rate","text":"<p>A hyperparameter controlling the step size in gradient-based optimization. Example: A learning rate that\u2019s too high might overshoot minima, while one that\u2019s too low could prolong training.</p>"},{"location":"glossary/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>Strategies to adjust the learning rate over time, typically reducing it as training progresses. Example: Step decay, exponential decay, and warm restarts are all scheduling techniques.</p>"},{"location":"glossary/#lime","title":"LIME","text":"<p>Local Interpretable Model-agnostic Explanations; explains predictions of any classifier by approximating it locally with an interpretable model. Example: LIME can highlight text snippets that most influenced a sentiment classifier\u2019s decision.</p>"},{"location":"glossary/#lstm-units","title":"LSTM Units","text":"<p>Long Short-Term Memory cells that maintain and control access to an internal cell state for long-range dependencies in sequences. Example: LSTMs are popular in NLP for capturing context across lengthy texts.</p>"},{"location":"glossary/#likelihood-estimation","title":"Likelihood Estimation","text":"<p>The process of determining parameters of a probabilistic model by maximizing the likelihood of observed data. Example: In generative models, training often involves maximizing log-likelihood.</p>"},{"location":"glossary/#local-minima","title":"Local Minima","text":"<p>Points in the loss landscape where no small move decreases the loss, but they may not be the global optimum. Example: Neural networks often rely on large parameter spaces that can have many local minima.</p>"},{"location":"glossary/#loss-functions","title":"Loss Functions","text":"<p>Mathematical functions quantifying the difference between predictions and targets, guiding parameter updates. Example: Mean Squared Error is a loss function for regression; Cross-Entropy is common for classification.</p>"},{"location":"glossary/#loss-surface","title":"Loss Surface","text":"<p>The multidimensional space defined by network parameters on which the loss function value is plotted. Example: Gradient descent methods navigate the loss surface to find minima.</p>"},{"location":"glossary/#manifold-hypothesis","title":"Manifold Hypothesis","text":"<p>The assumption that high-dimensional data points lie on lower-dimensional manifolds embedded in the input space. Example: Techniques like t-SNE or UMAP attempt to uncover these manifolds for visualization.</p>"},{"location":"glossary/#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>A mathematical framework for sequential decision-making, characterized by states, actions, rewards, and transition probabilities. Example: Reinforcement learning often models environments as MDPs for algorithmic exploration.</p>"},{"location":"glossary/#matplotlib-basics","title":"Matplotlib Basics","text":"<p>Fundamental features of Matplotlib, a Python library for creating static, animated, and interactive visualizations. Example: Plotting histograms, line graphs, and scatter plots to understand dataset distributions.</p>"},{"location":"glossary/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>A regression loss function that averages the squared differences between predictions and targets. Example: MSE is minimized during training of linear regression or certain autoencoders.</p>"},{"location":"glossary/#medical-imaging","title":"Medical Imaging","text":"<p>The application of AI to interpret and analyze medical scans such as MRIs, CTs, and X-rays. Example: CNNs detecting tumors in MRI scans to assist radiologists.</p>"},{"location":"glossary/#memory-management","title":"Memory Management","text":"<p>Techniques to efficiently allocate and handle data in GPU/CPU memory during model training. Example: Gradient checkpointing helps reduce memory usage in very deep networks.</p>"},{"location":"glossary/#mini-batch-training","title":"Mini-Batch Training","text":"<p>A compromise between batch and stochastic training, updating parameters after a small subset of samples. Example: Common in practice for balancing computational efficiency with stable gradient estimates.</p>"},{"location":"glossary/#ml-definition","title":"ML Definition","text":"<p>A subfield of AI that focuses on algorithms learning patterns from data rather than being explicitly programmed. Example: Classification and regression tasks where models improve their performance as they process more data.</p>"},{"location":"glossary/#mlops","title":"MLOps","text":"<p>A set of practices for deploying and maintaining machine learning models in production, analogous to DevOps. Example: Automated CI/CD pipelines that retrain and redeploy models as new data arrives.</p>"},{"location":"glossary/#model-compression","title":"Model Compression","text":"<p>Techniques to reduce model size and inference costs, such as pruning, quantization, or knowledge distillation. Example: Compressing a large CNN so it can run on edge devices with limited memory.</p>"},{"location":"glossary/#model-interpretability","title":"Model Interpretability","text":"<p>The clarity with which a human can understand a model\u2019s internal processes and decisions. Example: Using saliency maps to see which pixels in an image were key for a CNN\u2019s classification.</p>"},{"location":"glossary/#model-parallelism","title":"Model Parallelism","text":"<p>Splitting a model\u2019s layers or parameters across different computational units. Example: Large language models can be distributed across multiple GPUs, each storing only part of the network.</p>"},{"location":"glossary/#momentum-optimizer","title":"Momentum Optimizer","text":"<p>An extension of gradient descent that accumulates velocity from past updates to damp oscillations and accelerate learning. Example: Typically combined with a learning rate schedule to converge faster.</p>"},{"location":"glossary/#multi-layer-perceptron","title":"Multi-Layer Perceptron","text":"<p>A fully connected feedforward network with one or more hidden layers. Example: Often used as a baseline for tabular data classification or regression tasks.</p>"},{"location":"glossary/#natural-language-processing-nlp","title":"Natural Language Processing (NLP)","text":"<p>AI techniques enabling machines to understand, interpret, and generate human language. Example: Chatbots, machine translation, and sentiment analysis are common NLP applications.</p>"},{"location":"glossary/#neural-network","title":"Neural Network","text":"<p>A network of calculations designed to mimic the neurons in the human brain.</p>"},{"location":"glossary/#neural-machine-translation","title":"Neural Machine Translation","text":"<p>An approach to automated translation that uses deep neural networks, often encoder-decoder architectures. Example: Systems like Google Translate or DeepL rely on neural machine translation to handle multiple languages.</p>"},{"location":"glossary/#neural-network-origins","title":"Neural Network Origins","text":"<p>The historical foundation of neural networks, tracing back to early perceptron models and Hebbian learning. Example: The perceptron\u2019s creation in the 1950s laid groundwork for modern deep learning.</p>"},{"location":"glossary/#nobel-prizes","title":"Nobel Prizes","text":"<p>Prestigious awards that, while not commonly granted specifically for AI, have occasionally been given for foundational contributions relevant to AI fields. Example: John Nash\u2019s work in game theory influenced multi-agent AI, recognized with a Nobel in Economics.</p>"},{"location":"glossary/#nonlinear-embeddings","title":"Nonlinear Embeddings","text":"<p>Mapping high-dimensional data into a lower-dimensional space using nonlinear transformations. Example: t-SNE and UMAP produce nonlinear embeddings to visualize clusters.</p>"},{"location":"glossary/#numpy-basics","title":"NumPy Basics","text":"<p>Core functionalities of the NumPy library for handling n-dimensional arrays and performing vectorized operations. Example: Creating and reshaping arrays, broadcasting, and using universal functions like <code>np.exp()</code>.</p>"},{"location":"glossary/#object-detection","title":"Object Detection","text":"<p>Identifying and localizing objects within images or video frames, typically returning bounding boxes and class labels. Example: A YOLO-based model scanning real-time video to detect pedestrians and vehicles.</p>"},{"location":"glossary/#ollama","title":"Ollama","text":"<p>A community-driven project that democratizes access to powerful LLMs developed by various organizations.</p> <p>Ollama was founded by Michael Chiang and Jeffrey Morgan in Palo Alto, CA, and is an independent startup that participated in the W21 batch of Y Combinator.</p> <p>Ollama is not directly associated with Meta or the development of the Llama models.</p> <p>Ollama makes it easy to run LLMs on local hardware such as a consumer-grade GPU.  Installation can be done with two simple UNIX shell commands.</p> <ul> <li>Ollama website</li> </ul>"},{"location":"glossary/#one-hot-encoded-format","title":"One-hot Encoded Format","text":"<p>A data preprocessing technique that represents categorical variables as binary vectors where exactly one element is 1 (hot) and all others are 0 (cold).</p> <p>Example: In MNIST digit classification, each target label (0-9) is encoded as a 10-dimensional binary vector: the digit '3' becomes [0,0,0,1,0,0,0,0,0,0], where only the fourth position (index 3) contains a 1.</p>"},{"location":"glossary/#overfitting-vs-underfitting","title":"Overfitting Vs Underfitting","text":"<p>Overfitting occurs when a model learns spurious details in the training data, while underfitting fails to capture underlying trends. Example: A deep network might overfit a small dataset; a simple linear model might underfit a complex dataset.</p>"},{"location":"glossary/#pca","title":"PCA","text":"<p>Principal Component Analysis; a linear dimensionality reduction method finding directions of maximum variance. Example: PCA can compress a 100-dimensional dataset into a handful of principal components for visualization.</p>"},{"location":"glossary/#pandas-basics","title":"Pandas Basics","text":"<p>Common operations in the Pandas library, such as DataFrame creation, cleaning, merging, and analysis. Example: Using <code>pandas.read_csv()</code> to load a dataset and <code>DataFrame.describe()</code> for summary statistics.</p>"},{"location":"glossary/#perceptron-model","title":"Perceptron Model","text":"<p>A single-layer linear classifier that outputs a binary decision based on a weighted sum of inputs. Example: Historically important as the earliest form of a neural network unit.</p>"},{"location":"glossary/#policy-gradient","title":"Policy Gradient","text":"<p>A family of RL algorithms that optimize a parameterized policy by ascending an estimate of the gradient of expected reward. Example: REINFORCE is a basic policy gradient method using sampled trajectories.</p>"},{"location":"glossary/#pooling-layers","title":"Pooling Layers","text":"<p>Layers that reduce spatial dimensions by combining nearby feature responses, helping control overfitting. Example: Max pooling or average pooling in a CNN to halve the width and height of feature maps.</p>"},{"location":"glossary/#positional-encoding","title":"Positional Encoding","text":"<p>A technique used in transformer architectures to inject information about sequence order without recurrence. Example: Sine and cosine functions of different frequencies are added to word embeddings in Transformers.</p>"},{"location":"glossary/#precision","title":"Precision","text":"<p>The fraction of predicted positives that are truly positive, measuring correctness among positive predictions. Example: High precision in a medical test means few healthy people are incorrectly diagnosed as sick.</p>"},{"location":"glossary/#pruning","title":"Pruning","text":"<p>Removing weights or neurons from a trained model to reduce complexity and size, often with minimal accuracy loss. Example: Pruning can eliminate near-zero weights, speeding up inference in production.</p>"},{"location":"glossary/#privacy","title":"Privacy","text":"<p>The right or requirement that individuals control their personal data and how it is used. Example: Differential privacy in ML ensures training data cannot be reverse-engineered from model parameters.</p>"},{"location":"glossary/#prompt-engineering","title":"Prompt Engineering","text":"<p>The practice of carefully designing prompts or instructions to guide large language models for desired outputs. Example: Providing explicit context and constraints in a GPT prompt to extract factual answers.</p>"},{"location":"glossary/#protein-folding","title":"Protein Folding","text":"<p>The computational prediction of a protein\u2019s 3D structure from its amino acid sequence, aided by AI breakthroughs. Example: DeepMind\u2019s AlphaFold made significant progress in accurately modeling protein configurations.</p>"},{"location":"glossary/#project-scoping","title":"Project Scoping","text":"<p>Defining the objectives, resources, and deliverables of a machine learning or AI project. Example: Determining the data needed, performance targets, and timeline for a sentiment analysis project.</p>"},{"location":"glossary/#pytorch-intro","title":"PyTorch Intro","text":"<p>An overview of the PyTorch deep learning framework, emphasizing its dynamic computation graph. Example: PyTorch\u2019s imperative style makes debugging simpler and is favored by many researchers.</p>"},{"location":"glossary/#python-setup","title":"Python Setup","text":"<p>Basic installation and configuration of Python environments, packages, and tools. Example: Installing Python 3.x, pip, and setting up a virtual environment on a workstation.</p>"},{"location":"glossary/#q-learning","title":"Q-Learning","text":"<p>A model-free RL algorithm that learns an action-value function predicting future rewards. Example: The algorithm updates a Q-table or Q-network using Bellman equations and experiences from the environment.</p>"},{"location":"glossary/#quantization","title":"Quantization","text":"<p>Reducing the precision of model parameters (e.g., from 32-bit floating-point to 8-bit integers) to improve efficiency. Example: Quantized networks often run faster on edge devices with limited compute power.</p>"},{"location":"glossary/#random-search","title":"Random Search","text":"<p>A hyperparameter tuning method selecting parameter combinations randomly within predefined ranges. Example: Often more efficient than grid search when the parameter space is large.</p>"},{"location":"glossary/#recall","title":"Recall","text":"<p>The fraction of actual positives correctly identified by the model, measuring completeness. Example: In a medical test, recall indicates the percentage of sick patients who are correctly diagnosed.</p>"},{"location":"glossary/#recommender-systems","title":"Recommender Systems","text":"<p>Algorithms designed to suggest items, such as products or media, to users based on preferences or similarities. Example: Matrix factorization or deep collaborative filtering for movie recommendations.</p>"},{"location":"glossary/#regression-vs-classification","title":"Regression Vs Classification","text":"<p>Distinguishes between predicting a continuous value (regression) and predicting a discrete label (classification). Example: House price prediction is regression, while determining spam vs non-spam email is classification.</p>"},{"location":"glossary/#regularization-overview","title":"Regularization Overview","text":"<p>Techniques (like weight decay, dropout) that constrain model complexity to prevent overfitting. Example: Adding L2 regularization to a neural network\u2019s cost function penalizes large weights.</p>"},{"location":"glossary/#requirement-analysis","title":"Requirement Analysis","text":"<p>Identifying the scope, constraints, and success criteria for an AI project before implementation. Example: Determining user needs, regulatory constraints, and performance thresholds for a medical imaging system.</p>"},{"location":"glossary/#residual-networks-resnet","title":"Residual Networks (ResNet)","text":"<p>Deep neural networks with skip connections that help mitigate vanishing gradients by enabling identity mappings. Example: ResNet-50 is a popular architecture for ImageNet classification tasks.</p>"},{"location":"glossary/#responsible-ai","title":"Responsible AI","text":"<p>AI development and deployment aligned with legal, ethical, and societal values. Example: Designing systems that respect user privacy, avoid bias, and are transparent.</p>"},{"location":"glossary/#reward-shaping","title":"Reward Shaping","text":"<p>Modifying or adding auxiliary rewards in RL to guide the agent toward desired behaviors. Example: Giving an agent a small reward for each time step it stays on track in a self-driving simulation.</p>"},{"location":"glossary/#rl-definition","title":"RL Definition","text":"<p>Reinforcement Learning is a subfield of AI where agents learn optimal behaviors through trial-and-error feedback from an environment. Example: An RL agent in a game environment receives rewards (scores) for achieving objectives.</p>"},{"location":"glossary/#robotics","title":"Robotics","text":"<p>The intersection of engineering and AI focusing on designing and controlling physical machines that perform tasks autonomously or semi-autonomously. Example: Automated robotic arms in manufacturing lines guided by computer vision.</p>"},{"location":"glossary/#scaling-and-normalization","title":"Scaling And Normalization","text":"<p>Rescaling features to standardized ranges or distributions for more stable training. Example: Applying min-max scaling to each feature so that all values lie between 0 and 1.</p>"},{"location":"glossary/#score-matching","title":"Score Matching","text":"<p>A technique for training generative models by matching the score function (gradient of log-density) of the model to that of the data. Example: Used in diffusion-based methods to iteratively denoise samples.</p>"},{"location":"glossary/#scipy-basics","title":"SciPy Basics","text":"<p>Fundamental capabilities of SciPy for scientific computations, including optimization, integration, and statistics. Example: Using <code>scipy.optimize</code> to implement advanced fitting procedures in an ML pipeline.</p>"},{"location":"glossary/#scikit-learn-overview","title":"Scikit-Learn Overview","text":"<p>Core features of the scikit-learn library, offering high-level APIs for classical ML algorithms, preprocessing, and validation. Example: Using <code>GridSearchCV</code> for hyperparameter tuning on a random forest model.</p>"},{"location":"glossary/#security","title":"Security","text":"<p>Protective measures ensuring AI systems resist unauthorized access, tampering, or malicious attacks. Example: Hardening model APIs against adversarial inputs that might reveal sensitive data.</p>"},{"location":"glossary/#self-attention","title":"Self-Attention","text":"<p>A mechanism in transformer models enabling each position in a sequence to attend to every other position for context. Example: BERT\u2019s self-attention layers capture dependencies in a sentence without relying on recurrence.</p>"},{"location":"glossary/#sequence-modeling","title":"Sequence Modeling","text":"<p>Approaches to handle sequential data (text, time series, etc.), capturing dependencies across timesteps. Example: LSTMs, GRUs, and Transformers are common sequence modeling architectures.</p>"},{"location":"glossary/#sequence-to-sequence","title":"Sequence-To-Sequence","text":"<p>Neural architectures mapping an input sequence (e.g., a sentence in English) to an output sequence (e.g., a sentence in French). Example: Used extensively in neural machine translation and speech recognition tasks.</p>"},{"location":"glossary/#shap","title":"SHAP","text":"<p>SHapley Additive exPlanations; a method based on Shapley values to interpret predictions by attributing contributions of each feature. Example: Generating a SHAP plot to see which features most influence a credit-scoring model\u2019s decisions.</p>"},{"location":"glossary/#skip-connections","title":"Skip Connections","text":"<p>Links that bypass one or more layers, helping gradients flow more easily in deep networks. Example: ResNet\u2019s skip connections add layer outputs directly to subsequent layers to mitigate vanishing gradients.</p>"},{"location":"glossary/#speech-recognition","title":"Speech Recognition","text":"<p>The task of converting spoken language into text using acoustic and language models. Example: Voice assistants transcribe user commands in real-time for further processing.</p>"},{"location":"glossary/#stochastic-training","title":"Stochastic Training","text":"<p>Updating model parameters after each individual sample or a small random subset (mini-batch). Example: Known as SGD, helps the model converge faster with more frequent updates.</p>"},{"location":"glossary/#stride-and-padding","title":"Stride And Padding","text":"<p>Parameters in convolution operations that determine how filters slide over input data and whether border regions are preserved. Example: Padding=\u201csame\u201d ensures output dimensions remain the same as input for certain CNN layers.</p>"},{"location":"glossary/#style-transfer","title":"Style Transfer","text":"<p>Neural technique that reworks the style of one image onto the content of another. Example: Merging a photo with the painting style of Van Gogh\u2019s \u201cStarry Night.\u201d</p>"},{"location":"glossary/#supervised-vs-unsupervised","title":"Supervised Vs Unsupervised","text":"<p>A distinction between learning with labeled data (supervised) vs. discovering patterns in unlabeled data (unsupervised). Example: Classifying labeled images is supervised; clustering unlabeled images is unsupervised.</p>"},{"location":"glossary/#symbolic-ai","title":"Symbolic AI","text":"<p>AI methods using explicit, human-readable representations of problems, logic, and rules. Example: Expert systems that encode domain knowledge in symbolic form.</p>"},{"location":"glossary/#team-collaboration","title":"Team Collaboration","text":"<p>Collective effort where multiple individuals share responsibilities and knowledge to complete AI projects. Example: Data engineers, data scientists, and software developers working together on a production pipeline.</p>"},{"location":"glossary/#t-sne","title":"t-SNE","text":"<p>t-Distributed Stochastic Neighbor Embedding; a non-linear dimensionality reduction method for visualization. Example: Often used to plot high-dimensional data (like embeddings) in 2D or 3D.</p>"},{"location":"glossary/#tensorflow-intro","title":"TensorFlow Intro","text":"<p>An overview of the TensorFlow framework emphasizing its computational graph, eager mode, and ecosystem tools. Example: Students build and train neural networks with Keras, a high-level TensorFlow API.</p>"},{"location":"glossary/#text-generation","title":"Text Generation","text":"<p>The task of producing coherent text sequences, often using language models. Example: GPT-based systems can generate paragraphs of natural-sounding text from a prompt.</p>"},{"location":"glossary/#text-generation-models","title":"Text Generation Models","text":"<p>Architectures specialized in producing novel text, typically via learned probability distributions over tokens. Example: LSTM-based decoders or transformer-based language models for writing summaries or creative content.</p>"},{"location":"glossary/#text-to-image","title":"Text-To-Image","text":"<p>Models that generate images based on textual input descriptions. Example: DALL\u00b7E variants produce custom images from user prompts like \u201can armchair shaped like an avocado.\u201d</p>"},{"location":"glossary/#time-series-forecasting","title":"Time-Series Forecasting","text":"<p>Techniques to predict future values of a sequence based on past observations. Example: LSTMs or Prophet library used for forecasting stock prices or energy demands.</p>"},{"location":"glossary/#training-validation-test","title":"Training-Validation-Test","text":"<p>Standard data splitting strategy: a training set for model fitting, a validation set for hyperparameter tuning, and a test set for final evaluation. Example: 60% train, 20% validation, 20% test split for a typical classification dataset.</p>"},{"location":"glossary/#transfer-learning-basics","title":"Transfer Learning Basics","text":"<p>Techniques for leveraging pre-trained models on new tasks with limited additional data. Example: Using a pre-trained ResNet on ImageNet as a feature extractor for a custom dataset.</p>"},{"location":"glossary/#transformers","title":"Transformers","text":"<p>An architecture for training deep neural networks that can be parallelized by GPUs.</p>"},{"location":"glossary/#tpu-acceleration","title":"TPU Acceleration","text":"<p>Using Tensor Processing Units (custom ASICs by Google) to speed up large-scale model training. Example: TPUs can be accessed on Google Cloud to train large transformer models more efficiently than GPUs.</p>"},{"location":"glossary/#umap","title":"UMAP","text":"<p>Uniform Manifold Approximation and Projection; a non-linear technique for dimensionality reduction. Example: Faster and often better at preserving global structure compared to t-SNE on large datasets.</p>"},{"location":"glossary/#value-functions","title":"Value Functions","text":"<p>In reinforcement learning, functions estimating expected future rewards from a given state (or state-action pair). Example: The critic in actor-critic algorithms learns a value function to guide the actor\u2019s updates.</p>"},{"location":"glossary/#vanilla-rnn","title":"Vanilla RNN","text":"<p>A basic recurrent neural network that uses hidden states to process sequences one step at a time. Example: Applied to simple sequence tasks but prone to vanishing/exploding gradient issues for long sequences.</p>"},{"location":"glossary/#vanishing-gradients","title":"Vanishing Gradients","text":"<p>A phenomenon where gradients diminish in magnitude through backprop, hindering learning in deep networks. Example: Sigmoid activations can exacerbate vanishing gradients as the network depth grows.</p>"},{"location":"glossary/#version-control","title":"Version Control","text":"<p>Systems that track changes to code over time, allowing collaboration and reversion if needed. Example: Git and GitHub store historical commits and manage parallel development branches.</p>"},{"location":"glossary/#version-control-workflow","title":"Version Control Workflow","text":"<p>Best practices and procedures for using version control systems, including branching, merging, and reviewing. Example: A feature branch workflow ensures code is tested and reviewed before merging into <code>main</code>.</p>"},{"location":"glossary/#variational-autoencoders-vae","title":"Variational Autoencoders (VAE)","text":"<p>Generative models that learn a latent distribution of data using an encoder-decoder framework with a KL-divergence term. Example: VAEs create smooth latent spaces, enabling interpolation between different generated samples.</p>"},{"location":"glossary/#vision-transformers-vit","title":"Vision Transformers (ViT)","text":"<p>Transformer-based architectures adapted for computer vision tasks by splitting images into patches. Example: ViT processes each patch as a token, leveraging self-attention for image classification.</p>"},{"location":"glossary/#weights-and-biases","title":"Weights And Biases","text":"<p>Trainable parameters in a neural network that transform inputs into outputs. Example: A linear layer with 10 inputs and 5 outputs would have 10\u00d75 weights plus 5 bias terms.</p>"},{"location":"glossary/#weight-decay","title":"Weight Decay","text":"<p>A regularization method applying an L2 penalty on weights to encourage smaller parameter values. Example: Often implemented as a parameter in optimizers like SGD or Adam (e.g., <code>weight_decay=1e-4</code>).</p>"},{"location":"glossary/#xavier-initialization","title":"Xavier Initialization","text":"<p>A weight initialization method maintaining variance in both forward and backward passes for certain activation functions. Example: Often used with tanh or sigmoid activations to keep gradients stable early in training.</p>"},{"location":"glossary/#text-to-image_1","title":"Text-to-Image","text":"<p>(Duplicate check \u2014 We already covered \"Text-To-Image\" at concept 110. Ensuring no duplication. It's already listed. We\u2019ll ignore any duplicates.)</p> <p>(No more new Z or Y terms, so we\u2019re done.)</p> <p>This completes the alphabetical glossary of all 200 concepts.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":"<ol> <li>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"chapters/activation-functions/","title":"Activation Functions in Deep Learning","text":""},{"location":"chapters/activation-functions/#introduction","title":"Introduction","text":"<p>Activation functions are mathematical equations that determine the output of a neural network. They are crucial components that introduce non-linearity into the network, allowing it to learn complex patterns and relationships in data. Without activation functions, neural networks would simply be a series of linear transformations, severely limiting their ability to solve complex problems.</p> <p> FIGURE 1: Basic neuron diagram showing input, weighted sum, activation function, and output</p>"},{"location":"chapters/activation-functions/#core-concepts","title":"Core Concepts","text":"<p>The activation function takes the weighted sum of inputs and biases, then transforms this value into an output signal. This transformation serves several critical purposes:</p> <ol> <li>Non-linearity: Enables the network to approximate complex functions</li> <li>Output bounding: Many activation functions constrain outputs to specific ranges</li> <li>Gradient properties: Influences how effectively the network can learn through backpropagation</li> </ol>"},{"location":"chapters/activation-functions/#common-activation-functions","title":"Common Activation Functions","text":""},{"location":"chapters/activation-functions/#1-sigmoid-logistic-function","title":"1. Sigmoid (Logistic Function)","text":"\\[ f(x) = 1 / (1 + e^(-x)) \\] <p>Pros: - Outputs bounded between 0 and 1 - Smooth gradient - Clear probability interpretation - Historically significant</p> <p>Cons: - Suffers from vanishing gradient problem - Output not zero-centered - Computationally expensive (exponential function) - Saturates and kills gradients</p> <p>Computational Cost: High (requires exponential calculation)</p>"},{"location":"chapters/activation-functions/#2-hyperbolic-tangent-tanh","title":"2. Hyperbolic Tangent (tanh)","text":"\\[ f(x) = (e^x - e^(-x)) / (e^x + e^(-x)) \\] <p>Pros: - Zero-centered outputs (-1 to 1) - Stronger gradients than sigmoid - Often works better than sigmoid in practice</p> <p>Cons: - Still suffers from vanishing gradient - Computationally expensive - Saturates</p> <p>Computational Cost: High (requires exponential calculations)</p>"},{"location":"chapters/activation-functions/#3-rectified-linear-unit-relu","title":"3. Rectified Linear Unit (ReLU)","text":"\\[ f(x) = max(0, x) \\] <p>Pros: - Computationally efficient - No vanishing gradient for positive values - Sparse activation (natural regularization) - Faster convergence - Biologically inspired</p> <p>Cons: - \"Dying ReLU\" problem - Not zero-centered - Unbounded positive activation</p> <p>Computational Cost: Very Low (simple max operation)</p>"},{"location":"chapters/activation-functions/#4-leaky-relu","title":"4. Leaky ReLU","text":"\\[ f(x) = max(\u03b1x, x), where \u03b1 is typically 0.01 \\] <p>Pros: - Prevents dying ReLU problem - All benefits of ReLU - Allows for negative gradients</p> <p>Cons: - Results not consistent across problems - Additional hyperparameter (\u03b1) - Not zero-centered</p> <p>Computational Cost: Very Low (simple max operation with scalar multiplication)</p>"},{"location":"chapters/activation-functions/#5-exponential-linear-unit-elu","title":"5. Exponential Linear Unit (ELU)","text":"\\[ f(x) = x if x &gt; 0 f(x) = \u03b1(e^x - 1) if x \u2264 0 \\] <p>Pros: - Reduces bias shift - Natural gradient - Robust to noise - Better handling of negative values</p> <p>Cons: - Computationally more expensive than ReLU - Additional hyperparameter (\u03b1) - Slightly more complex derivative</p> <p>Computational Cost: Moderate (exponential calculation only for negative values)</p>"},{"location":"chapters/activation-functions/#computational-efficiency-and-scale","title":"Computational Efficiency and Scale","text":"<p>When building large-scale deep learning models, the computational efficiency of activation functions becomes crucial. Here's a comparison of relative computational costs:</p> <ol> <li>ReLU/Leaky ReLU: ~1x (baseline)</li> <li>ELU: ~2-3x</li> <li>Sigmoid/Tanh: ~4-5x</li> </ol> <p>For a model with millions or billions of parameters, these differences can significantly impact: - Training time - Inference latency - Energy consumption - Hardware requirements</p>"},{"location":"chapters/activation-functions/#modern-usage-patterns","title":"Modern Usage Patterns","text":"<p>In contemporary deep learning practice, activation functions are typically chosen as follows:</p>"},{"location":"chapters/activation-functions/#1-default-choice-relu","title":"1. Default Choice: ReLU","text":"<ul> <li>Used in ~80% of deep learning applications</li> <li>Standard for most convolutional neural networks</li> <li>Excellent balance of performance and efficiency</li> </ul>"},{"location":"chapters/activation-functions/#2-common-alternatives","title":"2. Common Alternatives:","text":"<ul> <li>Leaky ReLU: When dying ReLU is a concern</li> <li>ELU: When negative values need better handling</li> <li>Tanh: In RNNs and some specialized architectures</li> </ul>"},{"location":"chapters/activation-functions/#3-specific-use-cases","title":"3. Specific Use Cases:","text":"<ul> <li>Sigmoid: Final layer of binary classification</li> <li>Softmax: Final layer of multi-class classification</li> <li>Linear: Final layer of regression tasks</li> </ul>"},{"location":"chapters/activation-functions/#conclusion","title":"Conclusion","text":"<p>The choice of activation function can significantly impact both model performance and computational efficiency. While ReLU dominates modern architectures due to its simplicity and effectiveness, understanding the full range of options allows practitioners to make informed decisions based on their specific use cases and constraints.</p> <p>For large-scale deep learning projects, the computational efficiency of ReLU makes it particularly attractive, as the activation function computation can represent a significant portion of the overall computational budget. However, practitioners should remain aware of potential issues like the dying ReLU problem and be prepared to switch to alternatives when necessary.</p> <p> FIGURE 7: Comparative visualization of all activation functions discussed]</p>"},{"location":"chapters/activation-functions/#practice-questions","title":"Practice Questions","text":"<ol> <li>Why can't neural networks only use linear transformations without activation functions?</li> <li>What makes ReLU computationally efficient compared to sigmoid and tanh?</li> <li>In what scenarios might you choose ELU over ReLU despite its higher computational cost?</li> <li>How does the choice of activation function impact the training of very deep networks?</li> </ol>"},{"location":"chapters/cnns/","title":"Convolutional Neural Networks for Image Processing","text":""},{"location":"chapters/cnns/#chapter-outline","title":"Chapter Outline","text":""},{"location":"chapters/cnns/#1-introduction-to-cnns","title":"1. Introduction to CNNs","text":"<ul> <li>Historical context and development</li> <li>Comparison with traditional neural networks</li> <li>Key advantages for image processing tasks</li> <li>Overview of CNN applications in computer vision</li> </ul>"},{"location":"chapters/cnns/#2-fundamental-cnn-components","title":"2. Fundamental CNN Components","text":"<ul> <li>Convolutional layers</li> <li>Filters and kernels</li> <li>Feature maps</li> <li>Stride and padding options</li> <li>Pooling layers</li> <li>Max pooling</li> <li>Average pooling</li> <li>Purpose in reducing spatial dimensions</li> <li>Activation functions</li> <li>ReLU and variants</li> <li>Importance of non-linearity</li> <li>Fully connected layers</li> </ul>"},{"location":"chapters/cnns/#3-cnn-architectures","title":"3. CNN Architectures","text":"<ul> <li>LeNet</li> <li>AlexNet</li> <li>VGG</li> <li>Inception/GoogLeNet</li> <li>ResNet</li> <li>DenseNet</li> <li>Vision Transformers (ViT)</li> </ul>"},{"location":"chapters/cnns/#4-training-cnns","title":"4. Training CNNs","text":"<ul> <li>Loss functions for image tasks</li> <li>Backpropagation through convolutional layers</li> <li>Regularization techniques</li> <li>Dropout</li> <li>Batch normalization</li> <li>Data augmentation</li> <li>Optimization strategies</li> <li>Learning rate scheduling</li> <li>Weight initialization methods</li> </ul>"},{"location":"chapters/cnns/#5-transfer-learning-with-cnns","title":"5. Transfer Learning with CNNs","text":"<ul> <li>Pre-trained models on ImageNet</li> <li>Feature extraction approaches</li> <li>Fine-tuning strategies</li> <li>Domain adaptation challenges</li> </ul>"},{"location":"chapters/cnns/#6-advanced-cnn-applications","title":"6. Advanced CNN Applications","text":"<ul> <li>Object detection</li> <li>R-CNN family</li> <li>YOLO</li> <li>SSD</li> <li>Semantic segmentation</li> <li>FCN</li> <li>U-Net</li> <li>Instance segmentation</li> <li>Mask R-CNN</li> <li>Image generation and style transfer</li> </ul>"},{"location":"chapters/cnns/#7-explainable-cnns","title":"7. Explainable CNNs","text":"<ul> <li>Visualization techniques</li> <li>Feature maps</li> <li>Activation maximization</li> <li>Grad-CAM</li> <li>Interpretability approaches</li> <li>Ethical considerations</li> </ul>"},{"location":"chapters/cnns/#8-optimizing-cnn-performance","title":"8. Optimizing CNN Performance","text":"<ul> <li>Model compression techniques</li> <li>Pruning</li> <li>Quantization</li> <li>Hardware considerations</li> <li>GPU acceleration</li> <li>Edge deployment</li> <li>Model architecture search</li> </ul>"},{"location":"chapters/cnns/#9-case-studies","title":"9. Case Studies","text":"<ul> <li>Medical imaging analysis</li> <li>Autonomous driving perception</li> <li>Face recognition systems</li> <li>Industrial quality control</li> </ul>"},{"location":"chapters/cnns/#10-future-directions","title":"10. Future Directions","text":"<ul> <li>Hybrid architectures (CNN + Transformer)</li> <li>Self-supervised learning for CNNs</li> <li>Multimodal learning</li> <li>Neuromorphic approaches</li> </ul>"},{"location":"chapters/cnns/#11-practical-implementations","title":"11. Practical Implementations","text":"<ul> <li>CNN implementations in popular frameworks</li> <li>Deployment considerations</li> <li>Best practices for CNN development</li> </ul>"},{"location":"chapters/cnns/#12-glossary-and-resources","title":"12. Glossary and Resources","text":"<ul> <li>Key terminology</li> <li>Recommended reading</li> <li>Online courses and tutorials</li> <li>Datasets for practice</li> </ul>"},{"location":"chapters/cnns/cnn-microsim-ideas/","title":"CNN MicroSim Ideas","text":""},{"location":"chapters/cnns/cnn-microsim-ideas/#interactive-microsims-for-learning-cnn-concepts","title":"Interactive MicroSims for Learning CNN Concepts","text":"<p>Prompt</p> <p>A MicroSim is an in-browser interactive simulation written by a program such as p5.js that is used to illustrate a complex concept.  What topics in the above outline could be learned with an interactive simulation where the student run a small simulation they must interact with.  The simulation should have a start/stop/reset button, radio buttons and sliders in a control area that change drawing.  </p> <p>For each possible MicroSim, return the following:</p> <p>Clever Name Description Input Controls Learning Objective Assessment</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#response","title":"Response","text":"<p>Based on the chapter outline for Convolutional Neural Networks, here are several interactive MicroSim concepts that would effectively illustrate key concepts through simulation:</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#1-filtervision","title":"1. FilterVision","text":"<p>Clever Name: FilterVision</p> <p>Description: An interactive simulation that demonstrates how convolutional filters work on real images. Users can apply different filters (edge detection, blurring, sharpening) to sample images and see the resulting feature maps in real-time.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Slider for filter size (3x3, 5x5, 7x7)</li> <li>Slider for filter intensity</li> <li>Radio buttons for filter types (Sobel, Gaussian blur, emboss, custom)</li> <li>Custom filter value inputs for advanced users</li> </ul> <p>Learning Objective: Understand how different convolutional filters extract various features from images and how filter parameters affect the output.</p> <p>Assessment: Ask students to identify which filter would best detect specific features (edges, textures, corners) in an image, and to explain why filter size affects the feature detection capability.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#2-poolingplayground","title":"2. PoolingPlayground","text":"<p>Clever Name: PoolingPlayground</p> <p>Description: A visualization that shows how max pooling and average pooling affect feature maps. Users can see the original image section, apply convolutions, and then observe how pooling reduces dimensions while preserving important information.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Radio buttons for pooling type (Max, Average)</li> <li>Slider for pooling window size (2x2, 3x3, 4x4)</li> <li>Slider for stride value</li> <li>Toggle for displaying numerical values</li> </ul> <p>Learning Objective: Understand the differences between pooling types and how they preserve information while reducing computational requirements.</p> <p>Assessment: Have students predict the output dimensions after applying different pooling operations and explain why max pooling might be preferred for certain types of features.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#3-activationstation","title":"3. ActivationStation","text":"<p>Clever Name: ActivationStation</p> <p>Description: A simulation that visualizes how different activation functions transform input signals in neural networks, with special emphasis on how they affect feature maps in CNNs.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Radio buttons for activation functions (ReLU, Leaky ReLU, Sigmoid, Tanh)</li> <li>Slider for function parameters (e.g., leak rate for Leaky ReLU)</li> <li>Toggle for before/after comparison</li> <li>Slider to adjust input values</li> </ul> <p>Learning Objective: Understand how activation functions introduce non-linearity and how this impacts feature detection in CNNs.</p> <p>Assessment: Ask students to identify which activation function would work best for specific scenarios and explain why ReLU has become the standard in most CNN architectures.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#4-layerviz","title":"4. LayerViz","text":"<p>Clever Name: LayerViz</p> <p>Description: An interactive model of a small CNN where users can see how an image transforms as it passes through multiple layers. The simulation shows activations at each layer and allows slowing down the forward pass.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Slider for network depth (number of layers)</li> <li>Radio buttons for different sample images</li> <li>Toggle switches to enable/disable specific layers</li> <li>Slider for \"animation speed\" of forward propagation</li> </ul> <p>Learning Objective: Visualize the hierarchical feature extraction in CNNs, from low-level features in early layers to high-level concepts in deeper layers.</p> <p>Assessment: Have students describe what types of features are detected at different depths of the network and explain why deeper networks can recognize more complex patterns.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#5-backproptracker","title":"5. BackpropTracker","text":"<p>Clever Name: BackpropTracker</p> <p>Description: A visualization of how gradients flow backward through a CNN during training, showing which parts of the network are most affected by errors and how weights update.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Slider for learning rate</li> <li>Slider for batch size</li> <li>Radio buttons for different loss functions</li> <li>Toggle to show/hide gradient magnitudes</li> </ul> <p>Learning Objective: Understand the backpropagation process in CNNs and how parameter updates occur during training.</p> <p>Assessment: Ask students to explain what happens when learning rates are too high or too low, and how the gradient flow differs in early versus deep layers.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#6-architectbuilder","title":"6. ArchitectBuilder","text":"<p>Clever Name: ArchitectBuilder</p> <p>Description: A drag-and-drop interface for building simple CNN architectures, with real-time feedback on parameter count, computational complexity, and simulated performance on basic tasks.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Drag-and-drop layer components</li> <li>Sliders for layer parameters (filters, neurons, etc.)</li> <li>Radio buttons for different datasets (MNIST, CIFAR-10)</li> <li>Toggle for skip connections</li> </ul> <p>Learning Objective: Learn the tradeoffs in CNN design and how different architectural choices affect model performance and computational requirements.</p> <p>Assessment: Challenge students to design the smallest network that can achieve a target accuracy on a simple dataset, explaining their design decisions.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#7-transferinsight","title":"7. TransferInsight","text":"<p>Clever Name: TransferInsight</p> <p>Description: A simulation demonstrating transfer learning by showing feature activations from pre-trained models and how they can be repurposed for new tasks with limited data.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Radio buttons for pre-trained models (VGG, ResNet, etc.)</li> <li>Slider for freezing/unfreezing layers</li> <li>Slider for new dataset size</li> <li>Toggle to compare from-scratch vs. transfer learning</li> </ul> <p>Learning Objective: Understand how and why transfer learning works, and when to freeze or fine-tune pre-trained layers.</p> <p>Assessment: Have students determine which layers should be frozen versus fine-tuned for specific target tasks, with justification based on the visualization.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#8-explainablecnn","title":"8. ExplainableCNN","text":"<p>Clever Name: ExplainableCNN</p> <p>Description: An interactive tool showing various CNN interpretability methods like activation maps, Grad-CAM, and feature visualization on sample images, helping students understand what the network \"sees.\"</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Radio buttons for visualization technique</li> <li>Slider for layer selection</li> <li>Radio buttons for sample images</li> <li>Toggle for superimposing visualizations on original image</li> </ul> <p>Learning Objective: Learn techniques for making CNN decisions more transparent and interpretable.</p> <p>Assessment: Ask students to use different visualization techniques to determine why a CNN might misclassify specific examples and how the network's \"attention\" differs across classes.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#9-optimizenet","title":"9. OptimizeNet","text":"<p>Clever Name: OptimizeNet</p> <p>Description: A simulation demonstrating model optimization techniques like pruning and quantization, showing the effects on model size, inference speed, and accuracy.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Slider for pruning threshold</li> <li>Slider for quantization level (32-bit to 4-bit)</li> <li>Radio buttons for optimization strategy</li> <li>Toggle to show accuracy-vs-size tradeoff</li> </ul> <p>Learning Objective: Understand methods for making CNNs more efficient for deployment on resource-constrained devices.</p> <p>Assessment: Challenge students to optimize a model to fit within specific memory constraints while minimizing accuracy loss, explaining the tradeoffs they encounter.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#10-augmentationstation","title":"10. AugmentationStation","text":"<p>Clever Name: AugmentationStation</p> <p>Description: A hands-on tool demonstrating how data augmentation expands training datasets and improves CNN robustness, showing transformations applied to images and their impact on training.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Toggle switches for different augmentation types (rotation, flip, crop, color)</li> <li>Sliders for augmentation parameters (rotation angle, crop size, etc.)</li> <li>Radio buttons for base image selection</li> <li>Slider for augmentation intensity</li> </ul> <p>Learning Objective: Understand how data augmentation helps prevent overfitting and improves model generalization.</p> <p>Assessment: Have students design an augmentation strategy for specific scenarios (limited data, class imbalance, etc.) and explain how their choices would help the model generalize better.</p>"},{"location":"chapters/counting-model-parameters/","title":"Counting Model Parameters","text":"<p>Many discussion on deep learning models describe a model based on how many \"parameters\" it has.  The bigger the model, the more parameters it has.  But what exactly is a model parameter and how do we calculate the number of parameters for a deep neural network?</p> <p>Let's take a look at the formula for calculating the total parameters in a fully connected neural network:</p> <ol> <li>For weights:</li> <li>Each connection between two layers needs a weight parameter</li> <li>For each pair of adjacent layers: (nodes in previous layer \u00d7 nodes in current layer)</li> <li>Since input layer doesn't have incoming connections, we multiply by (numLayers - 1)</li> <li> <p>Total weight parameters = neuronsPerLayer \u00d7 neuronsPerLayer \u00d7 (numLayers - 1)</p> </li> <li> <p>For biases:</p> </li> <li>Each neuron (except in input layer) has one bias parameter</li> <li> <p>Total bias parameters = neuronsPerLayer \u00d7 (numLayers - 1)</p> </li> <li> <p>Total parameters = Total weight parameters + Total bias parameters</p> </li> </ol> <p>For example, in a network with 3 layers (including input) and 4 neurons per layer:</p> <ul> <li>Weight parameters: 4 \u00d7 4 \u00d7 (3-1) = 32</li> <li>Bias parameters: 4 \u00d7 (3-1) = 8</li> <li>Total parameters: 32 + 8 = 40</li> </ul> <p>We have created an interactive visualization where you can: 1. Adjust the number of layers (2-7) 2. Adjust the neurons per layer (2-10) 3. See the network structure 4. Get a breakdown of weight parameters, bias parameters, and total parameters</p> <p>Would you like me to add any additional features to the visualization?</p>"},{"location":"labs/","title":"Deep Learning Labs","text":"<p>There is a list of hands-on interactive labs that students can do to learn more about Deep Learning.</p> <p>Calculating Tokens Per Second on a Local GPU</p>"},{"location":"labs/calculating-tokens-per-second/","title":"Calculating Tokens Per Second","text":"<p>How quickly a model returns text is a key metric.  Here is a sample program that calculates the number of tokens per second for the Deepseek-r1:7b running in an Ollama framework.  This test was run on my local GPU which is a NVIDIA RTX 2080 Ti with 12GB RAM running CUDA 12.6.  The size of the model was 4.7GB which fits well within the 12GB ram of the GPU.</p> <p>To time the performance of a model we do the following:</p>"},{"location":"labs/calculating-tokens-per-second/#1-record-the-time-before-the-model-runs-with","title":"1. Record the time before the model runs with","text":"<pre><code>start_time = time.time()\n</code></pre>"},{"location":"labs/calculating-tokens-per-second/#2-record-the-end-time-and-calculate-the-elapsed-time","title":"2. Record the end time and calculate the elapsed time","text":"<pre><code>end_time = time.time()\nelapsed_time = end_time - start_time\n</code></pre>"},{"location":"labs/calculating-tokens-per-second/#3-count-the-total-number-of-tokens-in-the-result-and-calculate-the-tokens-per-second","title":"3. Count the total number of tokens in the result and calculate the tokens per second","text":"<pre><code>token_count = len(result.split())\ntokens_per_second = token_count / elapsed_time if elapsed_time &gt; 0 else 0\n</code></pre> <p>Note that we check to make sure that elapsed_time is greater than zero to avoid a divide-by-zero error.</p>"},{"location":"labs/calculating-tokens-per-second/#complete-program","title":"Complete Program","text":"<pre><code>import time\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_ollama.llms import OllamaLLM\n\n# Define your prompt template.\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\n# Initialize the Ollama model.\nmodel = OllamaLLM(model=\"llama3.1\")\n\n# Chain the prompt and the model.\nchain = prompt | model\n\n# Record the start time.\nstart_time = time.time()\n\n# Invoke the chain with your question.\nresult = chain.invoke({\"question\": \"What are the 50 most imporant concepts in a college level course on deep learning?\"})\n\n# Record the end time.\nend_time = time.time()\nelapsed_time = end_time - start_time\n\n# Count tokens in the output.\n# Here we assume tokens are separated by whitespace.\n# (For more accurate token counts, consider using the model's specific tokenizer.)\ntoken_count = len(result.split())\n\n# Calculate tokens per second.\ntokens_per_second = token_count / elapsed_time if elapsed_time &gt; 0 else 0\n\n# Print the result and statistics.\nprint(\"Result:\")\nprint(result)\nprint(\"\\nStatistics:\")\nprint(f\"Token Count: {token_count}\")\nprint(f\"Time Elapsed: {elapsed_time:.2f} seconds\")\nprint(f\"Tokens per Second: {tokens_per_second:.2f}\")\n</code></pre>"},{"location":"labs/calculating-tokens-per-second/#result","title":"Result","text":"<p>What a fascinating question!</p> <p>To tackle this, let's break down the process into manageable steps. Here's a suggested approach:</p>"},{"location":"labs/calculating-tokens-per-second/#step-1-identify-key-topics-in-deep-learning","title":"Step 1: Identify Key Topics in Deep Learning","text":"<p>At a high level, deep learning is a subset of machine learning that focuses on complex models inspired by the structure and function of the brain. The field encompasses various techniques for tasks such as image classification, object detection, speech recognition, natural language processing, and more.</p> <p>Some key topics to consider when identifying important concepts include:</p> <ol> <li>Neural network architectures (e.g., convolutional neural networks, recurrent neural networks)</li> <li>Deep learning frameworks (e.g., TensorFlow, PyTorch)</li> <li>Optimization algorithms (e.g., stochastic gradient descent, Adam)</li> <li>Regularization techniques (e.g., dropout, L1 and L2 regularization)</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#step-2-consider-mathematical-foundations","title":"Step 2: Consider Mathematical Foundations","text":"<p>Deep learning relies heavily on mathematical concepts from linear algebra, calculus, probability theory, and statistics.</p> <p>Some essential math concepts to cover include:</p> <ol> <li>Vector spaces and transformations</li> <li>Matrix operations (e.g., multiplication, inversion)</li> <li>Eigenvalue decomposition and singular value decomposition</li> <li>Probability distributions (e.g., Gaussian, Bernoulli)</li> <li>Expectation and variance</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#step-3-explore-practical-aspects-of-deep-learning","title":"Step 3: Explore Practical Aspects of Deep Learning","text":"<p>This includes the implementation, evaluation, and deployment of deep learning models.</p> <p>Some important practical concepts to consider:</p> <ol> <li>Data preprocessing and augmentation techniques</li> <li>Model selection and hyperparameter tuning</li> <li>Overfitting and regularization strategies</li> <li>Model interpretability and explainability</li> <li>Deployment and scalability in production environments</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#step-4-review-advanced-topics-and-emerging-trends","title":"Step 4: Review Advanced Topics and Emerging Trends","text":"<p>As the field of deep learning continues to evolve, it's essential to stay up-to-date with recent developments.</p> <p>Some advanced topics and emerging trends include:</p> <ol> <li>Transfer learning and pre-trained models</li> <li>Adversarial attacks and defenses</li> <li>Graph neural networks and graph-based techniques</li> <li>Attention mechanisms and transformer architectures</li> <li>Quantum computing and its potential applications in deep learning</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#step-5-refine-the-list-of-important-concepts","title":"Step 5: Refine the List of Important Concepts","text":"<p>Considering the topics identified above, let's prioritize them based on their relevance to a college-level course on deep learning.</p> <p>Here are the top 50 most important concepts:</p>"},{"location":"labs/calculating-tokens-per-second/#mathematical-foundations-1-10","title":"Mathematical Foundations (1-10)","text":"<ol> <li>Vector spaces and transformations</li> <li>Matrix operations (e.g., multiplication, inversion)</li> <li>Eigenvalue decomposition and singular value decomposition</li> <li>Probability distributions (e.g., Gaussian, Bernoulli)</li> <li>Expectation and variance</li> <li>Calculus basics (e.g., gradients, Hessians)</li> <li>Linear algebra review (e.g., determinants, eigenvectors)</li> <li>Probability and statistics review</li> <li>Optimization algorithms (e.g., gradient descent)</li> <li>Regularization techniques (e.g., L1, L2)</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#neural-network-architectures-11-20","title":"Neural Network Architectures (11-20)","text":"<ol> <li>Convolutional neural networks (CNNs)</li> <li>Recurrent neural networks (RNNs)</li> <li>Long short-term memory (LSTM) networks</li> <li>Gated recurrent units (GRUs)</li> <li>Residual connections and skip connections</li> <li>Autoencoders and variational autoencoders</li> <li>U-Net architectures for image segmentation</li> <li>Transformers and self-attention mechanisms</li> <li>Graph neural networks (GNNs)</li> <li>Other specialized architectures (e.g., capsule networks)</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#deep-learning-frameworks-and-tools-21-25","title":"Deep Learning Frameworks and Tools (21-25)","text":"<ol> <li>TensorFlow and Keras APIs</li> <li>PyTorch and Lightning-PyTorch</li> <li>Deep learning frameworks for GPU acceleration (e.g., cuDNN)</li> <li>Model serving and deployment tools (e.g., Docker, Kubernetes)</li> <li>Deep learning software development kits (SDKs) and libraries</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#practical-aspects-of-deep-learning-26-35","title":"Practical Aspects of Deep Learning (26-35)","text":"<ol> <li>Data preprocessing and augmentation techniques</li> <li>Model selection and hyperparameter tuning</li> <li>Overfitting and regularization strategies</li> <li>Model interpretability and explainability</li> <li>Deployment and scalability in production environments</li> <li>Model evaluation metrics (e.g., accuracy, precision)</li> <li>Common pitfalls and debugging techniques</li> <li>Data efficiency and transfer learning strategies</li> <li>Regularization techniques for large models</li> <li>Distributed training and parallelization</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#advanced-topics-and-emerging-trends-36-50","title":"Advanced Topics and Emerging Trends (36-50)","text":"<ol> <li>Transfer learning and pre-trained models</li> <li>Adversarial attacks and defenses</li> <li>Graph neural networks and graph-based techniques</li> <li>Attention mechanisms and transformer architectures</li> <li>Quantum computing and its potential applications in deep learning</li> <li>Exponential family distributions and link functions</li> <li>Causal inference and counterfactual reasoning</li> <li>Generative models (e.g., GANs, VAEs)</li> <li>Time series analysis with LSTM networks</li> <li>Text classification and sentiment analysis</li> <li>Image recognition and object detection</li> <li>Speech recognition and natural language processing</li> <li>Reinforcement learning and deep Q-networks (DQN)</li> <li>Multi-agent systems and distributed decision-making</li> <li>Explainability techniques for complex models</li> </ol> <p>Of course, this list is not exhaustive, but it should give you a solid starting point for creating a comprehensive college-level course on deep learning.</p> <p>How's that?</p>"},{"location":"labs/calculating-tokens-per-second/#inference-statistics","title":"Inference Statistics","text":"<p>Token Count: 687 Time Elapsed: 13.33 seconds Tokens per Second: 51.55</p> <p>Note</p> <p>Depending on your model's tokenization, you might need a more precise token counter   (e.g., using the <code>tiktoken</code> library for models like GPT).</p>"},{"location":"labs/calculating-tokens-per-second/#model-metadata","title":"Model Metadata","text":"<p>Knowing about the structure of a model is key to understanding its performance.</p> <p>Here is the information that ollama provided about deepseek-r1:</p> <pre><code>$ ollama show deepseek-r1\n  Model                          \n    arch                qwen2            \n    parameters          7.6B             \n    quantization        Q4_K_M           \n    context length      131072           \n    embedding length    3584             \n</code></pre> <p>Let's do a deep dive into each of these model metadata fields.</p>"},{"location":"labs/calculating-tokens-per-second/#1-arch-architecture","title":"1.  arch (Architecture):","text":"<ul> <li>What it means: This parameter indicates the underlying neural network architecture on which the model is based.</li> <li>In this case: The model uses the qwen2 architecture. This tells you which design or blueprint the model follows (e.g., similar to transformer-based architectures like GPT or BERT), which influences how it processes input data and generates responses.</li> </ul>"},{"location":"labs/calculating-tokens-per-second/#2-parameters-number-of-parameters","title":"2.  parameters (Number of Parameters):","text":"<ul> <li>What it means: This shows the total number of learnable weights (and biases) in the model. The size of this number is often used as a rough proxy for the model's capacity to learn and represent complex patterns.</li> <li>In this case: The model has 7.6B (7.6 billion) parameters. More parameters generally mean a higher capacity model, though they also require more memory and computational resources during inference.</li> </ul>"},{"location":"labs/calculating-tokens-per-second/#3-quantization","title":"3.  quantization:","text":"<ul> <li>What it means: Quantization refers to reducing the numerical precision of the model's parameters. This process converts high-precision weights (e.g., 32-bit floats) into lower-precision representations (e.g., 4-bit integers) to reduce model size and speed up computations with a minimal loss in accuracy.</li> <li>In this case: The value Q4_K_M indicates that a 4-bit quantization scheme is used. The \"Q4\" part tells you that the weights are represented with 4-bit precision, and \"K_M\" likely refers to the specific quantization method or variant implemented. This balance helps the model run more efficiently while retaining as much performance as possible.</li> </ul>"},{"location":"labs/calculating-tokens-per-second/#4-context-length","title":"4.  context length:","text":"<ul> <li>What it means: This parameter defines the maximum number of tokens the model can process in a single input prompt (or conversation). In transformer-based models, the context length determines how much text the model can consider at one time.</li> <li> <p>In this case: The model can handle a context of up to 131072 tokens. This is an exceptionally long context compared to most language models, which typically support only a few thousand tokens. It enables the model to process very large documents or maintain extended conversations.</p> </li> <li> <p>embedding length:</p> </li> <li> <p>What it means: This is the size (or dimensionality) of the vector used to represent each token in the model's internal computations. In other words, every token in the input is mapped to a vector of this length, which the model uses to capture semantic and syntactic information.</p> </li> <li>In this case: An embedding length of 3584 means that each token is converted into a 3584-dimensional vector. A higher embedding dimension can allow for richer representations but also increases the model's computational complexity.</li> </ul>"},{"location":"labs/calculating-tokens-per-second/#summary","title":"Summary","text":"<ul> <li>Architecture (arch): Defines the model's design (here, qwen2).</li> <li>Parameters: Indicates the model's size in terms of learnable weights (7.6B parameters).</li> <li>Quantization: Shows how the model's weights are stored (using 4-bit precision with the specific scheme Q4_K_M).</li> <li>Context Length: The maximum number of tokens the model can process at once (131072 tokens).</li> <li>Embedding Length: The dimensionality of token representations within the model (3584).</li> </ul> <p>Each of these parameters provides insight into the model's design, capacity, efficiency, and the scale at which it can process input data.</p>"},{"location":"labs/mnist/","title":"MINST","text":"<p>The MNIST (Modified National Institute of Standards and Technology) dataset is the widely-used dataset of handwritten digits that serves as a standard  benchmark for building and testing machine learning models, particularly deep learning models. It contains 60,000 training images  and 10,000 testing images of handwritten digits from 0-9, with each  image being 28x28 pixels in grayscale. </p> <p>The MNIST dataset is often used as a \"Hello World\" project for getting  started with deep learning and neural networks.</p> <p>Here's a detailed walkthrough of building a deep learning model using the MNIST dataset:</p> <p>Project Title: MNIST Handwritten Digit Classification Time Required: 2-3 hours Difficulty Level: Beginner-Intermediate</p>"},{"location":"labs/mnist/#introduction","title":"Introduction","text":"<p>The MNIST project is a fundamental introduction to deep learning where we build a neural network to recognize handwritten digits. This project demonstrates core concepts like data preprocessing, model architecture, training, and evaluation.</p> <p> Sample of MNIST images of grayscale digits (0-9)</p>"},{"location":"labs/mnist/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.x</li> <li>TensorFlow or PyTorch</li> <li>NumPy</li> <li>Matplotlib</li> <li>Basic understanding of neural networks</li> </ul>"},{"location":"labs/mnist/#steps","title":"Steps","text":""},{"location":"labs/mnist/#step-1-setup-conda-environment-and-libs","title":"Step 1: Setup Conda Environment and Libs","text":"<ol> <li>Create a conda environment called \"mnist\" and activate it</li> <li>Import the MNIST dataset through your chosen deep learning framework</li> </ol> <p>Here is a sample UNIX shell script to setup your conda environment and load the appropriate python libraries.</p> <pre><code>conda deactivate\nconda create --name mnist python=3.10\nconda activate mnist\nconda install matplotlib\nconda install numpy\nconda install seaborn\nconda install scikit-learn\n</code></pre>"},{"location":"labs/mnist/#step-2-data-profiling","title":"Step 2: Data Profiling","text":"<p>Data Profiling is the process of understanding what data you have as the distribution of data.  Having a good understanding of your input data is a key aspect to building a useful deep learning model.</p> <p>Without going into too much detail, here is a summary of the MNIST dataset</p> <p>The MNIST dataset contains:    - 60,000 training images (28x28 pixels each)    - 10,000 testing images    - Labels from 0-9 for each image</p> <p> Bar chart showing the distribution of digits in training set to demonstrate data balance Digit Distribution Python Code</p>"},{"location":"labs/mnist/#step-3-data-preprocessing","title":"Step 3: Data Preprocessing","text":"<ol> <li>Normalize pixel values from [0, 255] to [0, 1]</li> <li>Reshape images from 28x28 to 784-dimensional vectors</li> <li>Convert labels to one-hot encoded format</li> <li>Split training data into training and validation sets (80-20 split)</li> </ol> <p>In MNIST digit classification, each target label (0-9) is encoded as a 10-dimensional binary vector: the digit '3' becomes [0,0,0,1,0,0,0,0,0,0], where only the fourth position (index 3) contains a 1.</p> <p> MNIST Data preprocessing steps, showing original image \u2192 normalized image \u2192 flattened vector Mermaid Flowchart</p>"},{"location":"labs/mnist/#step-4-model-architecture","title":"Step 4: Model Architecture","text":"<p>Build a simple neural network with:</p> <ol> <li>Input layer (784 neurons)</li> <li>Hidden layer 1 (128 neurons, ReLU activation)</li> <li>Hidden layer 2 (64 neurons, ReLU activation)</li> <li>Output layer (10 neurons, softmax activation)</li> </ol> <p> Network architecture diagram showing layers, connections, and dimensions.</p>"},{"location":"labs/mnist/#step-5-model-training","title":"Step 5: Model Training","text":"<p>Configure training parameters:</p> <ul> <li>Loss function: Categorical cross entropy</li> <li>Optimizer: Adam</li> <li>Learning rate: 0.001</li> <li>Batch size: 32</li> <li>Epochs: 10</li> </ul>"},{"location":"labs/mnist/#step-6-train-the-model-while-monitoring","title":"Step 6: Train the model while monitoring:","text":"<ul> <li>Training accuracy</li> <li>Validation accuracy</li> <li>Training loss</li> <li>Validation loss</li> </ul> <p>[FIGURE DESCRIPTION: Two line plots showing training/validation accuracy and loss over epochs]</p>"},{"location":"labs/mnist/#step-7-model-evaluation","title":"Step 7: Model Evaluation","text":"<ol> <li>Evaluate model on test set</li> <li>Generate confusion matrix</li> <li>Calculate key metrics:<ol> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1-score</li> </ol> </li> </ol> <p>[FIGURE DESCRIPTION: Confusion matrix heatmap showing prediction results across all digits]</p>"},{"location":"labs/mnist/#step-8-making-predictions","title":"Step 8: Making Predictions","text":"<ol> <li>Select random test images</li> <li>Make predictions</li> <li>Display results with:<ul> <li>Original image</li> <li>Predicted digit</li> <li>Confidence score</li> </ul> </li> </ol> <p>[FIGURE DESCRIPTION: Grid of sample predictions showing original images, predicted digits, and confidence scores]</p>"},{"location":"labs/mnist/#common-challenges-and-solutions","title":"Common Challenges and Solutions","text":"<ol> <li> <p>Overfitting</p> <ul> <li>Solution: Add dropout layers</li> <li>Solution: Use data augmentation</li> </ul> </li> <li> <p>Poor Convergence</p> <ul> <li>Solution: Adjust learning rate</li> <li>Solution: Modify batch size</li> </ul> </li> <li> <p>Low Accuracy</p> <ul> <li>Solution: Deepen network</li> <li>Solution: Increase training time</li> </ul> </li> </ol>"},{"location":"labs/mnist/#tips-for-improvement","title":"Tips for Improvement","text":"<ol> <li> <p>Experiment with different architectures:</p> <ul> <li>Convolutional Neural Networks (CNNs)</li> <li>Different numbers of layers</li> <li>Various activation functions</li> </ul> </li> <li> <p>Try data augmentation:</p> <ul> <li>Random rotations</li> <li>Slight shifts</li> <li>Small zooms</li> </ul> </li> <li> <p>Implement regularization:</p> <ul> <li>Dropout</li> <li>L1/L2 regularization</li> <li>Batch normalization</li> </ul> </li> </ol>"},{"location":"labs/mnist/#project-extensions","title":"Project Extensions","text":"<ol> <li>Build a web interface for real-time digit recognition</li> <li>Implement different model architectures (CNN, ResNet)</li> <li>Create a mobile app for digit recognition</li> <li>Compare performance across different frameworks</li> </ol> <p>[FIGURE DESCRIPTION: Comparison chart showing accuracy vs. complexity for different model architectures]</p>"},{"location":"labs/mnist/#expected-results","title":"Expected Results","text":"<ul> <li>Training accuracy: &gt;98%</li> <li>Test accuracy: &gt;97%</li> <li>Training time: ~5 minutes on CPU</li> <li>Model size: ~5MB</li> </ul> <p>This project serves as an excellent introduction to deep learning, covering essential concepts while providing a solid foundation for more complex projects. The visual nature of the problem makes it easy to understand and debug, while the relatively small dataset size allows for quick experimentation with different approaches.</p>"},{"location":"labs/mnist/#references","title":"References","text":"<p>Claude Transcript</p>"},{"location":"prompts/","title":"Generative AI Prompts","text":"<p>Course Description</p>"},{"location":"prompts/02-bloom-taxonomy-description/","title":"Bloom Taxonomy Description","text":"<p>Prompt</p> <p>Give the course description, apply the Bloom Taxonomy to rewrite the course description into a set of precise levels.</p> <p>Leverage Bloom\u2019s Taxonomy to structure the course, detailing the knowledge, skills, and applications students will develop at each cognitive level (remember, understand, apply, analyze, evaluate, and create).</p> <p>In the description, highlight how generative AI will serve as a primary teaching tool in the following ways:</p> <p>Metaphors and Analogies: Explain how generative AI will dynamically create metaphors that help students understand complex circuits and system concepts intuitively, for example, comparing [XXX] to [YYY] different contexts.</p> <p>Stories and Lessons: Illustrate how AI-generated narratives will guide students through historical milestones in circuit design, with characters like pioneering engineers or scientists to add context and enhance engagement.</p> <p>Content Generation: Describe how AI will develop interactive content\u2014such as real-time quizzes, customized practice problems, and detailed walkthroughs of circuit diagrams\u2014that align with students' progress and areas of improvement.</p> <p>Simulations and Animations: Outline how AI-powered simulations will allow students to interact with virtual circuit models, adjusting parameters and observing outcomes in real-time, and how animations will visually represent phenomena like current flow and voltage changes.</p> <p>Learning Activities and Projects: Explain how generative AI will generate scaffolded project suggestions based on Bloom\u2019s higher-order objectives, like creating their own circuit designs or troubleshooting complex systems. AI should provide formative feedback throughout each step to foster critical thinking.</p> <p>Assessment and Feedback: Describe how AI-driven assessments will go beyond traditional grading, providing personalized feedback loops that prompt self-reflection, track learning progress, and guide students through iterative problem-solving exercises.</p> <p>The course description should aim to inspire curiosity and convey how AI enhances interactivity and personalizes the learning journey, supporting students in achieving a deep and practical understanding of circuits and systems.</p> <p>Give the course description, apply the Bloom Taxonomy to rewrite the course description into a set of precise levels.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-description","title":"Course Description","text":"<p>Organization: University of Minnesota Department of Electrical Engineering and Computer Design (EECD) Course Identifier: EE 4940 Course Title: : Introduction to Deep Learning Offering Date: Spring 2025</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-instructor","title":"Course Instructor","text":"<p>Sang-Hyun Oh, Professor of ECE, graduate faculty in neuroscience, physics, biomedical eng. Email: sang@umn.edu Office: 5-119 Keller Hall Teaching Assistant: Ike Goetzman (PhD candidate in ECE; email: goetz219@umn.edu) Lecture (Jan 22 \u2013 May 5, 2024): Monday/Wednesday 11:15am-12:30pm @ Appleby Hall 3</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-summary","title":"Course Summary","text":"<p>Machine learning, especially deep learning based on neural networks, has shown transformative impacts across life sciences, engineering, and medicine, as recently highlighted by the 2024 Nobel Prizes: </p> <ul> <li>The Physics Nobel was awarded to John Hopfield and Geoffrey Hinton for pioneering contributions to neural networks that enable machine learning</li> <li>Demis Hassabis, John Jumper, and David Baker received the Chemistry Nobel for advancing computational protein design.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#course-overview","title":"Course Overview","text":"<p>EE 4940: Introduction to Deep Learning aims to provide students with both fundamental and applied perspectives on these advances. Beginning with an overview of AI, the course introduces deep neural networks (DNNs) through practical, hands-on examples and guides students through specialized neural network architectures, including</p> <ul> <li>convolutional neural networks (CNNs) for diagnostic applications and </li> <li>recurrent neural networks (RNNs) for time series analysis.</li> </ul> <p>Emphasis is placed on applying neural networks to tackle real-world problems in classification, regression, image analysis, and time-series prediction. Scientific applications are demonstrated as well, such as using DNNs to solve equations.</p> <p>The course concludes with dimensionality reduction techniques, followed by advanced topics such as generative AI, and an introduction to reinforcement learning. </p> <p>Emphasis will be on practical experience with Python-based coding over mathematical theory.</p> <p>By the end of the course, students will have gained familiarity with Python-driven deep learning, hands-on skills, and a working knowledge of deep learning\u2019s potential for a wide range of problems.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-prerequisites","title":"Course Prerequisites","text":"<p>EE 4940 - Special Topics in Electrical and Computer Engineering is recommended but not required.</p> <p>This course is designed to be accessible and introductory, with essential concepts and coding skills introduced as needed.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#expected-workload","title":"Expected Workload","text":"<p>This course (EE 4940) is a 3-credit, lecture format course. The expected workload for this course this semester is, on average, 45 hours per credit, or just over 3 hours per credit hour per week (including lecture). The total workload of 9 hours per week.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-goals-and-objectives","title":"Course Goals and Objectives","text":"<p>EE 4940 aims to equip students with a foundation in deep learning, focusing on core architectures like deep neural networks (DNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).</p> <p>Through hands-on applications, students will learn to apply these models to solve practical problems using real-world datasets, such as cancer diagnostics and time-series analysis. The primary goal of the course is to develop students's ability to implement and troubleshoot neural network models, gaining some practical experiences, and be aware of potential pitfalls/artifacts while using DNNs.</p> <p>By the course's end, students will have a working knowledge of AI\u2019s role in scientific problem-solving, dimensionality reduction techniques, and a simple introduction to generative models and reinforcement learning (RL).</p>"},{"location":"prompts/02-bloom-taxonomy-description/#topics-to-be-covered","title":"Topics To Be Covered","text":""},{"location":"prompts/02-bloom-taxonomy-description/#1-introduction-to-ai-and-deep-learning","title":"1. Introduction to AI and deep learning","text":"<ul> <li>Brief history of AI, neural networks and recent Nobel-winning breakthroughs in physics and chemistry</li> <li>Course setup, tools, and brief introduction to Python for deep learning</li> <li>The universality theorem and the manifold hypothesis</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#2-fundamentals-of-deep-neural-networks-dnns","title":"2. Fundamentals of Deep Neural Networks (DNNs)","text":"<ul> <li>Architecture of DNNs: layers, activation functions, and backpropagation</li> <li>Training, validation, and testing sets;</li> <li>Loss functions, gradient descent, mini-batches</li> <li>Hands-on coding</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#3-convolutional-neural-networks-cnns-applications-in-diagnostic-imaging","title":"3. Convolutional Neural Networks (CNNs) - applications in diagnostic imaging","text":"<ul> <li>CNN architecture</li> <li>Applying CNNs, with MHIST as a case study for cancer diagnosis</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#4-recurrent-neural-networks-rnns-and-time-series-analysis","title":"4. Recurrent Neural Networks (RNNs) and time series analysis","text":"<ul> <li>Introduction to RNNs, LSTMs, for sequency data analysis.</li> <li>Hands-on practice with real-world data.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#5-chips-and-ai","title":"5. Chips and AI","text":"<p>Briefly explore AI chips, neuromorphic devices, and future directions.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#6-ai-for-scientific-problem-solving-using-neural-networks-to-approximate-solutions-for","title":"6. AI for scientific problem solving: Using neural networks to approximate solutions for","text":"<p>Maxwell\u2019s equations</p>"},{"location":"prompts/02-bloom-taxonomy-description/#7-dimensionality-reduction-and-autoencoders","title":"7. Dimensionality reduction and autoencoders","text":""},{"location":"prompts/02-bloom-taxonomy-description/#8-advanced-topics","title":"8. Advanced topics","text":""},{"location":"prompts/02-bloom-taxonomy-description/#course-format","title":"Course Format","text":"<ul> <li>Classroom lectures only</li> <li>Canvas LMS link will be provided</li> <li>Textbook and Required Materials: none</li> <li>Based on lecture notes (uploaded in canvas 1-2 days before class)</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#homework-assignments-and-mini-projects","title":"Homework Assignments and \u201cMini-projects\u201d:","text":"<ul> <li>Regularly-scheduled weekly or biweekly homework assignments and 4 multi-week \u201cmini-projects.\u201d The former will be designed to exercise basic concepts and modest extensions of the technical or computational tools introduced in class; the latter will be crafted to allow students to take a \u201cdeeper dive\u201d by applying techniques developed in the course to data sets arising in various aspects of ECE and life sciences.</li> <li>You are allowed (in fact, encouraged) to work with others when solving the homework and mini project problems. However, solutions submitted for grading should be formulated by each student independently! Late homework and mini-project submissions will be accepted, but without a legitimate (e.g., illness-based) justification, will result in a maximum grade of 80% for that assignment. Specifically, unjustified late assignments will be graded according to the same rubric as on-time assignments, but the corresponding score will be multiplied by 0.8 when entered into the gradebook.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#exams","title":"Exams","text":"<p>There will be one midterm exam and one final exam. The midterm exam will be open book/notes. The final exam will be open book/notes and will be due no earlier than the final exam time specified by the Registrar. Either or both may be take-home exams.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#grading","title":"Grading","text":"<p>The grade distributions for the course variations are provided below.</p> <p>Note: distributions for homework assignments and mini-projects are total; individual submissions will be equally weighted.</p> Deliverable Weight Homework Assignments 40% Mini-projects 40% Midterm Exam 10% Final Exam 10%"},{"location":"prompts/02-bloom-taxonomy-description/#software-requirements","title":"Software Requirements","text":"<p>Through the homework assignments and mini-projects, students will be required to perform a number of computer exercises to become familiar with the computational and practical aspects of deep learning. This will typically entail utilization of Python, often in concert with existing toolboxes such as NumPy, SciPy, Matplotlib, Scikit-learn, and PyTorch (or TensorFlow), all of which are freely available for Windows, Mac, and Linux operating systems.</p> <p>As a perhaps preferred alternative, students may wish to perform all (Python) programming computations using a free cloud-based account (i.e., Google Colab), which will be discussed and established at the beginning of the semester. Large-scale deep learning computations necessary for the final \u201cmini-project\u201d will utilize these resources.</p> <p>Note</p> <p>The Required University Policy Statements were provided in the original PDF of the course description.  They have been removed to allow the generative AI tools to focus on the concept enumeration.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#result","title":"Result","text":"<p>Chat GPT o1 Thought about deep learning course structure for 42 seconds</p> <p>Below is a revised course description for EE 4940: Introduction to Deep Learning structured according to Bloom's Taxonomy. Each level highlights the knowledge, skills, and applications students will develop, as well as how generative AI will serve as a primary teaching tool---through metaphor/analogy generation, narrative stories, adaptive content creation, real-time simulations, scaffolded projects, and personalized assessment.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-information","title":"Course Information","text":"<ul> <li>Organization: University of Minnesota Department of Electrical Engineering and Computer Design (EECD)</li> <li>Course Identifier: EE 4940</li> <li>Course Title: Introduction to Deep Learning</li> <li>Offering Date: Spring 2025</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#instructor-and-teaching-assistant","title":"Instructor and Teaching Assistant","text":"<ul> <li>Instructor:     Sang-Hyun Oh, Professor of ECE (graduate faculty in neuroscience, physics, biomedical engineering)     Email: sang@umn.edu Office: 5-119 Keller Hall</li> <li>Teaching Assistant:     Ike Goetzman (PhD candidate in ECE)     Email: goetz219@umn.edu</li> <li>Lecture Schedule (Jan 22 -- May 5, 2024):     Monday/Wednesday 11:15am-12:30pm @ Appleby Hall 3</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#course-summary_1","title":"Course Summary","text":"<p>Machine learning---particularly deep learning---is transforming fields from life sciences to engineering and medicine. Inspired by recent Nobel Prize recognitions (e.g., John Hopfield and Geoffrey Hinton in Physics for neural network breakthroughs, Demis Hassabis, John Jumper, and David Baker in Chemistry for computational protein design), this course provides an introductory yet hands-on exploration of deep neural networks (DNNs), covering CNNs for image analysis, RNNs for time-series data, dimensionality reduction, an introduction to generative AI, and a glimpse of reinforcement learning.</p> <p>Students will gain practical coding experience in Python with toolkits such as PyTorch or TensorFlow, applying deep learning techniques to real-world datasets (e.g., cancer diagnostics, time-series forecasting).</p>"},{"location":"prompts/02-bloom-taxonomy-description/#prerequisites-format-and-workload","title":"Prerequisites, Format, and Workload","text":"<ul> <li>Prerequisites: EE 4940 - Special Topics in Electrical and Computer Engineering recommended but not required.</li> <li>Format: In-person lectures; no required textbook---lectures and notes via Canvas LMS.</li> <li>Workload: 3-credit lecture course (~9 hours/week total).<ul> <li>Weekly/biweekly homework.</li> <li>4 \"mini-projects\" involving deeper applications of course concepts.</li> <li>One midterm exam (open book/notes) and one final exam (open book/notes).</li> </ul> </li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#course-goals-and-objectives_1","title":"Course Goals and Objectives","text":"<p>By the end of the course, students will:</p> <ul> <li>Have a working knowledge of deep neural network architectures (DNNs, CNNs, RNNs).</li> <li>Understand training, validation, and testing processes, gradient descent, backpropagation.</li> <li>Apply models to diagnostic imaging, time-series forecasting, and other real-world problems.</li> <li>Explore dimensionality reduction and see how DNNs can solve scientific problems (e.g., approximating solutions to Maxwell's equations).</li> <li>Gain a basic introduction to generative AI and reinforcement learning.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#blooms-taxonomy-levels-and-the-role-of-generative-ai","title":"Bloom's Taxonomy Levels and the Role of Generative AI","text":"<p>Below is how this course---and its use of generative AI---aligns with each cognitive level of Bloom's Taxonomy: Remember, Understand, Apply, Analyze, Evaluate, and Create.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#1-remember","title":"1. Remember","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Recall fundamental definitions of artificial intelligence, neural networks, activation functions, backpropagation, and key historical milestones (e.g., Hopfield networks, Hinton's contributions).</li> <li>Identify standard Python libraries and deep learning frameworks (NumPy, SciPy, Matplotlib, PyTorch, TensorFlow).</li> </ul> <p>Generative AI in Action</p> <ul> <li>Metaphors and Analogies: The AI tool will generate simple, memorable analogies (e.g., \"A neural network layer is like a network of interconnected 'filters'---just like the lenses in glasses that each focus on a different part of the scene.\"). These real-time analogies help students quickly remember and anchor new terminology.</li> <li>Content Generation: AI-powered flashcards and micro-quizzes will be automatically created to help students memorize key terms, layer types, and function names.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#2-understand","title":"2. Understand","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Interpret how DNNs learn internal representations of data.</li> <li>Summarize the logic behind training/validation/test splits and how loss functions guide model optimization.</li> <li>Explain the significance of convolutional filters in CNNs and gating mechanisms in RNNs.</li> </ul> <p>Generative AI in Action</p> <ul> <li>Stories and Lessons: AI-generated narratives and role-play scenarios (e.g., \"Geoffrey Hinton explains to a young Demis Hassabis how hidden layers revolutionized speech recognition\") immerse students in historical contexts. These stories make key concepts in deep learning more relatable, fostering deeper understanding.</li> <li>Real-Time Clarifications: When students pose questions, AI can craft new explanations or analogies on the spot---ensuring that confusing topics are re-framed until fully understood.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#3-apply","title":"3. Apply","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Use Python and popular libraries to build, train, and test basic DNN architectures.</li> <li>Implement CNNs for image-based classification tasks and RNNs for time-series predictions.</li> <li>Employ data preprocessing techniques (normalization, reshaping) and iterative model refinement in mini-projects.</li> </ul> <p>Generative AI in Action</p> <ul> <li>Content Generation: Interactive practice problems and short coding challenges adapt to each student's progress. If a learner struggles with CNN kernel concepts, AI will generate targeted exercises.</li> <li>Simulations and Animations: Students use AI-driven virtual labs to manipulate hyperparameters and watch real-time changes in learning curves, explore \"zoomed-in\" views of convolutional filters, or tweak RNN cell parameters to see how model predictions shift---bridging theory and practice.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#4-analyze","title":"4. Analyze","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Compare different network architectures (DNN, CNN, RNN) for specific applications, examining performance metrics (accuracy, precision, recall, F1 scores).</li> <li>Investigate model failures or anomalies, diagnosing potential issues like overfitting or underfitting.</li> <li>Break down real-world datasets (e.g., MHIST for cancer diagnosis) to identify patterns and design suitable learning strategies.</li> </ul> <p>Generative AI in Action</p> <ul> <li>Learning Activities and Projects: AI proposes incremental \"mini-projects\" that require critical data analysis---such as comparing performance across multiple model architectures or investigating feature maps in CNNs. The AI offers real-time hints, ensuring students learn systematic analysis methods.</li> <li>Adaptive Feedback: As students debug their models, AI-powered suggestions highlight where and why training might be failing, prompting reflection on data splits, loss function choices, or architectural mismatches.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#5-evaluate","title":"5. Evaluate","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Assess the reliability, interpretability, and ethical considerations of deep learning solutions.</li> <li>Critique model architecture decisions, optimization techniques, and the trade-offs between computational cost and accuracy.</li> <li>Determine when alternative approaches (e.g., simpler regressions or decision trees) might be more suitable than deep networks.</li> </ul> <p>Generative AI in Action</p> <ul> <li>Assessment and Feedback: AI-driven grading provides personalized feedback loops that go beyond numeric scores. Students receive suggestions (e.g., \"Try re-evaluating your dropout strategy. Are you balancing variance and bias properly?\"), enabling them to evaluate and refine their approach.</li> <li>Peer Review Simulations: Generative AI simulates a peer-review environment, providing constructive critiques of project proposals or final results---students practice giving and receiving evaluations in a supportive, simulated setting.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#6-create","title":"6. Create","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Design and implement original neural network architectures for novel data domains.</li> <li>Experiment with generative models (e.g., Variational Autoencoders or Generative Adversarial Networks) and propose new applications for them.</li> <li>Formulate and conduct reinforcement learning experiments for simple control or decision-making problems.</li> </ul> <p>Generative AI in Action</p> <ul> <li>Learning Activities and Projects: The AI tool auto-generates scaffolded project suggestions that encourage students to develop new solutions---\"Propose a CNN variant for biomedical image segmentation,\" or \"Design an RNN-based text predictor with user-defined constraints.\"</li> <li>Formative Feedback: Throughout the creative process, the AI continually checks code, results, and design decisions, prompting deeper critical thinking. This fosters an iterative cycle of hypothesis-testing and refinement---helping students truly create and innovate.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#how-generative-ai-personalizes-the-learning-journey","title":"How Generative AI Personalizes the Learning Journey","text":"<ol> <li> <p>Metaphors and Analogies</p> <ul> <li>Ongoing generation of simple and relatable comparisons for abstract concepts, ensuring students never get stuck on jargon.</li> <li> <p>Stories and Lessons</p> </li> <li> <p>AI-developed narratives infuse historical milestones (Hopfield nets, Hinton's backpropagation breakthroughs, Hassabis's achievements) with character-driven storytelling.</p> </li> <li> <p>Content Generation</p> </li> <li> <p>Adaptive quizzes, customized practice problems, and circuit-like diagram walk-throughs (adapted to deep learning diagrams) grow alongside the student's evolving competence.</p> </li> <li> <p>Simulations and Animations</p> </li> <li> <p>AI-powered interfaces let students see real-time \"inside\" changes as a model learns---activations, gradients, confusion matrices---supporting interactive exploration.</p> </li> <li> <p>Learning Activities and Projects</p> </li> <li> <p>Scaffolded tasks guide novices from basic classification examples to more advanced challenges---like building an autoencoder for dimensionality reduction or a basic reinforcement learning agent.</p> </li> <li> <p>Assessment and Feedback</p> </li> <li> <p>Detailed, iterative feedback loops ensure students understand why a concept or code block fails, equipping them to self-reflect, iterate, and improve.</p> </li> </ul> </li> </ol>"},{"location":"prompts/02-bloom-taxonomy-description/#grading-overview","title":"Grading Overview","text":"Deliverable Weight Homework Assignments 40% --- --- Mini-projects 40% Midterm Exam 10% Final Exam 10% <p>Note: Late submissions (without legitimate justification) receive up to 80% credit. Exams are open-book/notes and may include take-home components.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#conclusion","title":"Conclusion","text":"<p>By integrating Bloom's Taxonomy with generative AI-powered tools, EE 4940: Introduction to Deep Learning aspires to spark curiosity and deep engagement. Students will remember core principles, understand architectural choices, apply techniques to real data, analyze model results, evaluate ethical and practical considerations, and ultimately create new solutions in the rapidly evolving field of deep learning. This comprehensive and interactive approach---fueled by dynamic AI metaphors, narratives, simulations, and personalized feedback---ensures that each learner can explore and master the foundational and forward-looking aspects of modern neural network technology.</p> <p>o1</p> <p>O</p>"},{"location":"prompts/03-concept-enumeration/","title":"Concept Enumeration","text":"<p>Prompt</p> <p>Please generate a list of the 200 core concepts that need to be covered in this course and the order in which they should be taught.</p> <p>Return numbered list of these core concepts using a single line per concept.  For each concept, just return a short concept name and use acronyms or abbreviations when appropriate.  They will be used as labels in a graph network. Make sure that the concepts are distinct not duplicated in the list.</p> <p>Use title case for the concept names.</p> <p>Start with the most basic foundational concepts and then proceed to the most complex concepts.</p>"},{"location":"prompts/03-concept-enumeration/#workflow","title":"Workflow","text":"<p>Setting the stage</p> <p>I'm compiling a list of 200 unique core concepts for a deep learning course. The order will start with basic concepts and gradually advance to more complex ones.</p> <p>Expanding the list</p> <p>I'm building upon the course outline to include topics like generative AI and RL, ensuring each is distinct and enumerated correctly while avoiding duplicates.</p> <p>Organizing topics</p> <p>Structuring the course into categories like basic foundations, Python tools, and advanced topics such as generative AI and domain-specific architectures to ensure a systematic progression.</p> <p>Finalizing concepts</p> <p>Wrapping up the course with distinctions like \"AI Definition\" and \"ML Definition,\" ensuring clarity with concise, title-case concepts in 200 carefully enumerated lines.</p> <p>Ensuring clarity</p> <p>I'm refining concepts like \"AI Definition\" and \"ML Definition,\" combining title case and potentially using acronyms for distinct clarity. The goal is to maintain precision and avoid redundancy.</p> <p>Detailed line distribution</p> <p>I'm assigning 10-line chunks to various topics, such as Python, ML fundamentals, neural networks, and advanced architectures, ensuring comprehensive coverage across all 200 lines.</p> <p>Progressing through topics</p> <p>I'm charting our course with concise lines to cover \"NN Fundamentals\" and \"Training Processes,\" ensuring distinct and systematic progression across 200 meticulously controlled entries.</p> <p>Venturing into complexity</p> <p>I'm exploring advanced topics in neural networks, including special architectures like Capsule Networks and Tree-Augmented Naive Bayes, as well as Transformer techniques for natural language processing.</p> <p>Unraveling generative AI</p> <p>I'm analyzing advanced generative models and reinforcement learning techniques. This involves delving into diffusion models, denoising diffusion, principal components, and ethical considerations like fairness and privacy.</p> <p>Mapping deployment and application</p> <p>I'm covering GPU and TPU acceleration, memory management, model compression, and practical applications like computer vision and autonomous driving. This completes the cycle, ensuring comprehensive coverage.</p>"},{"location":"prompts/03-concept-enumeration/#result","title":"Result","text":"<p>Thought about deep learning curriculum concepts for 1m 42s</p> <pre><code>AI Definition\nML Definition\nHistory Of AI\nNobel Prizes\nKey Figures\nAI Branches\nSymbolic AI\nData-Driven AI\nNN Origins\nBiological Inspiration\nPython Setup\nConda Environment\nJupyter Notebooks\nNumPy Basics\nPandas Basics\nMatplotlib Basics\nSciPy Basics\nScikit-Learn Overview\nPyTorch Intro\nTensorFlow Intro\nSupervised Vs Unsupervised\nRegression Vs Classification\nOverfitting Vs Underfitting\nTraining-Validation-Test\nBias-Variance Tradeoff\nRegularization Overview\nCross-Validation\nEvaluation Metrics\nLoss Functions\nGradient Descent\nData Preprocessing\nScaling And Normalization\nData Augmentation\nFeature Engineering\nDimensionality Concepts\nData Splits\nData Visualization\nData Cleaning\nData Imbalance\nData Ethics\nPerceptron Model\nMulti-Layer Perceptron\nActivation Functions\nForward Propagation\nBackpropagation\nWeights And Biases\nBatch Training\nMini-Batch Training\nStochastic Training\nLearning Rate\nLoss Surface\nLocal Minima\nVanishing Gradients\nExploding Gradients\nInitialization Methods\nHe Initialization\nXavier Initialization\nMomentum Optimizer\nAdaGrad\nAdam Optimizer\nConvolution Operation\nFilters And Kernels\nPooling Layers\nStride And Padding\nFully Connected Layers\nBatch Normalization\nCNN Architectures\nImageNet\nTransfer Learning Basics\nObject Detection\nSequence Modeling\nVanilla RNN\nLSTM Units\nGRU Units\nBackprop Through Time (BPTT)\nExploding Gradient Control\nLanguage Modeling\nText Generation\nTime-Series Forecasting\nAttention Mechanism\nDeep Architecture\nLayer Stacking\nDepth Vs Width\nResidual Networks (ResNet)\nInception Modules\nDenseNet\nSkip Connections\nDropout\nBatchNorm\nWeight Decay\nCapsule Networks\nGraph Neural Networks (GNNs)\nTransformers Intro\nVision Transformers (ViT)\nSelf-Attention\nPositional Encoding\nBERT Overview\nGPT Overview\nSequence-To-Sequence\nNeural Machine Translation\nGenerative Models\nAutoencoders (AE)\nVariational Autoencoders (VAE)\nGAN Basics\nGenerator Vs Discriminator\nLatent Space\nConditional GAN\nCycleGAN\nStyle Transfer\nText-To-Image\nDiffusion Models\nDenoising Diffusion\nScore Matching\nLikelihood Estimation\nEnergy-Based Models\nFlow-Based Models\nPrompt Engineering\nText Generation Models\nLarge Language Models (LLMs)\nChatGPT Concept\nPCA\nt-SNE\nUMAP\nAutoencoder DR\nManifold Hypothesis\nFeature Space\nPrincipal Components\nNonlinear Embeddings\nHigh-Dimensional Data\nData Visualization Tools\nRL Definition\nMarkov Decision Process (MDP)\nValue Functions\nPolicy Gradient\nQ-Learning\nDeep Q-Network (DQN)\nActor-Critic\nReward Shaping\nExploration Vs Exploitation\nBaseline Algorithms\nModel Interpretability\nExplainable AI (XAI)\nSHAP\nLIME\nFairness\nBias Mitigation\nPrivacy\nSecurity\nAdversarial Examples\nResponsible AI\nGPU Acceleration\nTPU Acceleration\nMemory Management\nModel Compression\nQuantization\nPruning\nEdge Deployment\nCloud Services\nAutoML\nMLOps\nHyperparameter Tuning\nGrid Search\nRandom Search\nBayesian Optimization\nEarly Stopping\nLearning Rate Scheduling\nData Parallelism\nModel Parallelism\nCheckpointing\nVersion Control\nAccuracy\nPrecision\nRecall\nF1 Score\nAUC-ROC\nConfusion Matrix\nTop-1 Vs Top-5 Accuracy\nBLEU Score\nPerplexity\nMean Squared Error (MSE)\nComputer Vision\nNatural Language Processing (NLP)\nSpeech Recognition\nMedical Imaging\nDrug Discovery\nProtein Folding\nAutonomous Driving\nRobotics\nFinancial Forecasting\nRecommender Systems\nProject Scoping\nRequirement Analysis\nData Acquisition\nData Annotation\nResource Allocation\nTeam Collaboration\nVersion Control Workflow\nAgile Methods\nCode Review\nDocumentation\n</code></pre>"},{"location":"prompts/03-concept-enumeration/#getting-the-csv","title":"Getting the CSV","text":"<p>Prompt</p> <p>please return this list in a single csv file with the first column as ConceptID and the second column as ConceptLabel</p> <pre><code>import pandas as pd\n\n# Define the list of concepts\nconcepts = [\n    \"AI Definition\", \"ML Definition\", \"History Of AI\", \"Nobel Prizes\", \"Key Figures\", \"AI Branches\", \"Symbolic AI\",\n    \"Data-Driven AI\", \"NN Origins\", \"Biological Inspiration\", \"Python Setup\", \"Conda Environment\", \"Jupyter Notebooks\",\n    \"NumPy Basics\", \"Pandas Basics\", \"Matplotlib Basics\", \"SciPy Basics\", \"Scikit-Learn Overview\", \"PyTorch Intro\",\n    \"TensorFlow Intro\", \"Supervised Vs Unsupervised\", \"Regression Vs Classification\", \"Overfitting Vs Underfitting\",\n    \"Training-Validation-Test\", \"Bias-Variance Tradeoff\", \"Regularization Overview\", \"Cross-Validation\",\n    \"Evaluation Metrics\", \"Loss Functions\", \"Gradient Descent\", \"Data Preprocessing\", \"Scaling And Normalization\",\n    \"Data Augmentation\", \"Feature Engineering\", \"Dimensionality Concepts\", \"Data Splits\", \"Data Visualization\",\n    \"Data Cleaning\", \"Data Imbalance\", \"Data Ethics\", \"Perceptron Model\", \"Multi-Layer Perceptron\",\n    \"Activation Functions\", \"Forward Propagation\", \"Backpropagation\", \"Weights And Biases\", \"Batch Training\",\n    \"Mini-Batch Training\", \"Stochastic Training\", \"Learning Rate\", \"Loss Surface\", \"Local Minima\",\n    \"Vanishing Gradients\", \"Exploding Gradients\", \"Initialization Methods\", \"He Initialization\",\n    \"Xavier Initialization\", \"Momentum Optimizer\", \"AdaGrad\", \"Adam Optimizer\", \"Convolution Operation\",\n    \"Filters And Kernels\", \"Pooling Layers\", \"Stride And Padding\", \"Fully Connected Layers\", \"Batch Normalization\",\n    \"CNN Architectures\", \"ImageNet\", \"Transfer Learning Basics\", \"Object Detection\", \"Sequence Modeling\",\n    \"Vanilla RNN\", \"LSTM Units\", \"GRU Units\", \"Backprop Through Time (BPTT)\", \"Exploding Gradient Control\",\n    \"Language Modeling\", \"Text Generation\", \"Time-Series Forecasting\", \"Attention Mechanism\", \"Deep Architecture\",\n    \"Layer Stacking\", \"Depth Vs Width\", \"Residual Networks (ResNet)\", \"Inception Modules\", \"DenseNet\",\n    \"Skip Connections\", \"Dropout\", \"BatchNorm\", \"Weight Decay\", \"Capsule Networks\", \"Graph Neural Networks (GNNs)\",\n    \"Transformers Intro\", \"Vision Transformers (ViT)\", \"Self-Attention\", \"Positional Encoding\", \"BERT Overview\",\n    \"GPT Overview\", \"Sequence-To-Sequence\", \"Neural Machine Translation\", \"Generative Models\", \"Autoencoders (AE)\",\n    \"Variational Autoencoders (VAE)\", \"GAN Basics\", \"Generator Vs Discriminator\", \"Latent Space\", \"Conditional GAN\",\n    \"CycleGAN\", \"Style Transfer\", \"Text-To-Image\", \"Diffusion Models\", \"Denoising Diffusion\", \"Score Matching\",\n    \"Likelihood Estimation\", \"Energy-Based Models\", \"Flow-Based Models\", \"Prompt Engineering\", \"Text Generation Models\",\n    \"Large Language Models (LLMs)\", \"ChatGPT Concept\", \"PCA\", \"t-SNE\", \"UMAP\", \"Autoencoder DR\", \"Manifold Hypothesis\",\n    \"Feature Space\", \"Principal Components\", \"Nonlinear Embeddings\", \"High-Dimensional Data\", \"Data Visualization Tools\",\n    \"RL Definition\", \"Markov Decision Process (MDP)\", \"Value Functions\", \"Policy Gradient\", \"Q-Learning\",\n    \"Deep Q-Network (DQN)\", \"Actor-Critic\", \"Reward Shaping\", \"Exploration Vs Exploitation\", \"Baseline Algorithms\",\n    \"Model Interpretability\", \"Explainable AI (XAI)\", \"SHAP\", \"LIME\", \"Fairness\", \"Bias Mitigation\", \"Privacy\",\n    \"Security\", \"Adversarial Examples\", \"Responsible AI\", \"GPU Acceleration\", \"TPU Acceleration\", \"Memory Management\",\n    \"Model Compression\", \"Quantization\", \"Pruning\", \"Edge Deployment\", \"Cloud Services\", \"AutoML\", \"MLOps\",\n    \"Hyperparameter Tuning\", \"Grid Search\", \"Random Search\", \"Bayesian Optimization\", \"Early Stopping\",\n    \"Learning Rate Scheduling\", \"Data Parallelism\", \"Model Parallelism\", \"Checkpointing\", \"Version Control\",\n    \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC-ROC\", \"Confusion Matrix\", \"Top-1 Vs Top-5 Accuracy\",\n    \"BLEU Score\", \"Perplexity\", \"Mean Squared Error (MSE)\", \"Computer Vision\", \"Natural Language Processing (NLP)\",\n    \"Speech Recognition\", \"Medical Imaging\", \"Drug Discovery\", \"Protein Folding\", \"Autonomous Driving\", \"Robotics\",\n    \"Financial Forecasting\", \"Recommender Systems\", \"Project Scoping\", \"Requirement Analysis\", \"Data Acquisition\",\n    \"Data Annotation\", \"Resource Allocation\", \"Team Collaboration\", \"Version Control Workflow\", \"Agile Methods\",\n    \"Code Review\", \"Documentation\"\n]\n\n# Create a DataFrame with ConceptID and ConceptLabel\ndf = pd.DataFrame({\n    \"ConceptID\": range(1, len(concepts) + 1),\n    \"ConceptLabel\": concepts\n})\n\n# Save the DataFrame to a CSV file\nfile_path = \"/mnt/data/EE4940_Core_Concepts.csv\"\ndf.to_csv(file_path, index=False)\n\nfile_path\n</code></pre>"},{"location":"prompts/03-concept-enumeration/#result_1","title":"Result","text":"<p>Deep Learning Concepts</p>"},{"location":"prompts/04-concept-dependencies/","title":"Concept Dependencies","text":"<p>Prompt</p> <p>Below is a CSV file of concepts in a course on Deep Learning for junior or senior college students.  Please use this CSV file of 200 concepts for the next step.</p> <p>Next, please create a fully connected concept dependency graph for the concepts in this course. In this step, for each Concept in the CSV file, create a new list of other concepts on which it depends.  If a concept depends on a specific concept that is not listed, then we will need to add it as a new concept. This will help us determine the teaching order of the Concepts.</p> <p>Return the new list as a CSV file using the following format:</p> <ol> <li>The first column in the CSV file is a ConceptID you assigned in the list above.</li> <li>The second column is the ConceptLabel</li> <li>The third column is called Dependencies.  It's format is a pipe-delimited list of the Concept IDs that the concept depends upon.</li> </ol> <p>Check your results to make sure that all concepts are connected to at least one other concept.</p> <p>Foundation Concepts are concepts that have no dependant concepts in this set.  They have an empty third column of Dependencies.</p> <p>In this example, the following concepts have no dependencies (empty Dependencies column). They serve as possible \u201cstarting points\u201d in the curriculum:</p> <ul> <li>1: AI Definition</li> <li>3: History Of AI</li> <li>10: Biological Inspiration</li> <li>11: Python Setup</li> <li>191: Project Scoping</li> </ul> <p>All other concepts depend on at least one earlier concept\u2014ensuring every concept is connected in one unified graph. Feel free to modify or expand these dependencies to match your exact course structure.</p>"},{"location":"prompts/04-concept-dependencies/#response","title":"Response","text":"<p>Foundation Concepts</p>"},{"location":"prompts/05-concept-taxonomy/","title":"Concept Taxonomy","text":"<p>Prompt</p> <p>Given the list of concepts, your next task is to organize these concepts into a taxonomy by grouping them into appropriate categories. Each category should include concepts that share similar characteristics, functions, or applications.</p> <p>Suggest 10-12 categories for the taxonomy and assign each concept to one or more categories. Ensure that the categories cover a range of concept types, including abstract ideas, practical applications, Python libraries, common misunderstandings, and real-world examples.</p> <p>Avoid redundancy and ensure that each category is distinct.</p> <p>Your first category is \"Foundational Concepts\", your second is \"Terms and Definitions\".   A final category might be \"Goals\".</p>"},{"location":"prompts/05-concept-taxonomy/#workflow","title":"Workflow","text":"<p>ChatGPT o1 thought about course taxonomy and categories for 3m 36s</p> <p>Crafting a taxonomy</p> <p>I'm organizing 200 concepts into 10-12 categories, including foundational concepts, terms and definitions, and goals. Ensuring a variety of concept types without redundancy.</p> <p>Shaping the categories</p> <p>I'm organizing the taxonomy with 10-12 categories, focusing on topics like foundational concepts, key tools, data engineering, core ML, model architectures, optimization, advanced topics, and real-world examples.</p> <p>Clarifying \"Goals\"</p> <p>Hmm, I'm thinking about whether \"Goals\" means measuring success, project scoping, or responsible AI like interpretability. Each concept will fit into one of the 10 categories.</p> <p>Shaping the framework</p> <p>I'm honing in on Python tools, data engineering, Neural Network Architectures, Reinforcement Learning, and integration with interpretation, ethics, fairness, MLOps, and project workflow.</p> <p>Mapping the categories</p> <p>I'm working through merging \"Interpretation, Ethics, &amp; Fairness\" with \"Goals,\" but it seems intricate and possibly ineffective. It's interesting to see these elements might be better handled separately.</p> <p>Refining the taxonomy</p> <p>I'm merging \"Sequence &amp; Natural Language\" with \"Advanced &amp; Specialized Topics,\" and integrating \"Reinforcement Learning\" into the revised category. This results in a sleeker, more coherent framework.</p> <p>Finalizing the categories</p> <p>I'm looking at a streamlined taxonomy with 11 categories, including Foundational Concepts, Python Tools, Data Engineering, Advanced Topics, and Real-World Applications. Assigning concepts to these categories now.</p> <p>Voucher assignment</p> <p>Deciding whether to include \"Nobel Prizes\" in \"Foundational Concepts\" or \"History of AI\" is tricky. I'll weigh the typical context for each category to make a balanced decision.</p> <p>Piecing together the foundation</p> <p>I'm carefully grouping concepts like AI and ML definitions, AI history, Nobel Prizes, key figures, AI branches, and neural networks into foundational concepts.</p> <p>Mapping out key concepts</p> <p>I'm digging into foundational concepts like AI and ML definitions, AI history, Nobel Prizes, key figures, AI branches, and neural network origins, ensuring a balanced and comprehensive approach.</p> <p>Clarifying definitions</p> <p>I'm thinking through terms for \"Terms and Definitions,\" focusing on clear, concise definitions like Supervised vs. Unsupervised learning, Regression vs. Classification, and evaluation metrics.</p> <p>Mapping concepts</p> <p>Pulling together Python tools, data engineering, neural network architectures, and advanced topics like transformers and generative models. Assigning terms systematically to refine clarity and avoid redundancy.</p> <p>Refining category alignment</p> <p>Consider placing \"Attention Mechanism\" under \"Neural Network Architectures.\" \"Reinforcement Learning\" and \"Advanced Recognition Techniques\" seem to fit well inside \"Advanced &amp; Specialized Topics.\"</p> <p>Merging concepts</p> <p>I'm contemplating combining \"Reinforcement Learning\" with \"Advanced &amp; Specialized Topics,\" and \"Project Scoping\" seems to fit better in \"MLOps &amp; Project Workflow\" or as \"Goals.\"</p> <p>Mapping real applications</p> <p>I'm organizing concepts on ethical AI, MLOps, and project workflow, including resource allocation, version control, and agile methods. Next up: integrating \"Project Scoping\" and refining \"Goals.\"</p> <p>Clarifying project metrics</p> <p>I'm thinking about including \"Evaluation Metrics\" and \"Project Scoping\" under \"Goals,\" refining their roles to ensure they align cohesively within the broader context.</p> <p>Integrating distinct elements</p> <p>I'm honing the \"Goals\" category with evaluation metrics, precision, recall, and other metrics. For dimensionality reduction and data visualization tools, they seem to fit better in \"Data Preparation &amp; Engineering\" or \"Training &amp; Optimization.\"</p> <p>Mapping concepts</p> <p>I'm integrating dimensionality reduction and data visualization tools into \"Data Preparation &amp; Engineering,\" and placing \"Prompt Engineering\" in \"Advanced &amp; Specialized Topics\" or \"Sequence &amp; Natural Language.\"</p> <p>Mapping as I go</p> <p>Integrating concepts like \"Prompt Engineering\" into \"Advanced &amp; Specialized Topics\" and aligning \"Attention Mechanism\" with \"Neural Network Architectures.\" Also, I'm finalizing the assignment of various ML and data concepts.</p> <p>Mapping out assignments</p> <p>There's a clear progression: integrating \"Data Ethics\" into \"Interpretation, Ethics, &amp; Fairness,\" placing \"ImageNet\" in \"Neural Network Architectures,\" and categorizing \"Text Generation\" with \"Advanced &amp; Specialized Topics.\"</p> <p>Organizing concepts</p> <p>I'm grouping \"Language Modeling\" and \"Text Generation\" into \"Advanced &amp; Specialized Topics,\" and aligning \"Attention Mechanism\" with \"Neural Network Architectures\". Robotics fits best in \"Real-World Applications.\"</p> <p>Organizing and refining</p> <p>Assigning concepts like \"Loss Surface\" and \"Data Annotation\" to \"Training &amp; Optimization\" and \"Data Prep &amp; MLOps\" respectively. \"Robotics\" fits well in \"Real-World Applications.\"</p> <p>Finalizing concepts</p> <p>I'm wrapping up by mapping \"ChatGPT Concept\" and \"Adversarial Examples\" to \"Advanced &amp; Specialized Topics\" and \"Interpretation, Ethics, &amp; Fairness,\" ensuring each concept is logically categorized.</p> <p>Finalizing mappings</p> <p>Assigning concepts like \"Quantization,\" \"Pruning,\" and \"Edge Deployment\" to \"Advanced &amp; Specialized Topics,\" and ensuring all concepts fit logically within the intended categories.</p> <p>Finalizing concepts</p> <p>I'm completing the mapping of advanced concepts like \"DQN,\" \"Actor-Critic,\" and \"Reward Shaping\" to \"Advanced &amp; Specialized Topics,\" and ensuring all terms are logically categorized.</p> <p>Piecing together assignments</p> <p>I'm ensuring \"SciPy\" fits into \"Python Tools,\" aligning \"Precision,\" \"Recall,\" \"F1,\" and \"AUC-ROC\" with \"Goals,\" and placing \"Data Splits\" and \"Data Visualization Tools\" under \"Data Prep &amp; Engineering.\"</p> <p>Aligning the framework</p> <p>Aligning \"GPU Acceleration\" and \"TPU Acceleration\" with \"Advanced &amp; Specialized Topics,\" \"Project Scoping\" and \"Version Control Workflow\" with \"MLOps &amp; Project Workflow.\"</p> <p>Refining concept categories</p> <p>I'm evolving \"Adam Optimizer\" and \"Hyperparameter Tuning\" within \"Training &amp; Optimization.\" \"Data Visualization\" and \"PCA\" fit snugly under \"Data Prep &amp; Engineering.\"</p> <p>Finalizing taxonomy</p> <p>OK, let me see. I'm ensuring each concept lands in the right category, refining concepts like \"Weight Decay\" and \"Evaluation Metrics.\" I'm setting a clear, concise list of 11 categories.</p> <p>Mapping out preferences</p> <p>I'm factoring in the need for brevity and the user's desire for a range of concept types. Considering a summarized 11-category list with multi-category assignments, focusing on practical applications and Python libraries.</p>"},{"location":"prompts/05-concept-taxonomy/#result","title":"Result","text":"<p>Below is an example taxonomy of the 200 concepts, grouped into 11 distinct categories. Each concept may appear in more than one category if it makes sense pedagogically (e.g., a concept can be both a \"term/definition\" and also relevant to \"training &amp; optimization\").</p> <p>In practice, you might refine these groupings further or redistribute concepts according to your specific course design. The primary goals are (1) to ensure that each concept is accounted for at least once, (2) to keep categories mutually distinct, and (3) to cover a wide range of types (abstract ideas, practical tools, potential pitfalls, real-world use cases, etc.).</p>"},{"location":"prompts/05-concept-taxonomy/#1-foundational-concepts","title":"1. Foundational Concepts","text":"<p>These are broad, high-level ideas or historical touchpoints on which other topics build.</p> <ul> <li>(1) AI Definition</li> <li>(2) ML Definition</li> <li>(3) History Of AI</li> <li>(4) Nobel Prizes</li> <li>(5) Key Figures</li> <li>(6) AI Branches</li> <li>(7) Symbolic AI</li> <li>(8) Data-Driven AI</li> <li>(9) NN Origins</li> <li>(10) Biological Inspiration</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#2-terms-and-definitions","title":"2. Terms and Definitions","text":"<p>Key terminology and conceptual distinctions often introduced early to clarify the field's language.</p> <ul> <li>(21) Supervised Vs Unsupervised</li> <li>(22) Regression Vs Classification</li> <li>(23) Overfitting Vs Underfitting</li> <li>(25) Bias-Variance Tradeoff</li> <li>(35) Dimensionality Concepts</li> <li>(39) Data Imbalance</li> <li>(51) Loss Surface</li> <li>(52) Local Minima</li> <li>(62) Filters And Kernels</li> <li>(100) Neural Machine Translation</li> <li>(118) Text Generation Models</li> <li>(131) RL Definition</li> <li>(139) Exploration Vs Exploitation</li> </ul> <p>(Note that some of these also appear under other categories if they are central to training, optimization, or advanced topics.)</p>"},{"location":"prompts/05-concept-taxonomy/#3-python-tools-libraries","title":"3. Python Tools &amp; Libraries","text":"<p>Practical software tools used throughout the course for coding, data manipulation, and experimentation.</p> <ul> <li>(11) Python Setup</li> <li>(12) Conda Environment</li> <li>(13) Jupyter Notebooks</li> <li>(14) NumPy Basics</li> <li>(15) Pandas Basics</li> <li>(16) Matplotlib Basics</li> <li>(17) SciPy Basics</li> <li>(18) Scikit-Learn Overview</li> <li>(19) PyTorch Intro</li> <li>(20) TensorFlow Intro</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#4-data-preparation-engineering","title":"4. Data Preparation &amp; Engineering","text":"<p>Concepts and techniques for acquiring, cleaning, organizing, and transforming data before (and during) model development.</p> <ul> <li>(31) Data Preprocessing</li> <li>(32) Scaling And Normalization</li> <li>(33) Data Augmentation</li> <li>(34) Feature Engineering</li> <li>(36) Data Splits</li> <li>(37) Data Visualization</li> <li>(38) Data Cleaning</li> <li>(40) Data Ethics (also appears in Ethics/Ethical Use)</li> <li>(121) PCA</li> <li>(122) t-SNE</li> <li>(123) UMAP</li> <li>(124) Autoencoder DR</li> <li>(125) Manifold Hypothesis</li> <li>(126) Feature Space</li> <li>(127) Principal Components</li> <li>(128) Nonlinear Embeddings</li> <li>(129) High-Dimensional Data</li> <li>(130) Data Visualization Tools</li> <li>(193) Data Acquisition</li> <li>(194) Data Annotation</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#5-training-optimization","title":"5. Training &amp; Optimization","text":"<p>How models learn, common pitfalls, and methods to optimize performance.</p> <ul> <li>(23) Overfitting Vs Underfitting</li> <li>(24) Training-Validation-Test</li> <li>(25) Bias-Variance Tradeoff</li> <li>(26) Regularization Overview</li> <li>(27) Cross-Validation</li> <li>(29) Loss Functions</li> <li>(30) Gradient Descent</li> <li>(43) Activation Functions</li> <li>(44) Forward Propagation</li> <li>(45) Backpropagation</li> <li>(46) Weights And Biases</li> <li>(47) Batch Training</li> <li>(48) Mini-Batch Training</li> <li>(49) Stochastic Training</li> <li>(50) Learning Rate</li> <li>(52) Local Minima</li> <li>(53) Vanishing Gradients</li> <li>(54) Exploding Gradients</li> <li>(58) Momentum Optimizer</li> <li>(59) AdaGrad</li> <li>(60) Adam Optimizer</li> <li>(76) Exploding Gradient Control</li> <li>(90) Weight Decay</li> <li>(161) Hyperparameter Tuning</li> <li>(162) Grid Search</li> <li>(163) Random Search</li> <li>(164) Bayesian Optimization</li> <li>(165) Early Stopping</li> <li>(166) Learning Rate Scheduling</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#6-neural-network-architectures","title":"6. Neural Network Architectures","text":"<p>Core structures and design choices in building various types of deep neural networks.</p> <ul> <li>(41) Perceptron Model</li> <li>(42) Multi-Layer Perceptron</li> <li>(55) Initialization Methods</li> <li>(56) He Initialization</li> <li>(57) Xavier Initialization</li> <li>(61) Convolution Operation</li> <li>(63) Pooling Layers</li> <li>(64) Stride And Padding</li> <li>(65) Fully Connected Layers</li> <li>(66) Batch Normalization</li> <li>(67) CNN Architectures</li> <li>(68) ImageNet</li> <li>(69) Transfer Learning Basics</li> <li>(70) Object Detection (also a real-world CV application)</li> <li>(71) Sequence Modeling</li> <li>(72) Vanilla RNN</li> <li>(73) LSTM Units</li> <li>(74) GRU Units</li> <li>(75) Backprop Through Time (BPTT)</li> <li>(79) Time-Series Forecasting</li> <li>(80) Attention Mechanism</li> <li>(81) Deep Architecture</li> <li>(82) Layer Stacking</li> <li>(83) Depth Vs Width</li> <li>(84) Residual Networks (ResNet)</li> <li>(85) Inception Modules</li> <li>(86) DenseNet</li> <li>(87) Skip Connections</li> <li>(88) Dropout</li> <li>(89) BatchNorm (duplicate of 66's concept family)</li> <li>(91) Capsule Networks</li> <li>(92) Graph Neural Networks (GNNs)</li> <li>(93) Transformers Intro</li> <li>(94) Vision Transformers (ViT)</li> <li>(95) Self-Attention</li> <li>(96) Positional Encoding</li> <li>(97) BERT Overview</li> <li>(98) GPT Overview</li> <li>(99) Sequence-To-Sequence</li> <li>(101) Generative Models</li> <li>(102) Autoencoders (AE)</li> <li>(103) Variational Autoencoders (VAE)</li> <li>(104) GAN Basics</li> <li>(105) Generator Vs Discriminator</li> <li>(106) Latent Space</li> <li>(107) Conditional GAN</li> <li>(108) CycleGAN</li> <li>(109) Style Transfer</li> <li>(110) Text-To-Image</li> <li>(111) Diffusion Models</li> <li>(112) Denoising Diffusion</li> <li>(113) Score Matching</li> <li>(114) Likelihood Estimation</li> <li>(115) Energy-Based Models</li> <li>(116) Flow-Based Models</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#7-advanced-specialized-topics","title":"7. Advanced &amp; Specialized Topics","text":"<p>Cutting-edge methods, hardware accelerators, reinforcement learning, and larger-scale automation.</p> <ul> <li>(71) Sequence Modeling (repeated if focusing on advanced sequences)</li> <li>(77) Language Modeling</li> <li>(78) Text Generation</li> <li>(117) Prompt Engineering</li> <li>(119) Large Language Models (LLMs)</li> <li>(120) ChatGPT Concept</li> <li>(131) RL Definition</li> <li>(132) Markov Decision Process (MDP)</li> <li>(133) Value Functions</li> <li>(134) Policy Gradient</li> <li>(135) Q-Learning</li> <li>(136) Deep Q-Network (DQN)</li> <li>(137) Actor-Critic</li> <li>(138) Reward Shaping</li> <li>(140) Baseline Algorithms</li> <li>(151) GPU Acceleration</li> <li>(152) TPU Acceleration</li> <li>(153) Memory Management</li> <li>(154) Model Compression</li> <li>(155) Quantization</li> <li>(156) Pruning</li> <li>(157) Edge Deployment</li> <li>(158) Cloud Services</li> <li>(159) AutoML</li> <li>(167) Data Parallelism</li> <li>(168) Model Parallelism</li> <li>(169) Checkpointing</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#8-real-world-applications","title":"8. Real-World Applications","text":"<p>Domains and example uses that illustrate how AI/ML solutions are deployed.</p> <ul> <li>(70) Object Detection (also in CNNs)</li> <li>(181) Computer Vision</li> <li>(182) Natural Language Processing (NLP)</li> <li>(183) Speech Recognition</li> <li>(184) Medical Imaging</li> <li>(185) Drug Discovery</li> <li>(186) Protein Folding</li> <li>(187) Autonomous Driving</li> <li>(188) Robotics</li> <li>(189) Financial Forecasting</li> <li> <p>(190) Recommender Systems</p> </li> <li> <p>Interpretation, Ethics, &amp; Fairness</p> </li> </ul> <p>Ensuring AI systems are transparent, fair, secure, and used responsibly.</p> <ul> <li>(40) Data Ethics</li> <li>(141) Model Interpretability</li> <li>(142) Explainable AI (XAI)</li> <li>(143) SHAP</li> <li>(144) LIME</li> <li>(145) Fairness</li> <li>(146) Bias Mitigation</li> <li>(147) Privacy</li> <li>(148) Security</li> <li>(149) Adversarial Examples</li> <li>(150) Responsible AI</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#10-mlops-project-workflow","title":"10. MLOps &amp; Project Workflow","text":"<p>Practical, organizational, and life-cycle aspects of putting ML/DL models into production and managing teams.</p> <ul> <li>(160) MLOps</li> <li>(170) Version Control</li> <li>(191) Project Scoping</li> <li>(192) Requirement Analysis</li> <li>(193) Data Acquisition (also in Data Prep)</li> <li>(194) Data Annotation (also in Data Prep)</li> <li>(195) Resource Allocation</li> <li>(196) Team Collaboration</li> <li>(197) Version Control Workflow</li> <li>(198) Agile Methods</li> <li>(199) Code Review</li> <li>(200) Documentation</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#11-goals","title":"11. Goals","text":"<p>Metrics and broader objectives that guide model selection, validation, and success criteria.</p> <ul> <li>(28) Evaluation Metrics</li> <li>(171) Accuracy</li> <li>(172) Precision</li> <li>(173) Recall</li> <li>(174) F1 Score</li> <li>(175) AUC-ROC</li> <li>(176) Confusion Matrix</li> <li>(177) Top-1 Vs Top-5 Accuracy</li> <li>(178) BLEU Score</li> <li>(179) Perplexity</li> <li>(180) Mean Squared Error (MSE)</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#notes-on-overlaps","title":"Notes on Overlaps","text":"<ul> <li>Some items (e.g., \"Overfitting Vs Underfitting\") appear in both Terms and Definitions and Training &amp; Optimization, because they are foundational distinctions and deeply tied to optimization.</li> <li>\"Object Detection,\" \"Data Ethics,\" \"Data Acquisition,\" and several others also appear in more than one category when relevant.</li> </ul> <p>This multi-category approach reflects the reality that many deep learning concepts straddle theoretical and practical lines and can belong in more than one \"bucket.\" Depending on your course or textbook structure, you may wish to split them differently or provide cross-references.</p>"},{"location":"prompts/05-concept-taxonomy/#final-csv-file","title":"Final CSV File","text":"<p>Prompt</p> <p>That was excellent.  Now please regenerate the CSV file, but this time please add a new column for TaxonomyID.  Use the numbers for each category as the TaxonomyID.</p> <p>The format of the new file is:</p> <ol> <li>Column 1: ConceptID</li> <li>Column 2: ConceptLabel</li> <li>Column 3: Dependencies</li> <li>Column 4: TaxonomyID</li> </ol>"},{"location":"prompts/06-glossary-of-terms/","title":"Generating a Glossary","text":"<p>Prompt</p> <p>Next we need a glossary of terms.  For each of the 200 concepts, please create a ISO 11179 definition that is precise, concise, distinct and non-circular.  When appropriate, give an example of how this term is used in the course.  Return each concept label in Markdown level 4 and the definition for the term in the body text.  Use Example: before the example.</p> <p>Return the entire glossary of terms in a single markdown file.  Place \"# Glossary of Terms\" at the top of the file.  Return the terms in alphabetical order.</p>"},{"location":"prompts/06-glossary-of-terms/#result","title":"Result","text":""},{"location":"prompts/07-equations/","title":"Generating Commonly Used Deep Learning Equations","text":"<p>Prompt</p> <p>Create a list of the 100 most commonly used equations in a college level course in deep learning.</p> <p>For each equation, return the following:</p> <ol> <li>The name of the equation in markdown level 4.</li> <li>Within the body, return the equation in LaTeX format</li> <li>Surround the equation with dollar signs</li> <li>For each variable in the equation, add a \"where:\" that defines each variable</li> <li>Put blank lines around the list of variables</li> <li>Wrap the variables names in dollar signs</li> </ol> <p>Examples:</p>"},{"location":"prompts/07-equations/#slope-and-intercept","title":"Slope and Intercept","text":"<p>\\(y = m * x + b\\)</p> <p>where:</p> <ul> <li>\\(y\\) is the vertical axis</li> <li>\\(x\\) is the horizontal axis</li> <li>\\(m\\) is the slope</li> <li>\\(b\\) is the intercept</li> </ul>"},{"location":"prompts/07-equations/#area-of-a-circle","title":"Area of a Circle","text":"<p>\\(A = 2\\pi r^2\\) where:</p> <ul> <li>\\(A\\) is the area of the circle</li> <li>\\(r\\) is the radius</li> </ul>"},{"location":"prompts/07-equations/#response","title":"Response","text":"<p>Below are 100 commonly encountered equations in a college-level deep learning course, presented in alphabetical order. Each entry has:</p> <ol> <li>A heading in Markdown level 4.</li> <li>The equation in LaTeX, surrounded by dollar signs.</li> <li>A where: section defining each variable, with each variable in dollar signs and blank lines around the list.</li> </ol>"},{"location":"prompts/07-equations/#100-common-equations-in-deep-learning","title":"100 Common Equations in Deep Learning","text":"<p>Below is a curated list of 100 frequently encountered equations in a college-level Deep Learning course. Each entry includes:</p> <ul> <li>A name (in level 4 markdown).</li> <li>The equation (in LaTeX, surrounded by single dollar signs).</li> <li>A \u201cwhere:\u201d section explaining each variable.</li> </ul> <p>The equations are grouped loosely by theme (basic algebra, loss functions, optimization, activations, convolution, RNNs, attention mechanisms, etc.) but may appear in any Deep Learning curriculum.</p>"},{"location":"prompts/07-equations/#1-mean-squared-error-mse","title":"1. Mean Squared Error (MSE)","text":"<p>\\(L_{\\text{MSE}} = \\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)} - \\hat{y}^{(i)})^2\\)</p> <p>where:</p> <ul> <li>\\(N\\) is the number of samples</li> <li>\\(y^{(i)}\\) is the true label for the \\(i\\)-th sample</li> <li>\\(\\hat{y}^{(i)}\\) is the predicted value for the \\(i\\)-th sample</li> </ul>"},{"location":"prompts/07-equations/#2-cross-entropy-loss-binary","title":"2. Cross-Entropy Loss (Binary)","text":"<p>\\(L_{\\text{CE}} = -\\frac{1}{N}\\sum_{i=1}^{N}\\Big[y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\\Big]\\)</p> <p>where:</p> <ul> <li>\\(N\\) is the number of samples</li> <li>\\(y^{(i)} \\in \\{0, 1\\}\\) is the true binary label</li> <li>\\(\\hat{y}^{(i)}\\) is the predicted probability of the positive class</li> </ul>"},{"location":"prompts/07-equations/#3-cross-entropy-loss-multi-class","title":"3. Cross-Entropy Loss (Multi-Class)","text":"<p>\\(L_{\\text{CE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{c}^{(i)} \\log(\\hat{y}_{c}^{(i)})\\)</p> <p>where:</p> <ul> <li>\\(N\\) is the number of samples</li> <li>\\(C\\) is the number of classes</li> <li>\\(y_{c}^{(i)} \\in \\{0,1\\}\\) indicates the true class of sample \\(i\\)</li> <li>\\(\\hat{y}_{c}^{(i)}\\) is the predicted probability for class \\(c\\)</li> </ul>"},{"location":"prompts/07-equations/#4-softmax-function","title":"4. Softmax Function","text":"<p>\\(\\hat{y}_j = \\frac{\\exp(z_j)}{\\sum_{k=1}^{C}\\exp(z_k)}\\)</p> <p>where:</p> <ul> <li>\\(z_j\\) is the logit for class \\(j\\)</li> <li>\\(C\\) is the total number of classes</li> <li>\\(\\hat{y}_j\\) is the probability assigned to class \\(j\\)</li> </ul>"},{"location":"prompts/07-equations/#5-sigmoid-logistic-function","title":"5. Sigmoid (Logistic) Function","text":"<p>\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input or logit</li> <li>\\(\\sigma(z)\\) is the output between 0 and 1</li> </ul>"},{"location":"prompts/07-equations/#6-tanh-function","title":"6. Tanh Function","text":"<p>\\(\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> <li>\\(\\tanh(z)\\) outputs values in the range \\([-1, 1]\\)</li> </ul>"},{"location":"prompts/07-equations/#7-rectified-linear-unit-relu","title":"7. Rectified Linear Unit (ReLU)","text":"<p>\\(\\text{ReLU}(z) = \\max(0, z)\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> <li>\\(\\text{ReLU}(z)\\) is zero for negative \\(z\\) and \\(z\\) itself if \\(z &gt; 0\\)</li> </ul>"},{"location":"prompts/07-equations/#8-leaky-relu","title":"8. Leaky ReLU","text":"<p>\\(\\text{LeakyReLU}(z) =  \\begin{cases} z &amp; \\text{if } z \\ge 0\\\\ \\alpha \\, z &amp; \\text{if } z &lt; 0 \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> <li>\\(\\alpha\\) is a small positive slope (e.g., 0.01) for negative \\(z\\)</li> </ul>"},{"location":"prompts/07-equations/#9-weighted-sum-of-inputs","title":"9. Weighted Sum of Inputs","text":"<p>\\(z = \\sum_{i=1}^{d} w_i x_i + b\\)</p> <p>where:</p> <ul> <li>\\(d\\) is the number of input features</li> <li>\\(w_i\\) is the weight for feature \\(x_i\\)</li> <li>\\(x_i\\) is the \\(i\\)-th input</li> <li>\\(b\\) is the bias term</li> </ul>"},{"location":"prompts/07-equations/#10-neural-network-output-one-layer","title":"10. Neural Network Output (One Layer)","text":"<p>\\(\\hat{y} = f\\bigg(\\sum_{i=1}^{d} w_i x_i + b\\bigg)\\)</p> <p>where:</p> <ul> <li>\\(f(\\cdot)\\) is an activation function</li> <li>\\(w_i\\), \\(x_i\\), and \\(b\\) are defined as above</li> </ul>"},{"location":"prompts/07-equations/#11-gradient-descent-parameter-update","title":"11. Gradient Descent (Parameter Update)","text":"<p>\\(\\theta \\leftarrow \\theta - \\eta \\,\\nabla_\\theta L(\\theta)\\)</p> <p>where:</p> <ul> <li>\\(\\theta\\) represents parameters (weights, biases)</li> <li>\\(\\eta\\) is the learning rate</li> <li>\\(L(\\theta)\\) is the loss function</li> <li>\\(\\nabla_\\theta L(\\theta)\\) is the gradient of the loss w.r.t. parameters</li> </ul>"},{"location":"prompts/07-equations/#12-chain-rule-single-variable","title":"12. Chain Rule (Single-Variable)","text":"<p>\\(\\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x)\\)</p> <p>where:</p> <ul> <li>\\(f\\) and \\(g\\) are functions</li> <li>\\(f'\\) and \\(g'\\) are their derivatives</li> </ul>"},{"location":"prompts/07-equations/#13-chain-rule-multivariable","title":"13. Chain Rule (Multivariable)","text":"<p>\\(\\frac{\\partial}{\\partial x_i} L = \\sum_{j} \\frac{\\partial L}{\\partial z_j} \\frac{\\partial z_j}{\\partial x_i}\\)</p> <p>where:</p> <ul> <li>\\(L\\) is the loss function</li> <li>\\(z_j\\) are intermediate variables dependent on \\(x_i\\)</li> </ul>"},{"location":"prompts/07-equations/#14-momentum-update","title":"14. Momentum Update","text":"\\[ \\begin{aligned} v &amp;\\leftarrow \\beta v + (1-\\beta)\\nabla_\\theta L(\\theta), \\\\ \\theta &amp;\\leftarrow \\theta - \\eta\\, v \\end{aligned} \\] <p>where:</p> <ul> <li>\\(v\\) is the velocity term</li> <li>\\(\\beta\\) is the momentum hyperparameter (e.g., 0.9)</li> <li>\\(\\eta\\) is the learning rate</li> <li>\\(\\nabla_\\theta L(\\theta)\\) is the gradient</li> </ul>"},{"location":"prompts/07-equations/#15-nesterov-accelerated-gradient-nag","title":"15. Nesterov Accelerated Gradient (NAG)","text":"\\[ \\begin{aligned} v &amp;\\leftarrow \\beta v + (1-\\beta)\\nabla_\\theta L(\\theta - \\beta v),\\\\ \\theta &amp;\\leftarrow \\theta - \\eta\\, v \\end{aligned} \\] <p>where:</p> <ul> <li>\\(v\\) is the velocity term</li> <li>\\(\\beta\\) is the momentum factor</li> <li>\\(\\theta\\) are the parameters</li> <li>\\(\\eta\\) is the learning rate</li> </ul>"},{"location":"prompts/07-equations/#16-rmsprop-update","title":"16. RMSProp Update","text":"\\[ \\begin{aligned} E[g^2] &amp;\\leftarrow \\rho\\, E[g^2] + (1-\\rho) \\, (\\nabla_\\theta L(\\theta))^2, \\\\ \\theta &amp;\\leftarrow \\theta - \\frac{\\eta}{\\sqrt{E[g^2] + \\epsilon}} \\nabla_\\theta L(\\theta) \\end{aligned} \\] <p>where:</p> <ul> <li>\\(E[g^2]\\) is the running average of squared gradients</li> <li>\\(\\rho\\) is the decay rate (e.g., 0.9)</li> <li>\\(\\eta\\) is the learning rate</li> <li>\\(\\epsilon\\) is a small constant for numerical stability</li> </ul>"},{"location":"prompts/07-equations/#17-adam-optimizer","title":"17. Adam Optimizer","text":"\\[ \\begin{aligned} m &amp;\\leftarrow \\beta_1 m + (1 - \\beta_1)\\nabla_\\theta L(\\theta), \\\\ v &amp;\\leftarrow \\beta_2 v + (1 - \\beta_2)(\\nabla_\\theta L(\\theta))^2, \\\\ \\hat{m} &amp;\\leftarrow \\frac{m}{1 - \\beta_1^t}, \\quad \\hat{v} \\leftarrow \\frac{v}{1 - \\beta_2^t}, \\\\ \\theta &amp;\\leftarrow \\theta - \\eta \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon} \\end{aligned} \\] <p>where:</p> <ul> <li>\\(m\\) is the first moment estimate</li> <li>\\(v\\) is the second moment estimate</li> <li>\\(\\beta_1\\) and \\(\\beta_2\\) are exponential decay rates</li> <li>\\(\\eta\\) is the learning rate</li> <li>\\(t\\) is the current time step</li> <li>\\(\\epsilon\\) is a small constant</li> </ul>"},{"location":"prompts/07-equations/#18-l1-regularization","title":"18. L1 Regularization","text":"<p>\\(R_{L1} = \\lambda \\sum_{j} |w_j|\\)</p> <p>where:</p> <ul> <li>\\(\\lambda\\) is the regularization coefficient</li> <li>\\(w_j\\) are the weights</li> </ul>"},{"location":"prompts/07-equations/#19-l2-regularization","title":"19. L2 Regularization","text":"<p>\\(R_{L2} = \\frac{\\lambda}{2} \\sum_{j} w_j^2\\)</p> <p>where:</p> <ul> <li>\\(\\lambda\\) is the regularization strength</li> <li>\\(w_j\\) are the model\u2019s weights</li> <li>The factor \\(\\tfrac{1}{2}\\) is often included by convention</li> </ul>"},{"location":"prompts/07-equations/#20-weighted-cross-entropy-class-imbalance","title":"20. Weighted Cross-Entropy (Class Imbalance)","text":"<p>\\(L_{\\text{WCE}} = -\\frac{1}{N} \\sum_{i=1}^N \\Big[\\alpha \\, y^{(i)} \\log(\\hat{y}^{(i)}) + (1-\\alpha)\\,(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\Big]\\)</p> <p>where:</p> <ul> <li>\\(\\alpha\\) is a weight factor for the positive class</li> <li>\\(y^{(i)} \\in \\{0,1\\}\\) is the true label</li> <li>\\(\\hat{y}^{(i)}\\) is the predicted probability</li> </ul>"},{"location":"prompts/07-equations/#21-negative-log-likelihood-nll","title":"21. Negative Log-Likelihood (NLL)","text":"<p>\\(L_{\\text{NLL}} = -\\sum_{i=1}^N \\log \\, p(y^{(i)} | x^{(i)})\\)</p> <p>where:</p> <ul> <li>\\(p(y|x)\\) is the predicted probability of label \\(y\\) given input \\(x\\)</li> <li>\\(N\\) is the number of samples</li> </ul>"},{"location":"prompts/07-equations/#22-normal-equation-linear-regression","title":"22. Normal Equation (Linear Regression)","text":"<p>\\(\\theta = (X^\\top X)^{-1} X^\\top y\\)</p> <p>where:</p> <ul> <li>\\(X\\) is the design matrix</li> <li>\\(y\\) is the vector of targets</li> <li>\\(\\theta\\) is the vector of parameters</li> </ul>"},{"location":"prompts/07-equations/#23-coefficient-of-determination-r2","title":"23. Coefficient of Determination (\\(R^2\\))","text":"<p>\\(R^2 = 1 - \\frac{\\sum_{i=1}^N (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum_{i=1}^N (y^{(i)} - \\bar{y})^2}\\)</p> <p>where:</p> <ul> <li>\\(y^{(i)}\\) is the actual value</li> <li>\\(\\hat{y}^{(i)}\\) is the predicted value</li> <li>\\(\\bar{y}\\) is the mean of \\(y^{(i)}\\)</li> </ul>"},{"location":"prompts/07-equations/#24-batch-normalization-mean","title":"24. Batch Normalization (Mean)","text":"<p>\\(\\mu_B = \\frac{1}{m}\\sum_{i=1}^{m} x_i\\)</p> <p>where:</p> <ul> <li>\\(m\\) is the number of samples in a mini-batch</li> <li>\\(x_i\\) is the \\(i\\)-th activation in the batch</li> </ul>"},{"location":"prompts/07-equations/#25-batch-normalization-variance","title":"25. Batch Normalization (Variance)","text":"<p>\\(\\sigma_B^2 = \\frac{1}{m}\\sum_{i=1}^{m} (x_i - \\mu_B)^2\\)</p> <p>where:</p> <ul> <li>\\(x_i\\) is the \\(i\\)-th activation in the batch</li> <li>\\(\\mu_B\\) is the batch mean</li> <li>\\(\\sigma_B^2\\) is the batch variance</li> </ul>"},{"location":"prompts/07-equations/#26-batch-normalization-forward-pass","title":"26. Batch Normalization (Forward Pass)","text":"<p>\\(\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}, \\quad y_i = \\gamma \\, \\hat{x}_i + \\beta\\)</p> <p>where:</p> <ul> <li>\\(x_i\\) is the input activation</li> <li>\\(\\mu_B\\), \\(\\sigma_B^2\\) are batch statistics</li> <li>\\(\\epsilon\\) is a small constant</li> <li>\\(\\gamma\\), \\(\\beta\\) are learnable scale and shift parameters</li> </ul>"},{"location":"prompts/07-equations/#27-dropout-forward-pass","title":"27. Dropout (Forward Pass)","text":"<p>\\(\\tilde{h}^{(l)} = \\mathbf{m}^{(l)} \\odot h^{(l)}\\)</p> <p>where:</p> <ul> <li>\\(h^{(l)}\\) is the activation vector at layer \\(l\\)</li> <li>\\(\\mathbf{m}^{(l)}\\) is a random binary mask (each entry ~ Bernoulli(p))</li> <li>\\(\\odot\\) denotes elementwise multiplication</li> </ul>"},{"location":"prompts/07-equations/#28-2d-convolution-without-stridepadding","title":"28. 2D Convolution (Without Stride/Padding)","text":"<p>\\((I * K)(x,y) = \\sum_{u=-r}^{r} \\sum_{v=-s}^{s} I(x+u, y+v)\\, K(u,v)\\)</p> <p>where:</p> <ul> <li>\\(I\\) is the input image</li> <li>\\(K\\) is the kernel</li> <li>\\(r\\), \\(s\\) define kernel size offsets</li> </ul>"},{"location":"prompts/07-equations/#29-transposed-convolution","title":"29. Transposed Convolution","text":"<p>\\(\\text{ConvTranspose}(z, K) = \\sum_{u}\\sum_{v} z(u,v)\\, \\text{Upsample}(K, \\text{stride})\\)</p> <p>where:</p> <ul> <li>\\(z(u,v)\\) is an activation map</li> <li>\\(K\\) is the kernel</li> <li>\\(\\text{stride}\\) is the factor by which the output is upsampled</li> </ul>"},{"location":"prompts/07-equations/#30-max-pooling-2d","title":"30. Max Pooling (2D)","text":"<p>\\(\\text{MaxPool}(x,y) = \\max_{(p,q)\\in \\,\\mathcal{R}_{x,y}} I(p,q)\\)</p> <p>where:</p> <ul> <li>\\(I\\) is the input feature map</li> <li>\\(\\mathcal{R}_{x,y}\\) is the receptive field around location \\((x,y)\\)</li> </ul>"},{"location":"prompts/07-equations/#31-average-pooling-2d","title":"31. Average Pooling (2D)","text":"<p>\\(\\text{AvgPool}(x,y) = \\frac{1}{|\\mathcal{R}_{x,y}|}\\sum_{(p,q)\\in \\,\\mathcal{R}_{x,y}} I(p,q)\\)</p> <p>where:</p> <ul> <li>\\(\\mathcal{R}_{x,y}\\) is the region of pooling</li> <li>\\(|\\mathcal{R}_{x,y}|\\) is the size of that region</li> </ul>"},{"location":"prompts/07-equations/#32-rnn-hidden-state-update","title":"32. RNN Hidden State Update","text":"<p>\\(h_t = f(W_{hh} \\, h_{t-1} + W_{xh} \\, x_t + b_h)\\)</p> <p>where:</p> <ul> <li>\\(h_t\\) is the hidden state at time \\(t\\)</li> <li>\\(x_t\\) is the input at time \\(t\\)</li> <li>\\(W_{hh}, W_{xh}\\) are weight matrices</li> <li>\\(b_h\\) is a bias vector</li> <li>\\(f(\\cdot)\\) is an activation (e.g., tanh)</li> </ul>"},{"location":"prompts/07-equations/#33-lstm-input-gate","title":"33. LSTM Input Gate","text":"<p>\\(i_t = \\sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)\\)</p> <p>where:</p> <ul> <li>\\(i_t\\) is the input gate</li> <li>\\(x_t\\) is the current input</li> <li>\\(h_{t-1}\\) is the previous hidden state</li> <li>\\(W_{xi}, W_{hi}\\) are weight matrices</li> <li>\\(b_i\\) is the bias</li> <li>\\(\\sigma\\) is the sigmoid function</li> </ul>"},{"location":"prompts/07-equations/#34-lstm-forget-gate","title":"34. LSTM Forget Gate","text":"<p>\\(f_t = \\sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)\\)</p> <p>where:</p> <ul> <li>\\(f_t\\) is the forget gate</li> <li>\\(W_{xf}, W_{hf}\\) are weight matrices</li> <li>\\(b_f\\) is the bias</li> </ul>"},{"location":"prompts/07-equations/#35-lstm-output-gate","title":"35. LSTM Output Gate","text":"<p>\\(o_t = \\sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)\\)</p> <p>where:</p> <ul> <li>\\(o_t\\) is the output gate</li> <li>\\(W_{xo}, W_{ho}\\) are weight matrices</li> <li>\\(b_o\\) is the bias</li> </ul>"},{"location":"prompts/07-equations/#36-lstm-cell-state","title":"36. LSTM Cell State","text":"<p>\\(c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, \\quad \\tilde{c}_t = \\tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c)\\)</p> <p>where:</p> <ul> <li>\\(c_t\\) is the cell state</li> <li>\\(f_t, i_t\\) are forget and input gates</li> <li>\\(\\tilde{c}_t\\) is the candidate cell state</li> <li>\\(\\odot\\) is elementwise multiplication</li> </ul>"},{"location":"prompts/07-equations/#37-gru-update-gate","title":"37. GRU Update Gate","text":"<p>\\(z_t = \\sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z)\\)</p> <p>where:</p> <ul> <li>\\(z_t\\) is the update gate</li> <li>\\(x_t\\) is the input at time \\(t\\)</li> <li>\\(h_{t-1}\\) is the previous hidden state</li> <li>\\(W_{xz}, W_{hz}\\) are weight matrices</li> <li>\\(b_z\\) is the bias</li> </ul>"},{"location":"prompts/07-equations/#38-gru-reset-gate","title":"38. GRU Reset Gate","text":"<p>\\(r_t = \\sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r)\\)</p> <p>where:</p> <ul> <li>\\(r_t\\) is the reset gate</li> <li>\\(W_{xr}, W_{hr}\\) are weight matrices</li> <li>\\(b_r\\) is the bias</li> </ul>"},{"location":"prompts/07-equations/#39-gru-hidden-state","title":"39. GRU Hidden State","text":"<p>\\(h_t = z_t \\odot h_{t-1} + (1 - z_t) \\odot \\tilde{h}_t, \\quad \\tilde{h}_t = \\tanh(W_{xh} x_t + r_t \\odot (W_{hh} h_{t-1}))\\)</p> <p>where:</p> <ul> <li>\\(h_t\\) is the updated hidden state</li> <li>\\(z_t, r_t\\) are the update and reset gates</li> <li>\\(\\tilde{h}_t\\) is the candidate hidden state</li> </ul>"},{"location":"prompts/07-equations/#40-attention-score-dot-product","title":"40. Attention Score (Dot Product)","text":"<p>\\(e_{t,s} = h_t^\\top \\, s_s\\)</p> <p>where:</p> <ul> <li>\\(h_t\\) is the query vector</li> <li>\\(s_s\\) is the key vector</li> <li>\\(e_{t,s}\\) is the scalar score measuring alignment</li> </ul>"},{"location":"prompts/07-equations/#41-attention-weights-softmax","title":"41. Attention Weights (Softmax)","text":"<p>\\(\\alpha_{t,s} = \\frac{\\exp(e_{t,s})}{\\sum_{k=1}^{S} \\exp(e_{t,k})}\\)</p> <p>where:</p> <ul> <li>\\(e_{t,s}\\) is the attention score for query \\(t\\) and key \\(s\\)</li> <li>\\(S\\) is the total number of key vectors</li> </ul>"},{"location":"prompts/07-equations/#42-context-vector","title":"42. Context Vector","text":"<p>\\(c_t = \\sum_{s=1}^{S} \\alpha_{t,s}\\, v_s\\)</p> <p>where:</p> <ul> <li>\\(\\alpha_{t,s}\\) are attention weights</li> <li>\\(v_s\\) is the value vector associated with key \\(s\\)</li> <li>\\(c_t\\) is the resulting weighted sum</li> </ul>"},{"location":"prompts/07-equations/#43-scaled-dot-product-attention","title":"43. Scaled Dot-Product Attention","text":"<p>\\(\\text{Attention}(Q,K,V) = \\text{softmax}\\bigg(\\frac{QK^\\top}{\\sqrt{d_k}}\\bigg)V\\)</p> <p>where:</p> <ul> <li>\\(Q\\) is the matrix of query vectors</li> <li>\\(K\\) is the matrix of key vectors</li> <li>\\(V\\) is the matrix of value vectors</li> <li>\\(d_k\\) is the dimensionality of keys/queries</li> </ul>"},{"location":"prompts/07-equations/#44-multi-head-attention","title":"44. Multi-Head Attention","text":"<p>\\(\\text{MHA}(Q,K,V) = \\big[\\text{head}_1, \\dots, \\text{head}_h\\big]W^O\\)</p> <p>where:</p> <ul> <li>\\(\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\\)</li> <li>\\(W^O\\) is the output projection matrix</li> <li>\\(h\\) is the number of heads</li> </ul>"},{"location":"prompts/07-equations/#45-feed-forward-transformer","title":"45. Feed-Forward (Transformer)","text":"<p>\\(\\text{FFN}(x) = \\max(0, xW_1 + b_1) \\, W_2 + b_2\\)</p> <p>where:</p> <ul> <li>\\(x\\) is the input</li> <li>\\(W_1, b_1, W_2, b_2\\) are learned parameters</li> <li>\\(\\max(0,\\cdot)\\) indicates ReLU (or another activation)</li> </ul>"},{"location":"prompts/07-equations/#46-positional-encoding-sinusoidal","title":"46. Positional Encoding (Sinusoidal)","text":"<p>\\(\\text{PE}(pos, 2i) = \\sin\\big(pos / 10000^{2i/d_{\\text{model}}}\\big), \\quad \\text{PE}(pos, 2i+1) = \\cos\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\\)</p> <p>where:</p> <ul> <li>\\(pos\\) is the position index</li> <li>\\(i\\) is the dimension index</li> <li>\\(d_{\\text{model}}\\) is the model dimensionality</li> </ul>"},{"location":"prompts/07-equations/#47-perceptron-rule","title":"47. Perceptron Rule","text":"<p>\\(\\hat{y} =  \\begin{cases} 1 &amp; \\text{if } w^\\top x + b \\ge 0\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(w\\) is the weight vector</li> <li>\\(x\\) is the input vector</li> <li>\\(b\\) is the bias term</li> </ul>"},{"location":"prompts/07-equations/#48-general-activation-forward","title":"48. General Activation Forward","text":"<p>\\(a = f(z)\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the pre-activation (weighted sum)</li> <li>\\(f(\\cdot)\\) is an activation function (e.g., sigmoid, ReLU)</li> </ul>"},{"location":"prompts/07-equations/#49-derivative-of-relu","title":"49. Derivative of ReLU","text":"<p>\\(\\frac{d}{dz}\\text{ReLU}(z) = \\begin{cases} 1 &amp; z &gt; 0\\\\ 0 &amp; z \\le 0 \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> </ul>"},{"location":"prompts/07-equations/#50-derivative-of-sigmoid","title":"50. Derivative of Sigmoid","text":"<p>\\(\\sigma'(z) = \\sigma(z)\\big(1-\\sigma(z)\\big)\\)</p> <p>where:</p> <ul> <li>\\(\\sigma(z)\\) is the sigmoid function</li> </ul>"},{"location":"prompts/07-equations/#51-derivative-of-tanh","title":"51. Derivative of Tanh","text":"<p>\\(\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)\\)</p> <p>where:</p> <ul> <li>\\(\\tanh(z)\\) is the hyperbolic tangent function</li> </ul>"},{"location":"prompts/07-equations/#52-derivative-of-softmax-vector-form","title":"52. Derivative of Softmax (Vector Form)","text":"<p>\\(\\frac{\\partial \\hat{y}_j}{\\partial z_k} = \\hat{y}_j \\big(\\delta_{jk} - \\hat{y}_k\\big)\\)</p> <p>where:</p> <ul> <li>\\(\\hat{y}_j\\) is the softmax output for class \\(j\\)</li> <li>\\(z_k\\) is the logit for class \\(k\\)</li> <li>\\(\\delta_{jk}\\) is the Kronecker delta (1 if \\(j=k\\), 0 otherwise)</li> </ul>"},{"location":"prompts/07-equations/#53-derivative-of-cross-entropy-wrt-logits-softmax","title":"53. Derivative of Cross-Entropy w.r.t. Logits (Softmax)","text":"<p>\\(\\frac{\\partial L_{\\text{CE}}}{\\partial z_j} = \\hat{y}_j - y_j\\)</p> <p>where:</p> <ul> <li>\\(\\hat{y}_j\\) is the predicted probability (softmax output)</li> <li>\\(y_j\\) is the one-hot target</li> </ul>"},{"location":"prompts/07-equations/#54-l2-distance-euclidean","title":"54. L2 Distance (Euclidean)","text":"<p>\\(d_{\\text{Euclidean}}(x,y) = \\sqrt{\\sum_{i=1}^{d}(x_i - y_i)^2}\\)</p> <p>where:</p> <ul> <li>\\(x, y \\in \\mathbb{R}^d\\) are vectors</li> <li>\\(d\\) is the number of dimensions</li> </ul>"},{"location":"prompts/07-equations/#55-manhattan-distance-l1","title":"55. Manhattan Distance (L1)","text":"<p>\\(d_{\\text{Manhattan}}(x,y) = \\sum_{i=1}^d |x_i - y_i|\\)</p> <p>where:</p> <ul> <li>\\(x, y\\) are vectors in \\(\\mathbb{R}^d\\)</li> <li>\\(|\\,\\cdot\\,|\\) is the absolute value</li> </ul>"},{"location":"prompts/07-equations/#56-cosine-similarity","title":"56. Cosine Similarity","text":"<p>\\(\\text{cos\\_sim}(x, y) = \\frac{x \\cdot y}{\\|x\\| \\, \\|y\\|}\\)</p> <p>where:</p> <ul> <li>\\(x \\cdot y\\) is the dot product</li> <li>\\(\\|x\\|\\) is the norm of \\(x\\)</li> </ul>"},{"location":"prompts/07-equations/#57-dot-product","title":"57. Dot Product","text":"<p>\\(x \\cdot y = \\sum_{i=1}^{d} x_i \\, y_i\\)</p> <p>where:</p> <ul> <li>\\(x, y\\) are vectors in \\(\\mathbb{R}^d\\)</li> </ul>"},{"location":"prompts/07-equations/#58-matrix-multiplication","title":"58. Matrix Multiplication","text":"<p>\\((AB)_{ij} = \\sum_{k=1}^{n} A_{ik}\\, B_{kj}\\)</p> <p>where:</p> <ul> <li>\\(A\\) is a \\((m \\times n)\\) matrix</li> <li>\\(B\\) is a \\((n \\times p)\\) matrix</li> <li>\\((AB)\\) is a \\((m \\times p)\\) matrix</li> </ul>"},{"location":"prompts/07-equations/#59-determinant-of-a-22-matrix","title":"59. Determinant of a 2\u00d72 Matrix","text":"<p>\\(\\det\\begin{pmatrix} a &amp; b\\\\ c &amp; d \\end{pmatrix} = ad - bc\\)</p> <p>where:</p> <ul> <li>\\(a, b, c, d\\) are elements of the matrix</li> </ul>"},{"location":"prompts/07-equations/#60-matrix-inverse-22","title":"60. Matrix Inverse (2\u00d72)","text":"<p>\\(\\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}^{-1}  = \\frac{1}{ad - bc} \\begin{pmatrix} d &amp; -b \\\\ -c &amp; a\\end{pmatrix}\\)</p> <p>where:</p> <ul> <li>\\(a, b, c, d\\) are elements</li> <li>\\(ad - bc \\neq 0\\)</li> </ul>"},{"location":"prompts/07-equations/#61-frobenius-norm","title":"61. Frobenius Norm","text":"<p>\\(\\|A\\|_F = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n} |A_{ij}|^2}\\)</p> <p>where:</p> <ul> <li>\\(A\\) is an \\(m \\times n\\) matrix</li> <li>\\(A_{ij}\\) is the element in row \\(i\\), column \\(j\\)</li> </ul>"},{"location":"prompts/07-equations/#62-kl-divergence","title":"62. KL Divergence","text":"<p>\\(D_{\\text{KL}}(P \\| Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}\\)</p> <p>where:</p> <ul> <li>\\(P\\) and \\(Q\\) are discrete probability distributions</li> <li>The sum is over all possible outcomes \\(x\\)</li> </ul>"},{"location":"prompts/07-equations/#63-jensen-shannon-divergence","title":"63. Jensen-Shannon Divergence","text":"<p>\\(D_{\\text{JS}}(P\\|Q) = \\frac{1}{2} D_{\\text{KL}}\\big(P \\| M\\big) + \\frac{1}{2} D_{\\text{KL}}\\big(Q \\| M\\big)\\)</p> <p>where:</p> <ul> <li>\\(M = \\frac{1}{2}(P + Q)\\)</li> <li>\\(P\\) and \\(Q\\) are probability distributions</li> </ul>"},{"location":"prompts/07-equations/#64-bayes-rule","title":"64. Bayes\u2019 Rule","text":"<p>\\(P(A|B) = \\frac{P(B|A)\\,P(A)}{P(B)}\\)</p> <p>where:</p> <ul> <li>\\(A\\) and \\(B\\) are events</li> <li>\\(P(A)\\), \\(P(B)\\), \\(P(B|A)\\) are known or can be inferred</li> </ul>"},{"location":"prompts/07-equations/#65-covariance","title":"65. Covariance","text":"<p>\\(\\text{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])]\\)</p> <p>where:</p> <ul> <li>\\(E[\\cdot]\\) denotes the expectation</li> <li>\\(X, Y\\) are random variables</li> </ul>"},{"location":"prompts/07-equations/#66-pearson-correlation","title":"66. Pearson Correlation","text":"<p>\\(\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\)</p> <p>where:</p> <ul> <li>\\(\\sigma_X\\) and \\(\\sigma_Y\\) are standard deviations of \\(X\\) and \\(Y\\)</li> </ul>"},{"location":"prompts/07-equations/#67-pca-covariance-eigen-decomposition","title":"67. PCA Covariance Eigen-Decomposition","text":"<p>\\(C = U \\Lambda U^\\top\\)</p> <p>where:</p> <ul> <li>\\(C\\) is the covariance matrix</li> <li>\\(U\\) is the matrix of eigenvectors</li> <li>\\(\\Lambda\\) is the diagonal matrix of eigenvalues</li> </ul>"},{"location":"prompts/07-equations/#68-singular-value-decomposition-svd","title":"68. Singular Value Decomposition (SVD)","text":"<p>\\(A = U \\Sigma V^\\top\\)</p> <p>where:</p> <ul> <li>\\(A\\) is an \\((m \\times n)\\) matrix</li> <li>\\(U\\) and \\(V\\) are orthonormal matrices</li> <li>\\(\\Sigma\\) is the diagonal matrix of singular values</li> </ul>"},{"location":"prompts/07-equations/#69-gaussian-distribution-1d","title":"69. Gaussian Distribution (1D)","text":"<p>\\(\\mathcal{N}(x \\mid \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\Big(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\Big)\\)</p> <p>where:</p> <ul> <li>\\(\\mu\\) is the mean</li> <li>\\(\\sigma^2\\) is the variance</li> </ul>"},{"location":"prompts/07-equations/#70-probability-integral-discrete","title":"70. Probability Integral (Discrete)","text":"<p>\\(\\sum_{x} P(x) = 1\\)</p> <p>where:</p> <ul> <li>\\(x\\) ranges over all possible outcomes</li> <li>\\(P(x)\\) is a probability distribution</li> </ul>"},{"location":"prompts/07-equations/#71-probability-integral-continuous","title":"71. Probability Integral (Continuous)","text":"<p>\\(\\int_{-\\infty}^{\\infty} p(x)\\, dx = 1\\)</p> <p>where:</p> <ul> <li>\\(p(x)\\) is a probability density function</li> </ul>"},{"location":"prompts/07-equations/#72-exponential-family-general-form","title":"72. Exponential Family (General Form)","text":"<p>\\(p(x|\\theta) = h(x) \\exp\\big(\\eta(\\theta)^\\top T(x) - A(\\theta)\\big)\\)</p> <p>where:</p> <ul> <li>\\(h(x)\\), \\(T(x)\\) are known functions</li> <li>\\(\\eta(\\theta)\\) is the natural parameter</li> <li>\\(A(\\theta)\\) is the log-partition function</li> </ul>"},{"location":"prompts/07-equations/#73-weighted-adjacency-graph-neural-network","title":"73. Weighted Adjacency (Graph Neural Network)","text":"<p>\\(h_v^{(l+1)} = \\sigma\\Bigg(\\sum_{u \\in \\mathcal{N}(v)} \\alpha_{vu} \\, W^{(l)} h_u^{(l)}\\Bigg)\\)</p> <p>where:</p> <ul> <li>\\(h_v^{(l)}\\) is the hidden representation of node \\(v\\) at layer \\(l\\)</li> <li>\\(\\alpha_{vu}\\) is the attention or adjacency weight from node \\(u\\) to \\(v\\)</li> <li>\\(W^{(l)}\\) is a trainable weight matrix</li> <li>\\(\\sigma\\) is an activation function</li> </ul>"},{"location":"prompts/07-equations/#74-gnn-aggregation","title":"74. GNN Aggregation","text":"<p>\\(h_v^{(l+1)} = \\text{AGGREGATE}\\big(\\{\\,h_u^{(l)} : u \\in \\mathcal{N}(v)\\}\\big)\\)</p> <p>where:</p> <ul> <li>\\(\\mathcal{N}(v)\\) is the set of neighbors of node \\(v\\)</li> <li>\\(\\text{AGGREGATE}(\\cdot)\\) could be sum, mean, max, or an attention-based mechanism</li> </ul>"},{"location":"prompts/07-equations/#75-rmsprop-repeated-for-emphasis","title":"75. RMSProp (Repeated for Emphasis)","text":"<p>\\(\\theta \\leftarrow \\theta - \\frac{\\eta}{\\sqrt{E[g^2] + \\epsilon}} \\nabla_\\theta L(\\theta)\\)</p> <p>where:</p> <ul> <li>\\(\\theta\\) is the parameter vector</li> <li>\\(\\eta\\) is the learning rate</li> <li>\\(E[g^2]\\) is the running average of squared gradients</li> <li>\\(\\epsilon\\) prevents division by zero</li> </ul>"},{"location":"prompts/07-equations/#76-weighted-combination-for-multi-head","title":"76. Weighted Combination for Multi-Head","text":"<p>\\(\\text{head}_i = \\text{Attention}(QW_i^Q, \\; KW_i^K, \\; VW_i^V)\\)</p> <p>where:</p> <ul> <li>\\(Q, K, V\\) are query, key, and value matrices</li> <li>\\(W_i^Q, W_i^K, W_i^V\\) are parameter matrices for head \\(i\\)</li> </ul>"},{"location":"prompts/07-equations/#77-gradient-of-mse-wrt-weights","title":"77. Gradient of MSE w.r.t. Weights","text":"<p>\\(\\frac{\\partial L_{\\text{MSE}}}{\\partial w_j} = \\frac{1}{N}\\sum_{i=1}^N 2 \\,(\\hat{y}^{(i)} - y^{(i)}) \\, x_j^{(i)}\\)</p> <p>where:</p> <ul> <li>\\(w_j\\) is weight \\(j\\)</li> <li>\\(x_j^{(i)}\\) is the \\(j\\)-th feature of sample \\(i\\)</li> </ul>"},{"location":"prompts/07-equations/#78-gradient-of-binary-cross-entropy-wrt-weights","title":"78. Gradient of Binary Cross-Entropy w.r.t. Weights","text":"<p>\\(\\frac{\\partial L_{\\text{CE}}}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)})\\, x_j^{(i)}\\)</p> <p>where:</p> <ul> <li>\\(\\hat{y}^{(i)}\\) is the predicted probability</li> <li>\\(y^{(i)}\\) is the true label</li> </ul>"},{"location":"prompts/07-equations/#79-logistic-regression-decision-boundary","title":"79. Logistic Regression Decision Boundary","text":"<p>\\(\\hat{y} = \\sigma(w^\\top x + b) \\implies  \\text{Decision} = \\begin{cases} 1 &amp; \\text{if } w^\\top x + b \\ge 0\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(w, b\\) are parameters</li> <li>\\(x\\) is the input</li> <li>\\(\\sigma\\) is the sigmoid function</li> </ul>"},{"location":"prompts/07-equations/#80-balanced-cross-entropy","title":"80. Balanced Cross-Entropy","text":"<p>\\(L_{\\text{bal}} = -\\frac{1}{N} \\sum_{i=1}^N \\Big[\\beta \\, y^{(i)} \\log(\\hat{y}^{(i)}) + (1-\\beta)(1-y^{(i)}) \\log(1-\\hat{y}^{(i)})\\Big]\\)</p> <p>where:</p> <ul> <li>\\(\\beta\\) is a factor weighting the positive class</li> <li>\\(y^{(i)}, \\hat{y}^{(i)}\\) as before</li> </ul>"},{"location":"prompts/07-equations/#81-f1-score","title":"81. F1 Score","text":"<p>\\(\\text{F1} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\\)</p> <p>where:</p> <ul> <li>\\(\\text{precision} = \\frac{TP}{TP+FP}\\)</li> <li>\\(\\text{recall} = \\frac{TP}{TP+FN}\\)</li> </ul>"},{"location":"prompts/07-equations/#82-precision","title":"82. Precision","text":"<p>\\(\\text{precision} = \\frac{TP}{TP + FP}\\)</p> <p>where:</p> <ul> <li>\\(TP\\) = true positives</li> <li>\\(FP\\) = false positives</li> </ul>"},{"location":"prompts/07-equations/#83-recall","title":"83. Recall","text":"<p>\\(\\text{recall} = \\frac{TP}{TP + FN}\\)</p> <p>where:</p> <ul> <li>\\(TP\\) = true positives</li> <li>\\(FN\\) = false negatives</li> </ul>"},{"location":"prompts/07-equations/#84-auc-area-under-roc-curve","title":"84. AUC (Area Under ROC Curve)","text":"<p>\\(\\text{AUC} = \\int_0^1 TPR(\\text{FPR}^{-1}(x))\\, dx\\)</p> <p>where:</p> <ul> <li>\\(TPR\\) is the true positive rate</li> <li>\\(FPR\\) is the false positive rate</li> <li>The integral is conceptually the area under the ROC curve</li> </ul>"},{"location":"prompts/07-equations/#85-step-decay-learning-rate-scheduling","title":"85. Step Decay (Learning Rate Scheduling)","text":"<p>\\(\\eta_{\\text{new}} = \\eta_{\\text{old}} \\cdot \\gamma^{\\lfloor \\frac{\\text{epoch}}{k}\\rfloor}\\)</p> <p>where:</p> <ul> <li>\\(\\eta_{\\text{old}}\\) is the current learning rate</li> <li>\\(\\eta_{\\text{new}}\\) is the updated learning rate</li> <li>\\(\\gamma \\in (0,1)\\) is the decay factor</li> <li>\\(k\\) is the step size in epochs</li> </ul>"},{"location":"prompts/07-equations/#86-polynomial-kernel-svm","title":"86. Polynomial Kernel (SVM)","text":"<p>\\(K(x, x') = (x^\\top x' + c)^p\\)</p> <p>where:</p> <ul> <li>\\(x, x'\\) are feature vectors</li> <li>\\(c\\) is a constant (often 1)</li> <li>\\(p\\) is the polynomial degree</li> </ul>"},{"location":"prompts/07-equations/#87-radial-basis-function-kernel-rbf","title":"87. Radial Basis Function Kernel (RBF)","text":"<p>\\(K(x, x') = \\exp\\Big(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\Big)\\)</p> <p>where:</p> <ul> <li>\\(x, x'\\) are feature vectors</li> <li>\\(\\sigma\\) is the kernel width</li> </ul>"},{"location":"prompts/07-equations/#88-hinge-loss-svm","title":"88. Hinge Loss (SVM)","text":"<p>\\(L_{\\text{hinge}} = \\max\\big(0, 1 - y_i (w^\\top x_i + b)\\big)\\)</p> <p>where:</p> <ul> <li>\\(y_i \\in \\{-1, +1\\}\\) is the true label</li> <li>\\(w\\) is the weight vector</li> <li>\\(x_i\\) is the input</li> <li>\\(b\\) is the bias</li> </ul>"},{"location":"prompts/07-equations/#89-margin-hard-svm","title":"89. Margin (Hard-SVM)","text":"<p>\\(\\text{Margin} = \\frac{2}{\\|w\\|}\\)</p> <p>where:</p> <ul> <li>\\(w\\) is the normal vector to the decision boundary</li> <li>\\(\\|w\\|\\) is the Euclidean norm of \\(w\\)</li> </ul>"},{"location":"prompts/07-equations/#90-q-learning-update-rl","title":"90. Q-Learning Update (RL)","text":"<p>\\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a'} Q(s', a') - Q(s,a)\\Big]\\)</p> <p>where:</p> <ul> <li>\\(Q(s,a)\\) is the current Q-value</li> <li>\\(\\alpha\\) is the learning rate</li> <li>\\(r\\) is the reward</li> <li>\\(\\gamma\\) is the discount factor</li> <li>\\(s'\\) is the next state</li> <li>\\(a'\\) is the next action</li> </ul>"},{"location":"prompts/07-equations/#91-bellman-equation-value-function","title":"91. Bellman Equation (Value Function)","text":"<p>\\(V^\\pi(s) = \\mathbb{E}_{a\\sim \\pi}\\Big[r(s,a) + \\gamma V^\\pi(s')\\Big]\\)</p> <p>where:</p> <ul> <li>\\(V^\\pi(s)\\) is the value of state \\(s\\) under policy \\(\\pi\\)</li> <li>\\(r(s,a)\\) is the immediate reward</li> <li>\\(\\gamma\\) is the discount factor</li> <li>\\(s'\\) is the next state</li> </ul>"},{"location":"prompts/07-equations/#92-actor-critic-gradient","title":"92. Actor-Critic Gradient","text":"<p>\\(\\nabla_\\theta J(\\theta) = \\mathbb{E}\\big[\\nabla_\\theta \\log \\pi_\\theta(a|s)\\, (Q_w(s,a) - b(s))\\big]\\)</p> <p>where:</p> <ul> <li>\\(\\pi_\\theta\\) is the policy (actor)</li> <li>\\(Q_w(s,a)\\) is the critic\u2019s estimate of action-value</li> <li>\\(b(s)\\) is a baseline to reduce variance</li> </ul>"},{"location":"prompts/07-equations/#93-softplus-activation","title":"93. Softplus Activation","text":"<p>\\(\\text{Softplus}(z) = \\log(1 + e^z)\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> </ul>"},{"location":"prompts/07-equations/#94-elu-exponential-linear-unit","title":"94. ELU (Exponential Linear Unit)","text":"<p>\\(\\text{ELU}(z) = \\begin{cases} z &amp; z \\ge 0,\\\\ \\alpha(e^z - 1) &amp; z &lt; 0 \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(\\alpha\\) is a positive constant</li> </ul>"},{"location":"prompts/07-equations/#95-selu-scaled-elu","title":"95. SELU (Scaled ELU)","text":"<p>\\(\\text{SELU}(z) = \\lambda \\begin{cases} z &amp; z \\ge 0\\\\ \\alpha(e^z - 1) &amp; z &lt; 0 \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(\\alpha\\) and \\(\\lambda\\) are constants (e.g., \\(\\alpha \\approx 1.673\\), \\(\\lambda \\approx 1.051\\))</li> </ul>"},{"location":"prompts/07-equations/#96-knowledge-distillation","title":"96. Knowledge Distillation","text":"<p>\\(L_{\\text{KD}} = \\tau^2 \\cdot \\text{KL}\\big(p_\\text{teacher}(x;\\tau)\\,\\|\\,p_\\text{student}(x;\\tau)\\big)\\)</p> <p>where:</p> <ul> <li>\\(\\tau\\) is the temperature</li> <li>\\(p_\\text{teacher}\\) and \\(p_\\text{student}\\) are softmax outputs of teacher and student networks, respectively</li> </ul>"},{"location":"prompts/07-equations/#97-threshold-function-binary-step","title":"97. Threshold Function (Binary Step)","text":"<p>\\(\\theta(z) = \\begin{cases} 1 &amp; z \\ge 0\\\\ 0 &amp; z &lt; 0 \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> </ul>"},{"location":"prompts/07-equations/#98-weighted-hinge-loss","title":"98. Weighted Hinge Loss","text":"<p>\\(L_{\\text{whinge}} = w_i \\,\\max\\big(0, 1 - y_i (w^\\top x_i + b)\\big)\\)</p> <p>where:</p> <ul> <li>\\(w_i\\) is the sample weight or class weight</li> <li>\\(y_i \\in \\{-1, +1\\}\\) is the true label</li> </ul>"},{"location":"prompts/07-equations/#99-perplexity-language-modeling","title":"99. Perplexity (Language Modeling)","text":"<p>\\(\\text{PPL} = \\exp\\Big(\\frac{1}{N}\\sum_{i=1}^{N} -\\log p(w_i)\\Big)\\)</p> <p>where:</p> <ul> <li>\\(w_i\\) are tokens in the sequence</li> <li>\\(p(w_i)\\) is the predicted probability of token \\(w_i\\)</li> <li>\\(N\\) is the total number of tokens</li> </ul>"},{"location":"prompts/07-equations/#100-cycle-consistency-loss-cyclegan","title":"100. Cycle Consistency Loss (CycleGAN)","text":"<p>\\(L_{\\text{cyc}}(G,F) = \\mathbb{E}_{x \\sim p(x)}\\big[\\|F(G(x)) - x\\|\\big] + \\mathbb{E}_{y \\sim p(y)}\\big[\\|G(F(y)) - y\\|\\big]\\)</p> <p>where:</p> <ul> <li>\\(G\\) is the generator mapping domain \\(X \\to Y\\)</li> <li>\\(F\\) is the generator mapping domain \\(Y \\to X\\)</li> <li>\\(x, y\\) are samples from domains \\(X, Y\\)</li> <li>\\(\\|\\cdot\\|\\) is a distance measure (e.g., L1 norm)</li> </ul>"},{"location":"prompts/08-word-cloud/","title":"Word Cloud","text":"<p>Prompt</p> <p>Prompt Name: Wordcloud</p> <p>Create a list of the 50 most important Concepts in the deep learning course.  Use the concept dependency information to find the concepts that have the most other concepts depend upon. Cross check this with what you believe are the most important concepts to learn in a deep learning course. For each concept give it a score of between 40 and 70 for how important the concept is relative to the other concepts in the course.  The more important concepts should have a higher score.</p> <p>Then use the JavaScript wordcloud-2 library to generate the wordcloud for this list of 50 concepts. For each concept, allow the user to click on the link and  have the link go to the ../../glossary.md#[ConceptLabel] link.</p>"},{"location":"prompts/09-suggested-simulations/","title":"Interactive Simulation Concepts for Deep Learning Education","text":"<p>Prompt</p> <p>Our textbook development team would like to use generative AI to create simulations for explaining concepts that are difficult to understand. Of all of the concepts in this course, which concepts might benefit from an interactive simulation?  We are using tools like p5.js to build the simulations.  They are interactive and allow students to  move range control sliders to change the drawing parameters or simulation parameters.  For example to explain what \"parameter count\" is in a model we allow the students to change the number of layers in a neural network, and the number of neurons in a layer.</p> <p>Please list the concepts that can be explained by a simulation. Describe the simulation in detail and suggest what parameters the student might change.</p>"},{"location":"prompts/09-suggested-simulations/#response","title":"Response","text":""},{"location":"prompts/09-suggested-simulations/#1-gradient-descent","title":"1. Gradient Descent","text":"<p>Simulation Description: Visualize gradient descent optimization on a 2D or 3D loss surface. Interactive Parameters:</p> <ul> <li>Learning rate slider (0.001 to 1.0)</li> <li>Momentum coefficient (0 to 0.99)</li> <li>Initial starting point on the surface</li> <li>Type of surface (select between different loss landscapes)</li> <li>Batch size for stochastic gradient descent</li> </ul> <p>Visual Elements:</p> <ul> <li>Animated particle showing optimizer's path</li> <li>Contour plot of the loss surface</li> <li>Real-time plot of loss value vs. iterations</li> <li>Vector arrows showing gradient direction</li> </ul>"},{"location":"prompts/09-suggested-simulations/#2-activation-functions","title":"2. Activation Functions","text":"<p>Simulation Description: Interactive visualization of different activation functions and their derivatives. Interactive Parameters:</p> <ul> <li>Function type selector (ReLU, Sigmoid, Tanh, LeakyReLU)</li> <li>Input range slider (-10 to 10)</li> <li>Slope parameter for LeakyReLU</li> <li>Temperature parameter for Sigmoid</li> </ul> <p>Visual Elements:</p> <ul> <li>Side-by-side plots of function and its derivative</li> <li>Highlighted point showing input/output relationship</li> <li>Distribution of activations for random inputs</li> </ul>"},{"location":"prompts/09-suggested-simulations/#3-backpropagation","title":"3. Backpropagation","text":"<p>Simulation Description: Step-by-step visualization of the backpropagation process in a simple neural network. Interactive Parameters: - Network architecture (number of layers and neurons) - Learning rate - Input values - Target output - Activation function selection</p> <p>Visual Elements: - Color-coded weights showing magnitude - Animated gradient flow - Forward pass highlighting - Backward pass highlighting</p>"},{"location":"prompts/09-suggested-simulations/#4-convolution-operations","title":"4. Convolution Operations","text":"<p>Simulation Description: Visual demonstration of how convolution filters work on images. Interactive Parameters: - Kernel size (3x3, 5x5, 7x7) - Stride length - Padding amount - Custom filter weights - Input image selection</p> <p>Visual Elements: - Sliding window animation - Side-by-side original and filtered images - Heat map of filter responses - Feature map visualization</p>"},{"location":"prompts/09-suggested-simulations/#5-attention-mechanism","title":"5. Attention Mechanism","text":"<p>Simulation Description: Visualization of self-attention in transformer architectures. Interactive Parameters: - Number of attention heads - Query/Key dot product scaling - Input sequence length - Attention dropout rate - Positional encoding strength</p> <p>Visual Elements: - Attention weight matrices as heat maps - Token-to-token connection strength visualization - Multiple head view - Animated attention flow</p>"},{"location":"prompts/09-suggested-simulations/#6-dimensionality-reduction","title":"6. Dimensionality Reduction","text":"<p>Simulation Description: Interactive visualization of PCA, t-SNE, and UMAP. Interactive Parameters: - Algorithm selection (PCA, t-SNE, UMAP) - Number of components/dimensions - Perplexity (t-SNE) - Number of neighbors (UMAP) - Learning rate - Number of iterations</p> <p>Visual Elements: - 2D/3D scatter plots - Variance explained charts for PCA - Animation of dimension reduction process - Cluster separation visualization</p>"},{"location":"prompts/09-suggested-simulations/#7-batch-normalization","title":"7. Batch Normalization","text":"<p>Simulation Description: Visualization of how batch normalization affects feature distributions. Interactive Parameters: - Batch size - Momentum for running statistics - Epsilon value - Feature scale - Feature shift</p> <p>Visual Elements: - Before/after distribution plots - Layer activation histograms - Training speed comparison - Gradient magnitude visualization</p>"},{"location":"prompts/09-suggested-simulations/#8-gan-training","title":"8. GAN Training","text":"<p>Simulation Description: Interactive visualization of the GAN training process. Interactive Parameters: - Latent space dimension - Generator architecture - Discriminator architecture - Learning rates for both networks - Noise distribution parameters</p> <p>Visual Elements: - Generated samples grid - Loss curves for both networks - Discriminator decision boundary - Mode collapse detection</p>"},{"location":"prompts/09-suggested-simulations/#9-dropout","title":"9. Dropout","text":"<p>Simulation Description: Visualization of how dropout affects network activations and prevents co-adaptation. Interactive Parameters: - Dropout rate - Network depth - Number of neurons per layer - Input pattern selection - Training iterations</p> <p>Visual Elements: - Active/inactive neurons highlighting - Ensemble prediction visualization - Feature co-adaptation patterns - Performance comparison plots</p>"},{"location":"prompts/09-suggested-simulations/#10-loss-landscapes","title":"10. Loss Landscapes","text":"<p>Simulation Description: 3D visualization of neural network loss landscapes. Interactive Parameters: - Network architecture - Loss function selection - Weight initialization method - Optimization algorithm - Training data distribution</p> <p>Visual Elements: - 3D surface plot - 2D contour plot - Optimizer trajectory - Local minima highlighting</p>"},{"location":"prompts/09-suggested-simulations/#11-reinforcement-learning","title":"11. Reinforcement Learning","text":"<p>Simulation Description: Interactive grid world or continuous control environment. Interactive Parameters: - Environment complexity - Reward structure - Discount factor - Exploration rate - Learning rate</p> <p>Visual Elements: - Agent movement animation - Q-value heat maps - Policy visualization - Learning curve - State-value function surface</p>"},{"location":"prompts/09-suggested-simulations/#12-vanishingexploding-gradients","title":"12. Vanishing/Exploding Gradients","text":"<p>Simulation Description: Visualization of gradient flow through deep networks. Interactive Parameters: - Network depth - Activation function selection - Weight initialization method - Input data distribution - Learning rate</p> <p>Visual Elements: - Gradient magnitude plots - Layer-wise activation distributions - Training progress comparison - Gradient flow animation</p> <p>These simulations would provide students with hands-on experience and intuitive understanding of complex deep learning concepts. Each simulation should include:</p> <ul> <li>Clear instructions and educational objectives</li> <li>Reset button to start over</li> <li>Save/load capability for specific configurations</li> <li>Export functionality for generated visualizations</li> <li>Tooltips explaining each parameter</li> <li>Performance metrics where applicable</li> </ul> <p>The simulations should be progressive, starting with simple configurations and allowing students to gradually increase complexity as their understanding grows.</p>"},{"location":"prompts/10-activation-functions/","title":"Activation Functions Prompt","text":"<p>Prompt</p> <p>Please create a detailed section of the course  on Activation Functions.  Describe what they are and then give a list of all the different activation functions and what the pros and cons are of each activation function.  Pay attention to how quickly activation functions can be calculated and discuss implications for large scale deep learning projects.  Note when images should be inserted into the text.  Conclude with an analysis of what activation functions are used most commonly in practice.</p> <p>Response: Activation Functions</p> <p>Link to Anthropic Claude Dialog</p>"},{"location":"prompts/11-tensorflow-playground/","title":"Tensorflow Playground","text":"<p>Prompt</p> <p>Please describe Tensorflow Playground.  Why do students like it so much?</p>"},{"location":"prompts/11-tensorflow-playground/#response","title":"Response","text":"<p>TensorFlow Playground is a brilliant example of an interactive neural network visualization tool that makes abstract concepts tangible. Let me explain why it's so effective:</p> <p>Key Features that Make it Engaging:</p>"},{"location":"prompts/11-tensorflow-playground/#1-real-time-visualization","title":"1. Real-time Visualization:","text":"<ul> <li>Students can see neurons activating in real-time</li> <li>Weight values are shown through line thickness and color</li> <li>Feature detectors are visualized directly</li> <li>Loss curves update instantly as the network trains</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#2-interactive-elements","title":"2. Interactive Elements:","text":"<ul> <li>Students can draw their own data points</li> <li>Add/remove layers and neurons with simple clicks</li> <li>Toggle different features on/off</li> <li>Change learning rate and activation functions</li> <li>Select different datasets (spiral, circle, XOR etc.)</li> </ul> <p>Why Students Love It:</p>"},{"location":"prompts/11-tensorflow-playground/#1-immediate-feedback","title":"1. Immediate Feedback","text":"<ul> <li>Every change they make shows instant results</li> <li>They can develop intuition about how parameters affect learning</li> <li>Failed experiments are as instructive as successful ones</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#2-low-barrier-to-entry","title":"2. Low barrier to entry","text":"<ul> <li>No coding required</li> <li>Clean, intuitive interface</li> <li>Works in any browser</li> <li>Free and easily accessible</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#3-conceptual-understanding","title":"3. Conceptual Understanding","text":"<ul> <li>Makes abstract concepts like \"hidden layers\" visual and concrete</li> <li>Shows how neural networks actually separate data</li> <li>Demonstrates concepts like underfitting and overfitting visually</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#4-experimentation-friendly","title":"4. Experimentation-friendly","text":"<ul> <li>Students can test their hypotheses immediately</li> <li>Safe environment to make mistakes</li> <li>Encourages exploration and discovery</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#5-progressive-learning-journey","title":"5.  Progressive Learning Journey:","text":"<ul> <li>Students can start with simple problems (linear separation)</li> <li>Gradually move to more complex tasks (spiral classification)</li> <li>Build confidence through guided discovery</li> <li>Natural progression from basic to advanced concepts</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#6-demystifies-black-box-perception","title":"6.  Demystifies \"Black Box\" Perception:","text":"<ul> <li>Shows exactly how data flows through the network</li> <li>Visualizes internal representations at each layer</li> <li>Makes weight updates visible and understandable</li> <li>Breaks down complex concepts into digestible pieces</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#7-supports-multiple-learning-styles","title":"7.  Supports Multiple Learning Styles:","text":"<ul> <li>Visual learners see the network structure and data</li> <li>Kinesthetic learners can interact and experiment</li> <li>Analytical learners can observe numerical changes</li> <li>Social learners can share and discuss discoveries</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#8-reduces-math-anxiety","title":"8.  Reduces Math Anxiety:","text":"<ul> <li>Makes complex mathematics more approachable</li> <li>Shows concepts without requiring formula memorization</li> <li>Builds intuition before formal mathematical understanding</li> <li>Allows students to focus on concepts rather than calculations</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#9-encourages-what-if-thinking","title":"9.  Encourages \"What If\" Thinking:","text":"<ul> <li>\"What if I add another layer?\"</li> <li>\"What if I change the activation function?\"</li> <li>\"What if I use different features?\"</li> <li>Promotes scientific thinking and hypothesis testing</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#10-builds-problem-solving-skills","title":"10.  Builds Problem-Solving Skills:","text":"<ul> <li>Students learn to diagnose training issues</li> <li>Develop strategies for network architecture design</li> <li>Understand trade-offs in different approaches</li> <li>Learn from both successes and failures</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#11-creates-aha-moments","title":"11.  Creates \"Aha!\" Moments:","text":"<ul> <li>Students can suddenly understand why deep networks work</li> <li>See how feature hierarchies develop</li> <li>Grasp concepts like feature extraction visually</li> <li>Experience the joy of discovery</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#12-bridges-theory-and-practice","title":"12.  Bridges Theory and Practice:","text":"<ul> <li>Connects classroom concepts to real behavior</li> <li>Shows practical implications of theoretical choices</li> <li>Makes abstract mathematics concrete</li> <li>Demonstrates why certain practices work or fail</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#13-promotes-active-learning","title":"13.  Promotes Active Learning:","text":"<ul> <li>Students drive their own exploration</li> <li>Learn through trial and error</li> <li>Can repeat experiments easily</li> <li>Get immediate feedback on their understanding</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#14-builds-confidence","title":"14.  Builds Confidence:","text":"<ul> <li>Success is easily achievable</li> <li>Mistakes are not penalized</li> <li>Progress is visible and rewarding</li> <li>Creates positive associations with neural networks</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#learning-outcomes","title":"Learning Outcomes:","text":"<ul> <li>Better understanding of neural network architecture</li> <li>Intuitive grasp of feature learning</li> <li>Clear visualization of decision boundaries</li> <li>Practical insight into hyperparameter tuning</li> </ul>"},{"location":"prompts/12-timeline/","title":"Timeline of Deep Learning Events","text":"<p>Prompt</p> <p>You are an expert at the history of Deep Learning. Please create a timeline of the 100 most important events in the development of Deep Learning. Return the event list in a single HTML file that could be used by the vis-timeline JavaScript library. An example template is shown below.</p> <pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Timeline&lt;/title&gt;\n  &lt;script type=\"text/javascript\" src=\"https://unpkg.com/vis-timeline@latest/standalone/umd/vis-timeline-graph2d.min.js\"&gt;&lt;/script&gt;\n  &lt;link href=\"https://unpkg.com/vis-timeline@latest/styles/vis-timeline-graph2d.min.css\" rel=\"stylesheet\" type=\"text/css\" /&gt;\n  &lt;style type=\"text/css\"&gt;\n    #visualization {\n      width: 600px;\n      height: 400px;\n      border: 1px solid lightgray;\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;div id=\"visualization\"&gt;&lt;/div&gt;\n&lt;script type=\"text/javascript\"&gt;\n  // DOM element where the Timeline will be attached\n  var container = document.getElementById('visualization');\n\n  // Create a DataSet (allows two way data-binding)\n  var items = new vis.DataSet([\n    {id: 1, content: 'item 1', start: '2014-04-20'},\n    {id: 2, content: 'item 2', start: '2014-04-14'},\n    {id: 3, content: 'item 3', start: '2014-04-18'},\n    {id: 4, content: 'item 4', start: '2014-04-16', end: '2014-04-19'},\n    {id: 5, content: 'item 5', start: '2014-04-25'},\n    {id: 6, content: 'item 6', start: '2014-04-27', type: 'point'}\n  ]);\n\n  // Configuration for the Timeline\n  var options = {};\n\n  // Create a Timeline\n  var timeline = new vis.Timeline(container, items, options);\n&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"prompts/12-timeline/#workflow-for-initial-prompt","title":"Workflow for Initial Prompt","text":"<p>ChatGPT o1 thought about Deep Learning history timeline for 38 seconds</p> <p>Mapping evolution</p> <p>I'm tasked with crafting a timeline of 100 key deep learning events from 1943 to today. This involves producing a comprehensive HTML file using the vis-timeline library, ensuring clarity and segmentation in line with the provided example.</p> <p>Highlighting foundational events</p> <p>I'm mapping out key milestones in deep learning, including McCulloch &amp; Pitts in 1943, Donald Hebb's learning rule in 1949, and diverse breakthroughs through to 2023.</p> <p>Charting milestones</p> <p>I'm noting pivotal moments in deep learning, including the birth of AI in 1956 and Rosenblatt's perceptron in 1957. Progress continues through diverse breakthroughs up to 2023.</p> <p>Tracking key milestones</p> <p>I'm updating the timeline with significant events such as McCulloch &amp; Pitts in 1943, the Turing test in 1950, and Rosenblatt's perceptron in 1957, organizing them chronologically by year.</p> <p>Tracking deep learning strides</p> <p>Mapping out milestones like Minsky &amp; Papert's \"Perceptrons\" in 1969, the AI winter in the 1970s, and transformative advances like \"AlphaGo\" and BERT from 2016 onwards.</p> <p>Mapping key developments</p> <p>I'm laying out pivotal milestones in deep learning, like McCulloch &amp; Pitts' model in 1943, Donald Hebb's learning rule in 1949, and breakthroughs such as transformers from 2017 onwards.</p> <p>Charting progress</p> <p>Mapping significant developments in neural networks, like the Neocognitron's introduction as a precursor to convolutional neural nets in 1979, and the formalization of Hopfield networks for associative memory in 1981.</p> <p>Advancing neural network training</p> <p>Highlighting the creation and application of backpropagation, neural network training techniques, and landmark publications like \"Parallel Distributed Processing\" and LeCun's work on convolutional neural networks.</p> <p>Tracking breakthroughs</p> <p>I'm noting critical advancements like IBM's Deep Blue in 1997, the introduction of LSTM networks, and the availability of GPU programming for computations, which significantly influenced AI research and applications.</p> <p>Tracing key milestones</p> <p>I'm noting the development of deep learning technologies like AlexNet's breakthrough on ImageNet, the emergence of GANs, and the progression of neural machine translation, which have all significantly advanced AI capabilities.</p> <p>Charting AI evolution</p> <p>I'm tracking the progression of AI, showcasing milestones like the introduction of transformers, GPT-3's launch, and the significant advancement of text-to-image generation technologies.</p> <p>Tracking milestones</p> <p>I'm organizing AI milestones for each year, ensuring concise and consistent entries, and wrapping them in HTML with specific formatting. Progress is being made step by step.</p>"},{"location":"prompts/12-timeline/#result","title":"Result","text":"<p>Deep Learning Timeline MicroSim</p>"},{"location":"prompts/12-timeline/#reading-timeline-data-from-a-json-file","title":"Reading Timeline Data from a JSON File","text":"<p>Prompt</p> <p>Rewrite the program to read the timeline data from a JSON file.</p>"},{"location":"prompts/12-timeline/#workflow","title":"Workflow","text":"<p>Thinking through options</p> <p>I'm analyzing how to fetch data from an external JSON file into a vis-timeline. Using a fetch request will streamline the process by pulling the data from the file and then updating the timeline.</p> <p>Naming and consistency</p> <p>I'm keeping the code name and timeline config intact, specifying the JSON file format, and using fetch to pull in data and update the timeline with a single snippet.</p> <p>Below is an example of how you might restructure your HTML/JavaScript so that the timeline data comes from a separate JSON file named <code>items.json</code>. Make sure the <code>items.json</code> file is located in the same directory as your HTML page (or update the <code>fetch</code> path accordingly).</p> <pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Deep Learning Timeline (1935-2025)&lt;/title&gt;\n  &lt;script type=\"text/javascript\" src=\"https://unpkg.com/vis-timeline@latest/standalone/umd/vis-timeline-graph2d.min.js\"&gt;&lt;/script&gt;\n  &lt;link href=\"https://unpkg.com/vis-timeline@latest/styles/vis-timeline-graph2d.min.css\" rel=\"stylesheet\" type=\"text/css\" /&gt;\n  &lt;style type=\"text/css\"&gt;\n    #visualization {\n      width: 90%;\n      height: 600px;\n      border: 1px solid lightgray;\n      margin: 0 auto;\n    }\n    body {\n      font-family: Arial, sans-serif;\n      margin: 20px;\n    }\n    h1 {\n      text-align: center;\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Key Events in Deep Learning History (1935-2025)&lt;/h1&gt;\n&lt;div id=\"visualization\"&gt;&lt;/div&gt;\n\n&lt;script type=\"text/javascript\"&gt;\n  // DOM element where the Timeline will be attached\n  var container = document.getElementById('visualization');\n\n  // Fetch timeline data from external JSON file\n  fetch('items.json')\n    .then(response =&gt; response.json())\n    .then(data =&gt; {\n      // Create a DataSet (allows two way data-binding) using the fetched data\n      var items = new vis.DataSet(data);\n\n      // Configuration for the Timeline\n      var options = {\n        // Restrict the timeline's visible window (and scrolling) to 1935 - 2025\n        min: \"1935-01-01\",\n        max: \"2025-12-31\",\n        start: \"1935-01-01\",\n        end: \"2025-12-31\",\n        stack: true,\n        maxHeight: '90%',\n        minHeight: '300px',\n        margin: {\n          item: 10,\n          axis: 10\n        }\n      };\n\n      // Create and render the Timeline\n      var timeline = new vis.Timeline(container, items, options);\n    })\n    .catch(error =&gt; {\n      console.error('Error loading timeline data:', error);\n    });\n&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"prompts/12-timeline/#cutoff-date-analysis","title":"Cutoff Date Analysis","text":"<p>Prompt</p> <p>What is your cutoff date?  What are the latest events you are aware of?</p>"},{"location":"prompts/12-timeline/#response","title":"Response","text":"<p>Establishing key timelines</p> <p>I\u2019m figuring out the knowledge cutoff for the GPT model, which is January 2021. This means I have no updates on data after that, but can provide some info up to 2023 based on user interactions.</p>"},{"location":"prompts/12-timeline/#workflow_1","title":"Workflow","text":"<p>Addressing knowledge boundaries</p> <p>OK, let me see. I\u2019m noting that the knowledge cutoff for ChatGPT is around 2021, and I\u2019m not current with information post-2021. This indicates a key aspect of the assistant's capabilities.</p>"},{"location":"prompts/12-timeline/#result_1","title":"Result","text":"<p>I don\u2019t have direct, real-time knowledge of current events; instead, I rely on a snapshot (or \u201ccutoff\u201d) of information from my training. Generally, for most of the ChatGPT (GPT-3.5) models, that snapshot includes data up to some point in 2021 (with limited awareness of events after that time).</p> <p>However, since conversation can include user-provided information that extends beyond 2021, I can discuss some more recent topics if those details appear in the prompts (as happened with your timeline request). But strictly speaking, my model itself does not have built-in awareness of events after my original training cutoff. If you share newer information, I can process it in context, but I don\u2019t automatically \u201cknow\u201d facts that emerged after my training window.</p>"},{"location":"prompts/12-timeline/#openai-o1-is-not-self-aware","title":"OpenAI o1 is not Self Aware","text":""},{"location":"prompts/14-alexnet-story/","title":"Generating a Story about AlexNet","text":"<p>Prompt</p> <p>Tell a fun story about how Alexnet won the Imagnet competition in 2012.  Give as much detail as possible.  When appropriate, suggest an image that could be inserted to make the story a form of graphic novel.  Describe each image in detail and be consistent across all the images for style.</p>"},{"location":"prompts/15-ai-winters/","title":"AI Winter","text":"<p>Prompt</p> <p>Create a fun and entertaining story about the \"AI winters\" and how AI grew and died periodically until 2012 before AlexNet. Give as much detail as possible. Include quotes of famous people over-promising when AI would mature.  When appropriate, suggest an image that could be inserted into the story to make the story a form of graphic novel. Describe each image in detail and be consistent across all the images in the story for style.</p>"},{"location":"prompts/13-summarize-folder/","title":"Google Docs Summary of Deep Learning Folder","text":"<p>The folder \"EE 4940\" contains materials related to an introductory course on Deep Learning, likely taught at the University of Minnesota, specifically for EE 4940 in Spring 2025. The materials include lecture slides, homework assignments, and notes related to deep learning concepts and practical applications.</p> <p>Here are some key themes addressed in the folder:</p>"},{"location":"prompts/13-summarize-folder/#introduction-to-deep-learning-and-neural-networks","title":"Introduction to Deep Learning and Neural Networks","text":"<p>The folder covers fundamental concepts of artificial intelligence, machine learning, and deep learning, starting from the basics of perceptrons to more complex architectures like Convolutional Neural Networks (CNNs). It discusses the architecture of artificial neurons, activation functions, loss functions, and the backpropagation algorithm used to train neural networks. There's also a focus on the historical context of AI and the evolution of neural network architectures.</p>"},{"location":"prompts/13-summarize-folder/#practical-applications-and-tools","title":"Practical Applications and Tools","text":"<p>The materials delve into practical aspects of deep learning, including how to use high-performance computing resources like the Agate cluster at the Minnesota Supercomputing Institute (MSI). It mentions ready-to-run containers, Jupyter notebooks, and batch GPU computing for deep learning tasks. There's also information on using frameworks like PyTorch and TensorFlow for implementing deep learning models, as well as discussions about hardware requirements and optimization techniques.</p>"},{"location":"prompts/13-summarize-folder/#training-and-optimization-of-deep-learning-models","title":"Training and Optimization of Deep Learning Models","text":"<p>A significant portion of the folder is dedicated to the training process of neural networks. It covers various optimization algorithms like Stochastic Gradient Descent (SGD), the concept of learning rate, and how to address common issues like vanishing and exploding gradients. The materials also discuss the importance of validation and test sets, overfitting and underfitting, and techniques like dropout to improve model generalization.</p> <p>The \"EE 4940\" folder contains materials related to an \"Introduction to Deep Learning\" course, covering fundamental concepts, neural network architectures, and practical applications.</p> <p>Here are some key themes addressed:</p>"},{"location":"prompts/13-summarize-folder/#deep-learning-fundamentals","title":"Deep Learning Fundamentals","text":"<p>The folder includes materials explaining the basics of deep learning, such as neural network architecture (including perceptrons and deep neural networks), activation functions (like ReLU, sigmoid, and tanh), loss functions (MSE, cross-entropy), and optimization techniques (gradient descent, SGD). These materials aim to provide a solid foundation for understanding how neural networks learn and make predictions.</p>"},{"location":"prompts/13-summarize-folder/#convolutional-neural-networks-cnns","title":"Convolutional Neural Networks (CNNs)","text":"<p>A significant portion of the content focuses on CNNs, which are particularly effective for image processing tasks. The materials discuss how CNNs mimic biological vision by extracting hierarchical features, the concept of convolution as a matrix operation, pooling layers for downsampling, and practical implementations of CNNs for tasks like MNIST digit classification.</p>"},{"location":"prompts/13-summarize-folder/#practical-applications-and-tools_1","title":"Practical Applications and Tools","text":"<p>The folder also touches on practical aspects of deep learning, such as using TensorFlow/Keras and PyTorch for implementing models, using computing resources like the Minnesota Supercomputing Institute's Agate cluster, and the importance of data handling, training/validation/test splits, and techniques to prevent overfitting (like dropout). There are also examples of real-world applications, such as protein folding prediction and AI-assisted coding.</p>"},{"location":"sims/","title":"List of Deep Learning MicroSims","text":""},{"location":"sims/#learning-graph","title":"Learning Graph","text":"<p>Learning Graph</p>"},{"location":"sims/#wordcloud","title":"Wordcloud","text":"<p>Word Cloud</p>"},{"location":"sims/#model-parameter-count","title":"Model Parameter Count","text":"<p>Param Count</p>"},{"location":"sims/#responsive-template","title":"Responsive Template","text":"<p>Responsive Template</p>"},{"location":"sims/conv-filter-mult/","title":"Convolution Filter Multiply MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/deep-learning-course/sims/conv-filt-mul/main.html\" width=\"600px\" height=\"450px\" scrolling=\"no\"\n  style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim</p> <p>Edit the MicroSim</p>"},{"location":"sims/convo-with-filters/","title":"Convolution with Filters MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/deep-learning-course/sims/convo-with-filters/main.html\" height=\"500px\" scrolling=\"no\"\n  style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the Convolution with Filter MicroSim</p> <p>Edit the Convolution with Filter MicroSim</p>"},{"location":"sims/convolution/","title":"Convolution MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/deep-learning-course/sims/convolution/main.html\" width=\"600px\" height=\"450px\" scrolling=\"no\"\n  style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the Convolution MicroSim</p> <p>Edit the Convolution MicroSim</p>"},{"location":"sims/four-futures/","title":"Four AI Futures MicroSim","text":"<pre><code>&lt;iframe src=\"./https://dmccreary.github.io/deep-learning-course/sims/four-futures/main.html\" height=\"400px\" scrolling=\"no\"\n  style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the Four Futures MicroSim</p> <p>Edit the Four Futures MicroSim</p> <p>Prompt</p> <p>Create a p5.js MicroSim that generates a line chart. Across the bottom x axis draw an axis label \"Time\". In the center of the Time axis draw a vertical brown line with the label \"Now\".</p> <p>Across the left vertical axis draw an axis with the label \"Rate of Improvement\".</p> <p>Draw four lines on the chart:</p> <ol> <li>Growth and Decline: initial rapid growth but peaking now and will be declining in the future </li> <li>Growth and Flattening: initial rapid growth but flatting out </li> <li>Linear Growth: initial rapid growth but slow linear growth </li> <li>Exponential Growth: initial rapid growth and continued exponential growth</li> </ol> <p>Add a checkbox control to display each option.</p>"},{"location":"sims/learning-graph/","title":"Learning Graph for Deep Learning Course","text":"<p>Run Learning Graph MicroSim</p>"},{"location":"sims/param-count/","title":"Neural Network Parameter Count MicroSim","text":"<p>Run the Neural Network Parameter Count MicroSim</p> <p>Edit this MicroSim</p> <p>Prompt</p> <p>Generate a p5.js file that draws a simple neural network. Use circles to draw the nodes and lines to draw the weights. Use the responsive design template in the project area. The input layer is on the left.  The output layer is on the right. The layer on the left should be labeled \"Input\". The layer on the right should be labeled \"Output\" Add two slider controls in the controls region that change the number of layers and the number of neurons in a layer. Add two text items that show the number of bias weights and the total number of parameters in the model. Display the equation for calculating the total number of parameters.</p>"},{"location":"sims/param-count/#explanation","title":"Explanation","text":"<p>I'll help you calculate the correct formula and create a visualization. Let me analyze the current code and make necessary corrections.</p> <p>Let's first calculate the correct formula:</p> <p>Now I'll create a React component that visualizes this network:</p> <p>Let me explain the formula for calculating the total parameters in a fully connected neural network:</p> <ol> <li>For weights:</li> <li>Each connection between two layers needs a weight parameter</li> <li>For each pair of adjacent layers: (nodes in previous layer \u00d7 nodes in current layer)</li> <li>Since input layer doesn't have incoming connections, we multiply by (numLayers - 1)</li> <li> <p>Total weight parameters = neuronsPerLayer \u00d7 neuronsPerLayer \u00d7 (numLayers - 1)</p> </li> <li> <p>For biases:</p> </li> <li>Each neuron (except in input layer) has one bias parameter</li> <li> <p>Total bias parameters = neuronsPerLayer \u00d7 (numLayers - 1)</p> </li> <li> <p>Total parameters = Total weight parameters + Total bias parameters</p> </li> </ol> <p>For example, in a network with 3 layers (including input) and 4 neurons per layer: - Weight parameters: 4 \u00d7 4 \u00d7 (3-1) = 32 - Bias parameters: 4 \u00d7 (3-1) = 8 - Total parameters: 32 + 8 = 40</p> <p>I've created an interactive visualization where you can: 1. Adjust the number of layers (2-7) 2. Adjust the neurons per layer (2-10) 3. See the network structure 4. Get a breakdown of weight parameters, bias parameters, and total parameters</p> <p>Would you like me to add any additional features to the visualization?</p> <pre><code>// Demo of parameters in neural networks with a responsive design\nlet containerWidth;  // Will be set based on container size\nlet containerHeight = 450;  // Fixed height (400 for drawing + 50 for controls)\nlet drawHeight = 400;\nlet controlHeight = 50;\nlet sliderLeftMargin = 220;\nlet numLayersSlider;\nlet numNeuronsSlider;\nlet parameterCount = 0;\nlet labelValueWidth = 240;\nlet numLayers = 3;  // Default number of layers (including Input and Output)\nlet numNeurons = 4; // Default number of neurons in each layer\n\nfunction setup() {\n    updateCanvasSize();\n    const canvas = createCanvas(containerWidth, containerHeight);\n    var mainElement = document.querySelector('main');\n    canvas.parent(mainElement);\n\n    numLayersSlider = createSlider(2, 7, 3);\n    numLayersSlider.position(sliderLeftMargin, drawHeight + 6);\n    numLayersSlider.size(containerWidth - sliderLeftMargin - 25);\n    numLayersSlider.input(updateNetwork);\n\n    numNeuronsSlider = createSlider(2, 10, 4);\n    numNeuronsSlider.position(sliderLeftMargin, drawHeight + 26);\n    numNeuronsSlider.size(containerWidth - sliderLeftMargin - 25);\n    numNeuronsSlider.input(updateNetwork);\n\n    updateNetwork();\n}\n\nfunction updateCanvasSize() {\n    const container = document.querySelector('main').getBoundingClientRect();\n    containerWidth = Math.floor(container.width);\n}\n\nfunction windowResized() {\n    updateCanvasSize();\n    resizeCanvas(containerWidth, containerHeight);\n\n    // Update slider positions and sizes\n    numLayersSlider.size(containerWidth - sliderLeftMargin - 25);\n    numNeuronsSlider.size(containerWidth - sliderLeftMargin - 25);\n\n    updateNetwork();\n}\n\nfunction updateNetwork() {\n    // make the background of the drawing region a very light blue\n    fill('aliceblue');\n    stroke('silver');\n    rect(0, 0, containerWidth, drawHeight);\n\n    fill('white');\n    rect(0, drawHeight, containerWidth, controlHeight);\n    stroke('blue');\n\n    numLayers = numLayersSlider.value();\n    numNeurons = numNeuronsSlider.value();\n\n    let layerPositions = Array.from(\n        {length: numLayers}, \n        (_, i) =&gt; map(i, 0, numLayers - 1, 0.2, 0.8) * containerWidth\n    );\n\n    parameterCount = 0;\n\n    for (let i = 0; i &lt; layerPositions.length; i++) {\n        let label = \"Hidden\";\n        if (i === 0) label = \"Input\";\n        else if (i === layerPositions.length - 1) label = \"Output\";\n\n        drawNodes(layerPositions[i], drawHeight / 2, numNeurons, label);\n    }\n\n    for (let i = 0; i &lt; layerPositions.length - 1; i++) {\n        connectLayers(\n            layerPositions[i], drawHeight / 2, numNeurons,\n            layerPositions[i + 1], drawHeight / 2, numNeurons\n        );\n        parameterCount += numNeurons * numNeurons; // Adding weights\n    }\n\n    parameterCount += (numLayers - 1) * numNeurons; // Adding biases\n\n    // Show labels and parameter count\n    noStroke();\n    fill('black');\n    textSize(16);\n    textAlign(LEFT);\n    text('Number of Layers: ' + numLayers, 5, numLayersSlider.y + 5);\n    text('Number of Neurons/Layer: ' + numNeurons, 5, numNeuronsSlider.y + 5);\n\n    textAlign(LEFT);\n    text('Bias Weights: ' + numNeurons, 10, drawHeight - 20);\n    textAlign(CENTER, CENTER);\n    text('Total Parameter Count: ' + parameterCount, containerWidth / 2, drawHeight - 20);\n}\n\nfunction drawNodes(x, y, numNodes, label) {\n    stroke('blue');\n    strokeWeight(2);\n    let gap = 40;\n    let startY = y - ((numNodes - 1) * gap) / 2;\n    for (let i = 0; i &lt; numNodes; i++) {\n        ellipse(x, startY + i * gap, 20, 20);\n    }\n\n    noStroke();\n    fill('black');\n    textSize(16);\n    textAlign(CENTER, CENTER);\n    text(label, x, y + ((numNodes + 1) * gap) / 2);\n}\n\nfunction connectLayers(x1, y1, numNodes1, x2, y2, numNodes2) {\n    stroke('blue');\n    strokeWeight(1);\n    let gap1 = 40;\n    let gap2 = 40;\n    let startY1 = y1 - ((numNodes1 - 1) * gap1) / 2;\n    let startY2 = y2 - ((numNodes2 - 1) * gap2) / 2;\n    for (let i = 0; i &lt; numNodes1; i++) {\n        for (let j = 0; j &lt; numNodes2; j++) {\n            line(x1, startY1 + i * gap1, x2, startY2 + j * gap2);\n        }\n    }\n}\n</code></pre>"},{"location":"sims/sliding-window/","title":"CNN Sliding Window MicroSim","text":"<p>Run the Sliding Window MicroSim</p> <p>Edit the Sliding Window MicroSim</p> <p>This CNN Sliding Window MicroSim demonstrates one of the fundamental operations in Convolutional Neural Networks - how a kernel or filter moves across an input matrix to perform convolution.</p>"},{"location":"sims/sliding-window/#key-features","title":"Key Features","text":"<ul> <li>Input Matrix: An 8\u00d78 grid of random integers (0-9) representing pixel values or feature map data</li> <li>Sliding Window: A 3\u00d73 red-outlined window that moves across the input matrix</li> <li>Window Content: A separate display showing the current values inside the sliding window</li> <li>Interactive Controls:<ul> <li>Slider to manually position the sliding window</li> <li>Start/Pause button to automate the window movement</li> <li>Reset button to return the window to the starting position</li> </ul> </li> </ul>"},{"location":"sims/sliding-window/#how-it-works","title":"How It Works","text":"<p>The simulation follows the MicroSim rules with a responsive width layout. It visually demonstrates how convolution works in CNNs by:</p> <ol> <li>Showing the complete input matrix on the left side</li> <li>Highlighting the current 3\u00d73 window position with a red outline</li> <li>Extracting and displaying the values within that window on the right side</li> </ol> <p>This helps learners understand how a CNN processes information by examining small regions at a time, which is the key to how CNNs detect local patterns like edges, textures, and shapes in images.</p>"},{"location":"sims/sliding-window/#learning-value","title":"Learning Value","text":"<p>This simple simulation establishes a foundation for understanding more complex CNN operations. In an actual CNN, mathematical operations (like multiplication with filter weights and summation) would be performed on these window values to produce a single output value in the resulting feature map.</p> <p>You can adjust the sliding window position manually using the slider or watch it move automatically by pressing the Start button. The Reset button returns the window to the initial position (top-left corner).</p>"},{"location":"sims/sliding-window/#changes-for-responsive-design","title":"Changes for Responsive Design","text":"<ul> <li>Replaced static width/height with containerWidth/containerHeight</li> <li>Added updateCanvasSize() function to get container dimensions</li> <li>Added windowResized() handler to update canvas and UI elements</li> <li>Modified slider positioning and sizing to be relative to container width</li> <li>Updated all drawing calculations to use containerWidth instead of fixed width</li> <li>Set a fixed containerHeight while allowing width to be responsive</li> <li>Adjusted margins and spacing to work better with different screen sizes</li> </ul> <p>The visualization will now resize horizontally while maintaining its functionality and proportions.</p>"},{"location":"sims/templates/","title":"MicroSim Templates","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/deep-learning-course/sims/NAM/main.html\"  height=\"450px\" scrolling=\"no\"\n  style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim</p> <p>Edit the MicroSim</p>"},{"location":"sims/timeline/","title":"Deep Learning Timeline MicroSim","text":"<p>Here is a timeline that includes over 100 key events in the field of Deep Learning. The events span from 1935 to present, although events in the last two years had to be entered by hand since ChatGPT o1 is Frozen in Time</p> <p>Run the Deep Learning Timeline MicroSim</p>"},{"location":"sims/wordcloud/","title":"Wordcloud","text":"<p>Run the MicroSim</p>"},{"location":"stories/","title":"Deep Learning Stories","text":"<p>Here are some examples of how Generative AI tools can be used to generate graphic-novel like stories.  I used Anthropic Claude 3.7 to generate the narrative text for these stories.  In my prompt, I stated When appropriate, suggest an image that could be inserted into the story to make the story a form of graphic novel. I then use OpenAI's ChatGPT o4 to generate the images.</p>"},{"location":"stories/#the-ai-winters-story","title":"The AI Winters Story","text":"<p>The AI field experienced dramatic cycles of enthusiasm and disappointment known as \"AI winters\" from the 1950s until 2012. Starting with grand promises from pioneers like McCarthy, Simon, and Minsky in the 1950s-60s, the field faced its first winter in 1973 after the Perceptron limitations were exposed and funding dried up. It rebounded in the 1980s with expert systems, only to crash again by 1987 when these systems proved brittle and impractical. A quieter machine learning era emerged in the 1990s with modest successes like Deep Blue, but it wasn't until 2012 when AlexNet's breakthrough at the ImageNet competition finally broke the boom-bust cycle, ushering in an \"eternal spring\" for AI that continues today.</p> <p>Read The AI Winters Story</p>"},{"location":"stories/#the-alexnet-story","title":"The AlexNet Story","text":"<p>This graphic novel sequence tells the origin story of deep learning's revolution, beginning with \"The Toronto Trio\"\u2014Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton\u2014facing the monumental challenge of neural networks. In \"The GPU Awakening,\" they harness the power of gaming GPUs to accelerate training. \"The Training Vigil\" captures their relentless effort, leading to \"The Revealing,\" where AlexNet stuns the world with a groundbreaking 15.3% error rate. \"The Revolution Begins\" shows the global ripple effect as labs adopt deep learning and AI breakthroughs multiply. The story concludes with \"The Neural Future,\" a hopeful epilogue depicting modern AI applications branching out from the glowing seed of AlexNet.</p> <p>Read the Alexnet Story</p>"},{"location":"stories/ai-winters/","title":"The AI Winters: A Tale of Boom and Bust","text":"<p>[Image: The Cycles of AI - A grand timeline visualization showing a wavy roller-coaster like path from the 1950s to 2012, with dramatic peaks and valleys marked by calendar pages fluttering away. In the foreground, various symbolic elements from each era (vacuum tubes, robots, neurons) are scattered like autumn leaves. Art style: detailed line art with watercolor washes, muted blues and grays for winter periods, warm oranges and yellows for boom periods.]</p>"},{"location":"stories/ai-winters/#chapter-1-the-birth-of-a-dream-1950s-1960s","title":"Chapter 1: The Birth of a Dream (1950s-1960s)","text":"<p>[Image: \"The Dartmouth Dreamers\" - A retro-style illustration showing a summer campus scene at Dartmouth College. In the foreground, four enthusiastic men in 1950s attire stand before a chalkboard covered in flowcharts and equations. Speech bubbles above their heads show thought clouds containing robots, thinking machines, and neural networks. Art style: Norman Rockwell meets mid-century scientific illustration.]</p> <p>It was the summer of 1956 when John McCarthy invited fellow dreamers to Dartmouth College for what would become known as the birth of artificial intelligence. McCarthy boldly declared:</p> <p>\"We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956... An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.\"</p> <p>Herbert Simon and Allen Newell were even bolder, proclaiming at the conference: \"Within ten years, a digital computer will be the world's chess champion\" and \"within twenty years, machines will be capable of doing any work a man can do.\"</p> <p>The optimism was intoxicating. Money poured in from government and industry. Marvin Minsky predicted in 1970: \"In from three to eight years we will have a machine with the general intelligence of an average human being.\"</p>"},{"location":"stories/ai-winters/#chapter-2-the-first-ai-winter-1973-1980","title":"Chapter 2: The First AI Winter (1973-1980)","text":"<p>[Image: \"The Perceptron's Demise\" - A gothic-horror inspired scene showing a mechanical brain or perceptron being dissected on a table, with Minsky and Papert looming over it holding mathematical equations like scalpels. Snow blows through broken windows of a once-grand laboratory. Art style: Edward Gorey-esque crosshatching with dramatic shadows.]</p> <p>The first winter came suddenly. In 1969, Marvin Minsky and Seymour Papert published \"Perceptrons,\" mathematically proving that simple neural networks couldn't even solve basic problems like XOR. The book was like a bucket of ice water on the field's optimism.</p> <p>Government funding dried up after the 1973 Lighthill Report called AI research a dead end. Labs shuttered. Graduate students fled to other fields. The promised thinking machines had failed to materialize, and the world lost patience.</p>"},{"location":"stories/ai-winters/#chapter-3-the-expert-systems-spring-1980-1987","title":"Chapter 3: The Expert Systems Spring (1980-1987)","text":"<p>[Image: \"The Rule Kingdom\" - An illustration in the style of a medieval manuscript showing programmers as monks transcribing thousands of IF-THEN rules into giant books. In the background, executives throw gold coins at them. A mechanical decision tree towers like a cathedral spire. Art style: illuminated manuscript meets corporate tech, rich golds and blues.]</p> <p>Like crocuses pushing through snow, AI began to bloom again with expert systems. These rule-based programs promised to capture human expertise in code. Edward Feigenbaum declared:</p> <p>\"The world is moving from the industrial age to the age of knowledge... knowledge is power.\"</p> <p>DEC's XCON system saved the company $40 million a year configuring computers. Japan announced their Fifth Generation Computer project in 1982, promising to create intelligent computers by 1992. The AI race was back on.</p>"},{"location":"stories/ai-winters/#chapter-4-the-second-ai-winter-1987-1993","title":"Chapter 4: The Second AI Winter (1987-1993)","text":"<p>[Image: \"The Symbolic Collapse\" - A surreal Dali-esque landscape where giant IF-THEN rules melt like clocks, expert system books crumble to dust, and abandoned LISP machines rust in a wasteland. Government officials and investors flee under dark storm clouds. Art style: Salvador Dali meets corporate dystopia.]</p> <p>But expert systems hit a wall. They were brittle, expensive to maintain, and couldn't handle the messy real world. By 1987, the AI hardware market collapsed. The Japanese Fifth Generation project failed to deliver on its promises.</p> <p>Rodney Brooks wrote a scathing critique in 1990: \"The symbol system hypothesis upon which classical AI is based is fundamentally flawed.\" Companies that had bet heavily on AI went bankrupt. Another winter had begun.</p>"},{"location":"stories/ai-winters/#chapter-5-the-machine-learning-spring-1993-2001","title":"Chapter 5: The Machine Learning Spring (1993-2001)","text":"<p>[Image: \"The Data Dawn\" - A scene showing nerdy programmers in 90s attire feeding stacks of data into computers while decision trees and support vector machines grow like plants from the machines. In the background, the early internet is represented as glowing fiber optic cables spreading like vines. Art style: cyber-renaissance, mixing classical composition with 90s tech aesthetics.]</p> <p>AI quietly reinvented itself. Machine learning algorithms that could learn from data began to show promise. IBM's Deep Blue defeated world chess champion Garry Kasparov in 1997. Search engines started using statistical methods to rank web pages.</p> <p>Vladimir Vapnik developed support vector machines. Yann LeCun built convolutional neural networks for handwriting recognition. But these successes were modest compared to earlier promises.</p>"},{"location":"stories/ai-winters/#chapter-6-the-deep-learning-revolution-2006-2012","title":"Chapter 6: The Deep Learning Revolution (2006-2012)","text":"<p>[Image: \"The Persistent Professor\" - Geoffrey Hinton is shown as a lone figure nurturing a small neural network plant in a cold, abandoned greenhouse. Through broken glass, we can see other researchers walking away. On his desk are piles of rejected grant applications. Art style: melancholic botanical illustration with a hint of steampunk.]</p> <p>While most had abandoned neural networks, Geoffrey Hinton kept the faith. In 2006, he discovered a way to train deep neural networks using a technique called \"pre-training.\" But few paid attention.</p> <p>Hinton later recalled: \"For many years, deep neural networks were considered a joke in the machine learning community.\"</p>"},{"location":"stories/ai-winters/#chapter-7-the-alexnet-breakthrough-2012","title":"Chapter 7: The AlexNet Breakthrough (2012)","text":"<p>[Image: \"The ImageNet Triumph\" - A heroic scene showing AlexNet as a glowing neural network architecture towering over a field of failed algorithms. At its base, the Toronto team raises their GPUs like olympic torches. In the sky, the error rate plummets like a shooting star. Art style: heroic socialist realism meets cyberpunk, dramatic angles and lighting.]</p> <p>And then came 2012. AlexNet's victory at ImageNet shattered decades of conventional wisdom. The 10.8% improvement wasn't just impressive\u2014it was revolutionary.</p> <p>Geoffrey Hinton reflected: \"It was pretty clear that as soon as we got deep learning to work well on ImageNet, the game was over. The people who thought neural nets were never going to work were always going to find reasons why the next thing wouldn't work.\"</p>"},{"location":"stories/ai-winters/#epilogue-the-eternal-spring","title":"Epilogue: The Eternal Spring","text":"<p>[Image: \"The Endless Spring\" - A final panoramic view showing all the AI winters and springs as a continuous landscape, but now with a great branching tree of achievement rising from AlexNet, its branches reaching into a starry sky filled with AI applications. The art style combines all previous styles into a cohesive grand finale.]</p> <p>After AlexNet, there would be no more winters. The combination of big data, GPU acceleration, and improved algorithms had finally unlocked the promise of AI. The boom-and-bust cycle that had characterized the field for half a century was over.</p> <p>The AI winters teach us that technology rarely advances in a straight line. Progress comes in fits and starts, with periods of irrational exuberance followed by crushing disappointment. But sometimes, persistence in the face of skepticism leads to breakthroughs that change everything.</p> <p>As we look to the future of AI, it's worth remembering this history\u2014not just to avoid repeating mistakes, but to appreciate how today's achievements were built on decades of perseverance through the coldest winters.</p>"},{"location":"stories/alexnet/","title":"The Day AlexNet Changed Everything: The ImageNet Conquest of 2012","text":"<p>In the world of artificial intelligence, there are watershed moments that fundamentally change the course of history. The ImageNet 2012 competition was one such moment, where an unlikely group of researchers staged what would become known as the \"deep learning revolution.\"</p>"},{"location":"stories/alexnet/#the-underdog-challengers","title":"The Underdog Challengers","text":"<p>[Image 1: \"The Toronto Trio\" - A stylized graphic novel panel showing three figures in silhouette standing before a massive wall of circuit boards and mathematical equations. Hands on hips, they look up at the challenge ahead. The art style should be high-contrast black and white with bold digital line work, reminiscent of a superhero origin story.  The students are Alex Krizhevsky, Ilya Sutskever, and their advisor Geoffrey Hinton]</p> <p>In 2012, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was already in its third year. The competition asked teams to classify millions of images into 1,000 different categories\u2014everything from goldfish to golf balls, from strawberries to space shuttles. Previous winners had achieved error rates of around 26%, using methods that had dominated computer vision for decades.</p> <p>Enter three researchers from the University of Toronto: Alex Krizhevsky, Ilya Sutskever, and their advisor Geoffrey Hinton. They had an audacious plan: to use neural networks, a technology most researchers had written off as computationally infeasible, to tackle the world's largest image recognition challenge.</p>"},{"location":"stories/alexnet/#the-secret-weapon-gpu-acceleration","title":"The Secret Weapon: GPU Acceleration","text":"<p>[Image 2: \"The GPU Awakening\" - A dynamic comic panel showing hands connecting gaming graphics cards into a server rack, with electric energy bolts emanating from the connections. The scene is rendered in neon blues and purples against a dark background, cyberpunk-style.]</p> <p>Krizhevsky had a brilliant insight: while CPUs were too slow to train large neural networks, the massively parallel architecture of gaming GPUs could potentially speed up the process by orders of magnitude. The team secured two NVIDIA GTX 580 GPUs, each with just 3GB of memory\u2014a pittance by today's standards.</p> <p>Their architecture, which would later be named AlexNet after Krizhevsky, was unlike anything the computer vision community had seen: - 60 million parameters - 5 convolutional layers - 3 fully connected layers - Novel use of ReLU activation functions - Local response normalization - Overlapping pooling - Dropout for regularization</p>"},{"location":"stories/alexnet/#training-day","title":"Training Day","text":"<p>[Image 3: \"The Training Vigil\" - A split-panel showing: Left side, researchers hunched over computers with coffee cups and pizza boxes scattered around, code scrolling on screens. Right side, a visualization of neural networks learning patterns, represented as streams of colored light flowing through an abstract brain structure. The art style mixes realistic elements with digital abstraction.]</p> <p>The team spent weeks training their network on 1.2 million images. The training process was grueling\u2014they had to split the network across two GPUs just to fit it in memory. Each epoch took hours, and they needed to run hundreds of epochs to achieve convergence. The lab became their second home, with team members taking shifts to monitor the training process day and night.</p>"},{"location":"stories/alexnet/#competition-day","title":"Competition Day","text":"<p>[Image 4: \"The Revealing\" - A dramatic panel showing the moment the results are displayed on a giant screen. In the foreground, the crowd gasps in shock, while the Toronto team stands quietly confident. The screen shows: \"AlexNet: 15.3% error rate\" with second place at \"26.2%\". The scene is rendered in a cinematic style with dramatic lighting.]</p> <p>When the results were announced, the computer vision community was stunned. AlexNet achieved a top-5 error rate of 15.3%, an astounding 10.8% improvement over the second-place finisher. It wasn't just an incremental improvement\u2014it was a quantum leap that shattered conventional wisdom.</p>"},{"location":"stories/alexnet/#the-aftermath","title":"The Aftermath","text":"<p>[Image 5: \"The Revolution Begins\" - A montage panel showing the aftermath: labs around the world switching to deep learning, GPUs being installed in research centers, and a timeline arrow shooting upward with AI breakthroughs popping up like sparks. The art style transitions from the grounded reality of the previous panels to a more abstract, inspiring vision of the future.]</p> <p>The victory of AlexNet marked the beginning of the deep learning revolution. Almost overnight, computer vision researchers around the world began adopting neural networks. The demand for GPUs skyrocketed, NVIDIA's stock price soared, and deep learning became the dominant paradigm in AI research.</p> <p>Geoffrey Hinton would later call it \"the moment when deep learning went from being a niche pursuit to being mainstream.\" Alex Krizhevsky went on to work at Google, then launched his own company. Ilya Sutskever would become one of the co-founders of OpenAI.</p>"},{"location":"stories/alexnet/#legacy","title":"Legacy","text":"<p>[Image 6: \"The Neural Future\" - An epilogue panel showing modern AI applications branching out from AlexNet like a technological tree of life: self-driving cars, medical imaging, facial recognition, and creative AI. At the center is the original AlexNet architecture diagram, glowing like a seed. The art style is optimistic and bright, with a touch of retro-futurism.]</p> <p>Today, AlexNet seems almost quaint compared to massive transformers and diffusion models. But its impact cannot be overstated. It proved that with enough data and computational power, neural networks could achieve what decades of hand-crafted algorithms could not. It sparked a paradigm shift that led to today's AI revolution.</p> <p>The story of AlexNet is a reminder that sometimes, the most revolutionary breakthroughs come not from incremental improvements, but from those willing to challenge conventional wisdom and pursue ideas that others have dismissed as impossible.</p> <p>This graphic novel-style account celebrates the historic moment when AlexNet changed the trajectory of artificial intelligence forever, proving that sometimes the greatest revolutions start with a few researchers, some gaming hardware, and an unwavering belief in an idea whose time has come.</p>"},{"location":"stories/transformers/","title":"Transformers and the Attention Revolution: How a Paper Changed AI Forever","text":"Show Narrative  Prompt     Please create a detailed fun and entertaining story about the publication of the paper \"Attention is All You Need\" and how the authors had a huge impact on the creation of LLMs.      Our goal is to have you generate the full text of the story, but to turn the story into a graphic novel with many illustrations that explain how transformers work.      As an example of attention, use the following sentence:      The chicken did not cross the road because it was too [tired|wide].      Give as much detail as possible about the team that wrote the paper and their ability to publish the paper despite being a key innovation at Google.       When appropriate, suggest an image that could be inserted into the story to make the story a form of graphic novel. Describe each image in detail and be consistent across all the images in the story for style."},{"location":"stories/transformers/#chapter-1-the-spark-of-an-idea","title":"Chapter 1: The Spark of an Idea","text":"Show Image Prompt     Panel 1: A nighttime scene at Google Brain's offices in Mountain View, California. Through the windows, we see various researchers working late. Some at whiteboards covered with equations, others at computers with multiple screens displaying neural network diagrams. The art style is reminiscent of a modern graphic novel with clean lines and rich colors, using deep blues and purples for the night sky contrasted with the warm yellow glow from the office windows.  <p>In the summer of 2016, at Google Brain's offices in Mountain View, California, a small team of researchers was grappling with a fundamental problem: neural machine translation systems were becoming increasingly complex, yet their performance improvements were plateauing.</p> <p>Ashish Vaswani, a senior software engineer with a quiet demeanor and brilliant analytical mind, had been pondering an elegant solution. What if they removed the recurrent neural networks (RNNs) and convolutional neural networks (CNNs) entirely from the architecture? What if attention \u2013 a mechanism that helped models focus on relevant parts of the input \u2013 could be the only component they needed?</p> <p>\"What if attention is all we need?\" he mused to his colleague Noam Shazeer one evening, not realizing he had just coined what would become one of the most famous paper titles in AI history.</p>"},{"location":"stories/transformers/#chapter-2-assembling-the-team","title":"Chapter 2: Assembling the Team","text":"Show Image Prompt Panel 2: Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A split-screen showing eight different researchers in their natural working environments. Each person has a thought bubble showing different aspects of the transformer architecture (embeddings, positional encoding, self-attention, etc.). The art style maintains consistency with Panel 1 but adds warm highlights to each character to emphasize their individual contributions. The panel is arranged like a Brady Bunch grid, suggesting how these separate minds would come together.  <p>The team that would revolutionize NLP came together almost by chance \u2013 a collection of researchers from Google Brain, Google Research, and the University of Toronto:</p> <p>Ashish Vaswani and Noam Shazeer from Google Brain were quickly joined by Niki Parmar and Jakob Uszkoreit, who brought expertise in neural network architectures.</p> <p>Llion Jones, known for his meticulous attention to detail, joined to help with the implementation. Aidan Gomez, a visiting undergraduate researcher from the University of Toronto, brought fresh perspective and energy.</p> <p>\u0141ukasz Kaiser, with his background in theoretical computer science and linguistics, provided crucial insights into how language models could be structured.</p> <p>Completing the team was Illia Polosukhin, whose practical engineering experience would be vital for making their theoretical ideas work in practice.</p> <p>Despite being at one of the world's leading AI research labs, they were going against the prevailing wisdom. RNNs and LSTMs were the dominant architectures for sequence modeling, and suggesting to completely remove them seemed almost heretical.</p>"},{"location":"stories/transformers/#chapter-3-the-breakthrough-mechanism","title":"Chapter 3: The Breakthrough Mechanism","text":"Show Image Prompt   Panel 3: Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A large, detailed diagram showing the self-attention mechanism. In the center is a key sentence: \"The chicken did not cross the road because it was too tired.\" Visual lines of different thicknesses connect the word \"it\" to other words, with the thickest line connecting to \"chicken,\" showing how attention helps resolve references. The diagram uses a vibrant, colorful scheme with blues, purples, and yellows to show attention weights. Small mathematical equations float around the edges of the panel, suggesting the formal underpinnings of the concept.  <p>At the heart of their innovation was the self-attention mechanism \u2013 a revolutionary way to process sequences of data like sentences. Unlike previous approaches that processed words in order or used sliding windows, self-attention allowed each word in a sentence to directly \"attend\" to every other word.</p> <p>\"Let me show you why this matters,\" Ashish explained during a team meeting. He wrote on the whiteboard: \"The chicken did not cross the road because it was too tired.\"</p> <p>\"What does it refer to? As humans, we know immediately that 'it' refers to 'the chicken.' But how does a computer know? In previous models, we'd need complex architectures to capture this relationship. With self-attention, each word calculates attention weights with every other word in the sentence.\"</p> <p>He drew arrows from \"it\" to every other word, making the arrow to \"chicken\" much thicker than the others.</p> <p>\"See? The model learns to pay attention to relevant context. If we change the sentence to 'The chicken did not cross the road because it was too wide,' the attention pattern would shift, with 'it' now attending more to 'road' instead of 'chicken'.\"</p> <p>This elegant solution allowed the model to capture long-range dependencies that had been challenging for previous architectures.</p>"},{"location":"stories/transformers/#chapter-4-building-in-secret","title":"Chapter 4: Building in Secret","text":"Show Image Prompt   Panel 4: Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A sequence of small panels showing the team working in various settings \u2013 late nights at the office, discussions over coffee, coding sessions with multiple screens. In the background, a large transformer architecture diagram gradually takes shape, like a blueprint being assembled. The art style should evoke a sense of clandestine development, with shadows and highlights creating a feeling of working on something revolutionary. The color palette shifts to include more reds and oranges, suggesting the building energy and excitement of creation.  <p>For months, the team worked tirelessly, often staying late into the night. They weren't just building a new model; they were reimagining how neural networks could process language.</p> <p>\"We need to be thorough,\" \u0141ukasz insisted. \"If we're going to challenge the status quo, our results need to be undeniable.\"</p> <p>The work wasn't officially sanctioned as a major project, and the team members often contributed during their \"20% time\" \u2013 Google's policy that allowed employees to spend one-fifth of their work week on projects of personal interest.</p> <p>They faced numerous challenges. The model was complex and required significant computational resources. Training was unstable at first, with the network either converging too slowly or diverging entirely.</p> <p>Llion Jones spent weeks debugging gradient issues, while Noam Shazeer optimized the implementation to work efficiently on Google's TPUs (Tensor Processing Units).</p> <p>Aidan Gomez, despite being the most junior member, proposed the multi-head attention mechanism \u2013 allowing the model to attend to information from different representation subspaces simultaneously.</p> <p>\"It's like having multiple perspectives on the same information,\" he explained. \"If single-head attention is like looking at something with one eye, multi-head attention gives you depth perception.\"</p>"},{"location":"stories/transformers/#chapter-5-the-architecture-takes-shape","title":"Chapter 5: The Architecture Takes Shape","text":"Show Image PromptPanel 5: Architecture Diagram  Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A detailed, exploded-view diagram of the Transformer architecture, styled like a technical blueprint but with a modern, digital aesthetic. Each component is color-coded: embeddings (blue), positional encoding (green), multi-head attention (purple), feed-forward networks (orange), and output layer (yellow). Small animated-style figures of the researchers are shown pointing to different components, explaining their functions. The background has a subtle grid pattern, emphasizing the technical nature while maintaining the graphic novel aesthetic.  <p>The final architecture they designed was remarkably elegant. At its core were several key innovations:</p> <ol> <li>Input Embeddings: Converting words into vectors</li> <li>Positional Encoding: Adding information about word position</li> <li>Multi-Head Self-Attention: Allowing words to attend to other words</li> <li>Feed-Forward Networks: Processing the attended information</li> <li>Residual Connections: Helping information flow through the network</li> <li>Layer Normalization: Stabilizing the training process</li> </ol> <p>\"Positional encoding was a breakthrough,\" Jakob explained. \"In RNNs, the order of words is implicit in the processing. But since our attention mechanism processes all words simultaneously, we needed a way to incorporate position information.\"</p> <p>Their solution was ingenious \u2013 they added sinusoidal position embeddings to the word embeddings, creating a unique pattern for each position that the model could learn to interpret.</p> <p>Niki Parmar demonstrated how the encoder-decoder structure allowed the model to be used for various tasks: \"The encoder processes the input sequence, and the decoder generates the output sequence. This works for translation, summarization, question answering \u2013 potentially any language task.\"</p>"},{"location":"stories/transformers/#chapter-6-the-paper-dilemma","title":"Chapter 6: The Paper Dilemma","text":"Show Image PromptPanel 6:   Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A tense meeting room scene. On one side of a long table sit the eight researchers, looking determined but nervous. On the other side are Google executives and managers with skeptical expressions. Documents and laptop screens show competing research from other teams. The lighting creates dramatic shadows, and the color palette uses contrasting cool tones for the executives and warmer tones for the research team, symbolizing the conflict. Outside the windows, we see the Google campus.  <p>By early 2017, the team knew they had something revolutionary. Their model was outperforming state-of-the-art systems in machine translation with a simpler, more elegant architecture. But they faced a dilemma: publishing their findings could give competitors an advantage.</p> <p>\"This is groundbreaking work,\" said one research director during a review meeting. \"But do we want to give away this technology to our competitors?\"</p> <p>Google, like other tech giants, struggled with balancing open science and competitive advantage. The AI research community valued open publication, but companies had business interests to protect.</p> <p>The team argued passionately for publication. \"This will advance the entire field,\" Illia contended. \"And Google will be recognized as the pioneer.\"</p> <p>\u0141ukasz pointed out another reality: \"If we don't publish, someone else will discover this eventually. Better to lead than follow.\"</p> <p>After weeks of internal debate and multiple review meetings, they received conditional approval to publish \u2013 but they would need to navigate the publication process carefully.</p>"},{"location":"stories/transformers/#chapter-7-nips-submission-crunch","title":"Chapter 7: NIPS Submission Crunch","text":"Show Image PromptPanel 7:   Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A montage showing the team in crunch mode. Papers and coffee cups are scattered everywhere. Some team members are slumped over desks, exhausted, while others are frantically typing. Clock faces around the margins show time passing. Computer screens display experimental results and paper drafts. The lighting shifts from daylight to night and back again, suggesting multiple days of intense work. The color palette is intense and slightly oversaturated, conveying the high-pressure environment.  <p>With approval secured, the team now faced a deadline: the submission deadline for NIPS (Neural Information Processing Systems), one of the premier conferences in machine learning, was rapidly approaching.</p> <p>\"We have three weeks,\" Ashish announced. \"We need to run all the experiments, finalize the architecture, and write the paper.\"</p> <p>The team entered crunch mode. Days blurred into nights as they worked to perfect their submission. Additional experiments were run to answer potential reviewer questions. The paper went through dozens of drafts.</p> <p>\"We need a catchy title,\" Noam suggested during a late-night writing session.</p> <p>\"Well, our whole premise is that attention mechanisms alone are sufficient,\" Ashish replied. \"So... 'Attention Is All You Need'?\"</p> <p>The room fell silent for a moment, then erupted in laughter and agreement. It was perfect \u2013 concise, bold, and slightly provocative, just like their approach.</p> <p>With minutes to spare before the deadline, they submitted the paper to NIPS 2017.</p>"},{"location":"stories/transformers/#chapter-8-the-world-notices","title":"Chapter 8: The World Notices","text":"Show Image PromptPanel 8:   Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A world map with glowing points of light appearing across research centers globally, connected by lines forming a network. Inset panels show researchers at different institutions reading the paper with expressions of surprise and excitement. Speech bubbles contain reactions like \"This changes everything!\" and \"We need to implement this immediately!\" The background transitions from dark to light, suggesting the dawn of a new era. The glowing network resembles transformer attention patterns, subtly reinforcing the concept.  <p>When \"Attention Is All You Need\" was published as a preprint on arXiv in June 2017, the reaction was immediate and electric. Research labs and tech companies around the world dropped what they were doing to implement and build upon the Transformer architecture.</p> <p>\"I've never seen anything like this,\" Jakob marveled as he tracked the paper's citations, which were growing exponentially. \"Labs everywhere are adopting our approach.\"</p> <p>Within months, the Transformer had become the foundation for new state-of-the-art results in almost every NLP task. Its efficient parallel computation meant it could be scaled to previously unimaginable sizes.</p> <p>At OpenAI, researchers were already thinking about how to adapt the Transformer architecture for generative pre-training. At Google itself, another team was developing BERT, which would use the Transformer encoder to revolutionize language understanding.</p> <p>What made the paper so influential wasn't just the architecture itself but its timing and accessibility. The code was released openly, allowing researchers everywhere to start experimenting immediately. The paper was clear and well-written, making the complex ideas accessible to a wide audience.</p>"},{"location":"stories/transformers/#chapter-9-the-birth-of-llms","title":"Chapter 9: The Birth of LLMs","text":"Show Image PromptPanel 9:   Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A chronological progression showing the evolution of transformer-based models, presented as a family tree. Starting with the original Transformer at the top, branches split to show BERT, GPT, T5, and other key models. Each model is represented by a distinct icon with size proportional to its parameter count. As the timeline moves downward, the models grow dramatically larger. Small annotations show key capabilities gained at each stage. The overall design resembles both a technological evolution chart and a natural growth pattern, reinforcing how these models evolved organically from the original idea. (Panel 9)  <p>The years following the Transformer paper saw an explosion of innovation. The architecture proved to be remarkably scalable \u2013 as models got bigger, their capabilities improved in unexpected ways.</p> <p>In 2018, OpenAI released GPT (Generative Pre-trained Transformer), adapting the decoder portion of the original architecture for text generation.</p> <p>Google countered with BERT (Bidirectional Encoder Representations from Transformers), which used the encoder portion for language understanding.</p> <p>Both approaches demonstrated the versatility of the Transformer architecture.</p> <p>\"What's fascinating,\" Llion remarked during a reunion dinner with the original team in 2019, \"is how different teams took our architecture in different directions. Some focused on the encoder, others on the decoder, and some used the full model.\"</p> <p>The scaling era had begun. GPT-2, released in 2019, showed that scaling up transformer models led to surprisingly capable text generation. GPT-3 in 2020, with 175 billion parameters, demonstrated emergent abilities that nobody had predicted.</p> <p>By 2022, transformer-based Large Language Models (LLMs) like PaLM, LaMDA, and later GPT-4 and Claude were showing capabilities that approached human-level performance on many tasks.</p> <p>\"Did you imagine this would happen when we were working late nights back in 2016?\" \u0141ukasz asked Ashish.</p> <p>\"Never,\" he admitted. \"We thought we had a better architecture for machine translation. We never imagined it would lead to models that could write essays, code programs, and have conversations.\"</p>"},{"location":"stories/transformers/#chapter-10-the-attention-legacy","title":"Chapter 10: The Attention Legacy","text":"Show Image PromptCreate a brightly colored wide-landscape drawing as it would appear in a graphic novel.      Panel 10: A futuristic scene showing applications of transformer technology in everyday life. The panel is divided into multiple areas showing diverse uses: medical diagnosis, creative writing, scientific discovery, education, and more. In the center, the original attention equation floats like a unifying principle connecting everything. The eight original researchers are shown in the foreground, older now, looking at this landscape with expressions of pride and amazement. The style combines realism with conceptual elements, and the color palette is bright and optimistic, suggesting a positive technological future built on their foundation.  <p>By 2025, the Transformer architecture had transformed not just natural language processing but artificial intelligence as a whole. The original eight authors found themselves cited in tens of thousands of papers, their work forming the foundation for an entirely new generation of AI systems.</p> <p>\"Attention is all you need,\" Jakob reflected, \"turned out to be prophetic in ways we never anticipated.\"</p> <p>Their paper, which almost wasn't published due to competitive concerns, had accelerated AI progress by years or possibly decades by being openly shared with the research community.</p> <p>The attention mechanism they pioneered provided a new way of thinking about how neural networks could process information \u2013 not just sequentially or locally, but with direct connections between any elements, regardless of distance.</p> <p>This insight extended beyond language to computer vision, reinforcement learning, protein folding, and numerous other domains. The ability to model relationships between arbitrary elements of a dataset proved universally valuable.</p> <p>\"I think what I'm most proud of,\" Ashish said during a retrospective talk in 2025, \"is not just the paper itself, but how the research community built upon it. Science progresses when ideas are shared and improved upon collectively.\"</p> <p>The legacy of \"Attention Is All You Need\" wasn't just a more efficient architecture \u2013 it was a reminder that sometimes, the most profound innovations come from questioning fundamental assumptions and pursuing elegant simplicity.</p> <p>In the end, attention wasn't all they needed \u2013 they also needed courage to challenge the status quo, persistence to overcome technical challenges, and openness to share their discoveries with the world. But attention was the key that unlocked a new era of artificial intelligence.</p>"},{"location":"stories/transformers/#appendix-understanding-attention-a-visual-explanation","title":"Appendix: Understanding Attention: A Visual Explanation**","text":"Show Image Prompt   Panel 11:  Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A detailed diagram showing the sentence \"The chicken did not cross the road because it was too [tired/wide].\" The word \"it\" is highlighted and connects to other words with lines of varying thickness. Two versions are shown side by side - in one, \"it\" connects strongly to \"chicken\" (when \"tired\" is used), and in the other, \"it\" connects strongly to \"road\" (when \"wide\" is used). Arrows and zoomed-in sections show how the attention mechanism calculates scores between words. Mathematical formulas for calculating attention weights appear in simplified, visually appealing formats around the edges. The color scheme matches the main story's palette.  <p>The key insight of the attention mechanism can be understood through our example sentence:</p> <p>\"The chicken did not cross the road because it was too tired.\"</p> <ol> <li> <p>The Problem: What does \"it\" refer to? Is it the chicken or the road?</p> </li> <li> <p>How Attention Solves It: The model calculates \"compatibility scores\" between each word and every other word:</p> </li> <li>it \u2192 chicken: Strong score (0.85)</li> <li>it \u2192 road: Weaker score (0.12)</li> <li>it \u2192 cross: Very weak score (0.03)</li> <li> <p>And so on...</p> </li> <li> <p>Context Matters: If we change the sentence to \"The chicken did not cross the road because it was too wide,\" the attention pattern shifts:</p> </li> <li>it \u2192 chicken: Weaker score (0.14)</li> <li>it \u2192 road: Strong score (0.81)</li> <li> <p>it \u2192 cross: Very weak score (0.05)</p> </li> <li> <p>How It's Calculated: Each word is converted into three vectors:</p> </li> <li>A Query vector (what information am I looking for?)</li> <li>A Key vector (what information do I contain?)</li> <li> <p>A Value vector (what information do I pass along?)</p> </li> <li> <p>The Score: Attention score between words is calculated as the dot product of the Query of one word with the Keys of all other words.</p> </li> <li> <p>The Result: These scores determine how much each word's Value impacts the representation of the current word.</p> </li> </ol> <p>This simple mechanism allows transformers to handle references, understand context, and capture long-range dependencies in text, which proved revolutionary for language understanding and generation.</p>"},{"location":"stories/transformers/#formulas-and-equations","title":"Formulas and Equations","text":""},{"location":"stories/transformers/#attention-formula-scaled-dot-product-attention","title":"Attention Formula (Scaled Dot-Product Attention)","text":"\\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V  \\] <p>where:</p> <ul> <li>\\(Q\\) is the query matrix</li> <li>\\(K\\) is the key matrix</li> <li>\\(K^T\\) is the transpose of the key matrix</li> <li>\\(V\\) is the value matrix</li> <li>\\(d_k\\) is the dimension of the keys (used for scaling)</li> <li>\\(\\text{softmax}\\) is the softmax function applied row-wise</li> </ul>"},{"location":"stories/transformers/#multi-head-attention","title":"Multi-Head Attention","text":"\\[ MultiHead(Q,K,V)\\=Concat(head1,...,headh)WO\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O MultiHead(Q,K,V)\\=Concat(head1\u200b,...,headh\u200b)WO \\]"},{"location":"stories/transformers/#multi-head-attention_1","title":"Multi-Head Attention","text":"<p>Attention Formula (Scaled Dot-Product Attention)</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] <p>where:</p> <ul> <li>\\(Q\\) is the query matrix</li> <li>\\(K\\) is the key matrix</li> <li>\\(K^T\\) is the transpose of the key matrix</li> <li>\\(V\\) is the value matrix</li> <li>\\(d_k\\) is the dimension of the keys (used for scaling)</li> <li>\\text{softmax} is the softmax function applied row-wise</li> </ul>"},{"location":"stories/transformers/#multi-head-attention_2","title":"Multi-Head Attention","text":"\\[ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\] <p>where each head is computed as:</p> \\[ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\] <p>and \\(QW_i^Q\\), \\(KW_i^K\\), \\(VW_i^V\\) are parameter matrices for each attention head</p> <ul> <li>\\(W^O\\) is the output projection matrix</li> <li>\\(h\\) is the number of attention heads</li> </ul> <p>The softmax function itself is defined as:</p> \\[ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\] <p>This formula is the core of the transformer architecture described in the \"Attention Is All You Need\" paper, allowing the model to weigh the importance of different words in relation to each other.</p>"}]}