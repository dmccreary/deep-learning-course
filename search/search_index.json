{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Deep Learning with AI","text":"<p>This website contains resources for teaching a college-level course on Deep Learning with AI.  We assume that both instructors and students have access to generative AI tools such as ChatGPT, Anthropic Claude or Ollama.</p> <p>Please contact me on LinkedIn if you have any questions about the course.</p>"},{"location":"about/","title":"About this Website","text":"<p>This website contains resources for teaching a college-level course on Deep Learning.  We assume that both instructors and students have access to generative AI tools such as ChatGPT, Anthropic Claude or Ollama.  Understanding prompt engineering is key to generating high quality Python code, test cases and acceptance test plans.</p>"},{"location":"about/#why-this-site-why-now","title":"Why This Site?  Why Now?","text":"<p>Deep learning has become an indispensable pillar of modern computer science and electrical engineering, fundamentally transforming how we approach complex problems across virtually every technical domain. For CS and EE students, understanding deep learning is no longer optional \u2013 it's a critical skill that will shape their future careers and their ability to innovate in their fields.</p> <p>Consider how deep learning has revolutionized traditional EE and CS domains: In electrical engineering, deep learning has dramatically improved signal processing, circuit design optimization, and power systems management. EE students who understand deep learning can develop more efficient hardware architectures, design better communication systems, and create more sophisticated embedded systems that can adapt to real-world conditions.</p> <p>For computer science students, deep learning has become essential for solving complex problems in computer vision, natural language processing, and autonomous systems. The ability to design and implement neural networks is now as fundamental as understanding data structures and algorithms.</p> <p>This intelligent textbook is an excellent resource for instructors, as it presents a comprehensive taxonomy of deep learning concepts. Based on the structure I can see in the data (with ConceptIDs, concept labels, and dependencies), it seems to offer a well-organized progression through the material that helps students build their knowledge systematically.</p> <p>What makes deep learning particularly crucial for today's students is its rapid evolution and widespread adoption across industries. From healthcare to autonomous vehicles, from financial systems to renewable energy optimization \u2013 virtually every sector now leverages deep learning. Students who graduate without this knowledge risk falling behind in an increasingly AI-driven job market.</p> <p>Moreover, deep learning represents a fundamental shift in how we approach problem-solving in computing. Rather than explicitly programming solutions, students learn to design systems that can learn from data \u2013 a paradigm shift that will only become more important as the complexity of our technical challenges grows.</p>"},{"location":"checklist/","title":"Site Checklist","text":"<ol> <li>Customize the fields in your mkdocs.yml file - Done</li> <li>Configure Google Analytics to use the right site ID</li> <li>Make sure that your .gitignore file includes the <code>site</code> directory, the .DS_Store and the ~$*</li> <li>Test the build with <code>mkdocs build</code></li> <li>Make sure the Edit button appears</li> <li>Make sure that code color heightening renders correctly</li> <li>run <code>git config advice.addIgnoredFile false</code></li> <li>Generate a Bloom 2001 Taxonomy normalized course description</li> <li>Generate a list of concepts</li> <li>Generate concept dependencies</li> <li>Generate a 12 element taxonomy</li> <li>Generate a Glossary of Terms</li> </ol>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Course Description","text":"<p>Organization: University of Minnesota Department of Electrical and Computer Engineering (ECE) Course Identifier: EE 4940 Sequence Name: Special Topics in Electrical and Computer Engineering (1-4 Cr.) Offering Course Title: : Introduction to Deep Learning Offering Date: Spring 2025</p>"},{"location":"course-description/#course-instructor-ta-and-format","title":"Course Instructor, TA and Format","text":"<p>Sang-Hyun Oh, Professor of ECE, graduate faculty in neuroscience, physics, biomedical eng. Email: sang@umn.edu Office: 5-119 Keller Hall Teaching Assistant: Ike Goetzman (PhD candidate in ECE; email: goetz219@umn.edu) Format: Lecture (Jan 22 \u2013 May 5, 2024): Monday/Wednesday 11:15am-12:30pm @ Appleby Hall 3</p>"},{"location":"course-description/#course-summary","title":"Course Summary","text":"<p>Machine learning, especially deep learning based on neural networks, has shown transformative impacts across life sciences, engineering, and medicine, as recently highlighted by the 2024 Nobel Prizes: </p> <ul> <li>The Physics Nobel was awarded to John Hopfield and Geoffrey Hinton for pioneering contributions to neural networks that enable machine learning</li> <li>Demis Hassabis, John Jumper, and David Baker received the Chemistry Nobel for advancing computational protein design.</li> </ul>"},{"location":"course-description/#course-overview","title":"Course Overview","text":"<p>EE 4940: Introduction to Deep Learning aims to provide students with both fundamental and applied perspectives on these advances. Beginning with an overview of AI, the course introduces deep neural networks (DNNs) through practical, hands-on examples and guides students through specialized neural network architectures, including</p> <ul> <li>convolutional neural networks (CNNs) for diagnostic applications and </li> <li>recurrent neural networks (RNNs) for time series analysis.</li> </ul> <p>Emphasis is placed on applying neural networks to tackle real-world problems in classification, regression, image analysis, and time-series prediction. Scientific applications are demonstrated as well, such as using DNNs to solve equations.</p> <p>The course concludes with dimensionality reduction techniques, followed by advanced topics such as generative AI, and an introduction to reinforcement learning. </p> <p>Emphasis will be on practical experience with Python-based coding over mathematical theory.</p> <p>By the end of the course, students will have gained familiarity with Python-driven deep learning, hands-on skills, and a working knowledge of deep learning\u2019s potential for a wide range of problems.</p>"},{"location":"course-description/#course-prerequisites","title":"Course Prerequisites","text":"<p>EE 4940 - Special Topics in Electrical and Computer Engineering is recommended but not required.</p> <p>This course is designed to be accessible and introductory, with essential concepts and coding skills introduced as needed.</p>"},{"location":"course-description/#expected-workload","title":"Expected Workload","text":"<p>This course (EE 4940) is a 3-credit, lecture format course. The expected workload for this course this semester is, on average, 45 hours per credit, or just over 3 hours per credit hour per week (including lecture). The total workload of 9 hours per week.</p>"},{"location":"course-description/#course-goals-and-objectives","title":"Course Goals and Objectives","text":"<p>EE 4940 aims to equip students with a foundation in deep learning, focusing on core architectures like deep neural networks (DNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).</p> <p>Through hands-on applications, students will learn to apply these models to solve practical problems using real-world datasets, such as cancer diagnostics and time-series analysis. The primary goal of the course is to develop students's ability to implement and troubleshoot neural network models, gaining some practical experiences, and be aware of potential pitfalls/artifacts while using DNNs.</p> <p>By the course's end, students will have a working knowledge of AI\u2019s role in scientific problem-solving, dimensionality reduction techniques, and a simple introduction to generative models and reinforcement learning (RL).</p>"},{"location":"course-description/#topics-to-be-covered","title":"Topics To Be Covered","text":""},{"location":"course-description/#1-introduction-to-ai-and-deep-learning","title":"1. Introduction to AI and deep learning","text":"<ul> <li>Brief history of AI, neural networks and recent Nobel-winning breakthroughs in physics and chemistry</li> <li>Course setup, tools, and brief introduction to Python for deep learning</li> <li>The universality theorem and the manifold hypothesis</li> </ul>"},{"location":"course-description/#2-fundamentals-of-deep-neural-networks-dnns","title":"2. Fundamentals of Deep Neural Networks (DNNs)","text":"<ul> <li>Architecture of DNNs: layers, activation functions, and backpropagation</li> <li>Training, validation, and testing sets;</li> <li>Loss functions, gradient descent, mini-batches</li> <li>Hands-on coding</li> </ul>"},{"location":"course-description/#3-convolutional-neural-networks-cnns-applications-in-diagnostic-imaging","title":"3. Convolutional Neural Networks (CNNs) - applications in diagnostic imaging","text":"<ul> <li>CNN architecture</li> <li>Applying CNNs, with MHIST as a case study for cancer diagnosis</li> </ul>"},{"location":"course-description/#4-recurrent-neural-networks-rnns-and-time-series-analysis","title":"4. Recurrent Neural Networks (RNNs) and time series analysis","text":"<ul> <li>Introduction to RNNs, LSTMs, for sequency data analysis.</li> <li>Hands-on practice with real-world data.</li> </ul>"},{"location":"course-description/#5-chips-and-ai","title":"5. Chips and AI","text":"<p>Briefly explore AI chips, neuromorphic devices, and future directions.</p>"},{"location":"course-description/#6-ai-for-scientific-problem-solving-using-neural-networks-to-approximate-solutions-for","title":"6. AI for scientific problem solving: Using neural networks to approximate solutions for","text":"<p>Maxwell\u2019s equations</p>"},{"location":"course-description/#7-dimensionality-reduction-and-autoencoders","title":"7. Dimensionality reduction and autoencoders","text":""},{"location":"course-description/#8-advanced-topics","title":"8. Advanced topics","text":""},{"location":"course-description/#course-format","title":"Course Format","text":"<ul> <li>Classroom lectures only</li> <li>Canvas LMS link will be provided</li> <li>Textbook and Required Materials: none</li> <li>Based on lecture notes (uploaded in canvas 1-2 days before class)</li> </ul>"},{"location":"course-description/#homework-assignments-and-mini-projects","title":"Homework Assignments and \u201cMini-projects\u201d:","text":"<ul> <li>Regularly-scheduled weekly or biweekly homework assignments and 4 multi-week \u201cmini-projects.\u201d The former will be designed to exercise basic concepts and modest extensions of the technical or computational tools introduced in class; the latter will be crafted to allow students to take a \u201cdeeper dive\u201d by applying techniques developed in the course to data sets arising in various aspects of ECE and life sciences.</li> <li>You are allowed (in fact, encouraged) to work with others when solving the homework and mini project problems. However, solutions submitted for grading should be formulated by each student independently! Late homework and mini-project submissions will be accepted, but without a legitimate (e.g., illness-based) justification, will result in a maximum grade of 80% for that assignment. Specifically, unjustified late assignments will be graded according to the same rubric as on-time assignments, but the corresponding score will be multiplied by 0.8 when entered into the gradebook.</li> </ul>"},{"location":"course-description/#exams","title":"Exams","text":"<p>There will be one midterm exam and one final exam. The midterm exam will be open book/notes. The final exam will be open book/notes and will be due no earlier than the final exam time specified by the Registrar. Either or both may be take-home exams.</p>"},{"location":"course-description/#grading","title":"Grading","text":"<p>The grade distributions for the course variations are provided below.</p> <p>Note: distributions for homework assignments and mini-projects are total; individual submissions will be equally weighted.</p> Deliverable Weight Homework Assignments 40% Mini-projects 40% Midterm Exam 10% Final Exam 10%"},{"location":"course-description/#software-requirements","title":"Software Requirements","text":"<p>Through the homework assignments and mini-projects, students will be required to perform a number of computer exercises to become familiar with the computational and practical aspects of deep learning. This will typically entail utilization of Python, often in concert with existing toolboxes such as NumPy, SciPy, Matplotlib, Scikit-learn, and PyTorch (or TensorFlow), all of which are freely available for Windows, Mac, and Linux operating systems.</p> <p>As a perhaps preferred alternative, students may wish to perform all (Python) programming computations using a free cloud-based account (i.e., Google Colab), which will be discussed and established at the beginning of the semester. Large-scale deep learning computations necessary for the final \u201cmini-project\u201d will utilize these resources.</p> <p>Note</p> <p>The Required University Policy Statements were provided in the original PDF of the course description.  They have been removed to allow the generative AI tools to focus on the concept enumeration.</p>"},{"location":"feedback/","title":"Feedback on Graph Data Modeling","text":"<p>You are welcome to connect with me on anytime on LinkedIn or submit any issues to GitHub Issue Log.  All pull-requests with fixes to errors or additions are always welcome.</p> <p>If you would like to fill out a short survey and give us ideas on how we can create better tools for intelligent textbooks in the future.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":"<p>Below is an alphabetical glossary of 200 key concepts, each defined according to ISO 11179 guidelines. When relevant, an example of how the term is used in a Deep Learning context is provided.  A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"glossary/#accuracy","title":"Accuracy","text":"<p>A performance metric that measures the proportion of correct predictions out of all predictions made by a model. Example: In a binary classification problem, accuracy is the ratio of true positives and true negatives to the total sample size.</p>"},{"location":"glossary/#activation-functions","title":"Activation Functions","text":"<p>Mathematical functions applied to neuron outputs that introduce non-linearity into a neural network. Example: ReLU and Sigmoid are common activation functions used to help networks learn complex patterns.</p>"},{"location":"glossary/#actor-critic","title":"Actor-Critic","text":"<p>A class of reinforcement learning algorithms that maintain both a policy (actor) and a value function (critic) to guide action selection. Example: The actor-critic approach is used to stabilize training by combining policy gradient methods with value-based techniques.</p>"},{"location":"glossary/#adagrad","title":"AdaGrad","text":"<p>An adaptive gradient algorithm that individually scales learning rates based on the historical gradient magnitudes for each parameter. Example: AdaGrad can help train models more efficiently in scenarios where some parameters require more frequent updates than others.</p>"},{"location":"glossary/#adam-optimizer","title":"Adam Optimizer","text":"<p>An optimization algorithm that combines ideas from momentum (exponential moving averages of gradients) and AdaGrad (adaptive learning rates). Example: Adam is often the default choice for training deep neural networks due to its speed and stable convergence properties.</p>"},{"location":"glossary/#adversarial-examples","title":"Adversarial Examples","text":"<p>Inputs to a model that have been intentionally crafted to cause incorrect predictions, despite appearing normal to humans. Example: Slightly perturbed images that force a well-trained image classifier to misclassify a stop sign as a yield sign.</p>"},{"location":"glossary/#agile-methods","title":"Agile Methods","text":"<p>Project management practices emphasizing iterative development, collaboration, and adaptability. Example: In a machine learning project, sprints are used to rapidly prototype models and incorporate stakeholder feedback.</p>"},{"location":"glossary/#ai-branches","title":"AI Branches","text":"<p>Various subfields of Artificial Intelligence, including symbolic AI, data-driven AI, evolutionary computation, and others. Example: A course might compare symbolic reasoning systems with modern deep learning architectures.</p>"},{"location":"glossary/#ai-definition","title":"AI Definition","text":"<p>The study and development of machines capable of performing tasks that typically require human intelligence. Example: AI encompasses problem-solving, perception, language understanding, and learning processes.</p>"},{"location":"glossary/#attention","title":"Attention","text":"<p>The area of a subject that has a high weight when classifying the subject.</p> <p>The paper \"Attention is All You Need\" was the name of the landmark paper on Transformers.</p>"},{"location":"glossary/#automl","title":"AutoML","text":"<p>Automated Machine Learning techniques that handle tasks such as model selection, hyperparameter tuning, and feature engineering with minimal human intervention. Example: Tools like Google AutoML or AutoKeras can automatically try multiple pipelines and present the best-performing model.</p>"},{"location":"glossary/#autoencoder-dr","title":"Autoencoder DR","text":"<p>Dimensionality reduction technique that uses an autoencoder neural network to learn a compressed representation of data. Example: An autoencoder might reduce a 1000-dimensional dataset down to 50 dimensions for visualization.</p>"},{"location":"glossary/#autoencoders-ae","title":"Autoencoders (AE)","text":"<p>Neural network architectures designed to learn a compressed representation (encoding) of input data, then reconstruct it (decoding). Example: AEs are used to denoise images or perform feature extraction for anomaly detection.</p>"},{"location":"glossary/#autonomous-driving","title":"Autonomous Driving","text":"<p>The application of AI/ML methods to enable vehicles to sense their environment and operate without human intervention. Example: Deep neural networks can process camera and lidar data in real-time for self-driving car navigation.</p>"},{"location":"glossary/#auc-roc","title":"AUC-ROC","text":"<p>A performance metric summarizing the trade-off between true positive rate and false positive rate across different thresholds. Example: The area under the ROC curve (AUC) is often used to compare classifiers independent of a specific decision threshold.</p>"},{"location":"glossary/#backprop-through-time-bptt","title":"Backprop Through Time (BPTT)","text":"<p>A method for training recurrent neural networks by unrolling the network for a certain number of timesteps and applying backpropagation. Example: LSTM networks rely on BPTT to learn dependencies in sequential data such as text or time series.</p>"},{"location":"glossary/#backpropagation","title":"Backpropagation","text":"<p>The algorithm used to calculate gradients of error with respect to each weight in a neural network, enabling efficient training. Example: During backprop, partial derivatives of the loss function are propagated backward to update model parameters.</p>"},{"location":"glossary/#baseline-algorithms","title":"Baseline Algorithms","text":"<p>Simple or well-understood methods used as reference points for performance comparison. Example: A linear regression might serve as a baseline for a more complex deep model on a regression task.</p>"},{"location":"glossary/#batch-normalization","title":"Batch Normalization","text":"<p>A technique to normalize intermediate layer outputs to accelerate training and stabilize learning in neural networks. Example: By normalizing activations, batch norm helps reduce internal covariate shift.</p>"},{"location":"glossary/#batch-training","title":"Batch Training","text":"<p>A training paradigm where the model parameters are updated after processing an entire batch of training examples. Example: Large datasets are often split into batches to efficiently utilize GPU memory.</p>"},{"location":"glossary/#batchnorm","title":"BatchNorm","text":"<p>Another name for batch normalization, focusing on normalizing activations per mini-batch. Example: \u201cBatchNorm\u201d layers are common in architectures like ResNet and Inception.</p>"},{"location":"glossary/#bayesian-optimization","title":"Bayesian Optimization","text":"<p>A global optimization strategy using Bayesian inference to intelligently sample hyperparameters and find optimal configurations. Example: Given a function that\u2019s expensive to evaluate (e.g., a deep network training loop), Bayesian optimization tries to minimize the total evaluations required.</p>"},{"location":"glossary/#bert-overview","title":"BERT Overview","text":"<p>Description of Bidirectional Encoder Representations from Transformers, a popular transformer-based NLP model that learns context in both directions. Example: BERT is used to improve tasks like question-answering and sentence classification.</p>"},{"location":"glossary/#bias-mitigation","title":"Bias Mitigation","text":"<p>Techniques to reduce or remove unwanted bias in AI models, ensuring fair and equitable outcomes. Example: Reweighting training data to address underrepresentation of certain groups.</p>"},{"location":"glossary/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>The balance between the accuracy of a model (bias) and its sensitivity to small fluctuations in the training set (variance). Example: A highly complex model might overfit (low bias, high variance), while a simple model might underfit (high bias, low variance).</p>"},{"location":"glossary/#biological-inspiration","title":"Biological Inspiration","text":"<p>The design of AI techniques based on biological systems or processes, such as neural networks modeled after the human brain. Example: Convolutional neural networks are partially inspired by the visual cortex in mammals.</p>"},{"location":"glossary/#bleu-score","title":"BLEU Score","text":"<p>Bilingual Evaluation Understudy; a metric used to evaluate the quality of machine-translated text against reference translations. Example: BLEU is commonly reported in NLP tasks such as neural machine translation experiments.</p>"},{"location":"glossary/#capsule-networks","title":"Capsule Networks","text":"<p>Neural architectures that group neurons into \u201ccapsules\u201d to better capture positional and hierarchical relationships in data. Example: Capsule networks aim to preserve spatial relationships more effectively than traditional CNNs.</p>"},{"location":"glossary/#chatgpt-concept","title":"ChatGPT Concept","text":"<p>A high-level idea of OpenAI\u2019s ChatGPT, which uses large language models to generate context-aware text responses. Example: Students explore ChatGPT\u2019s approach to answer generation and how it leverages conversational context.</p>"},{"location":"glossary/#cloud-services","title":"Cloud Services","text":"<p>Remote computing resources (servers, storage, APIs) for hosting, training, and deploying AI models. Example: AWS, Azure, and Google Cloud offer GPU/TPU instances to accelerate model training.</p>"},{"location":"glossary/#cnn-architectures","title":"CNN Architectures","text":"<p>Convolutional Neural Network designs specialized for handling spatial data such as images. Example: Classic architectures include LeNet, AlexNet, VGG, ResNet, and DenseNet.</p>"},{"location":"glossary/#code-review","title":"Code Review","text":"<p>The process of examining and discussing code changes to improve code quality and share knowledge within a team. Example: Before merging new data preprocessing scripts, peers review for correctness and maintainability.</p>"},{"location":"glossary/#computer-vision","title":"Computer Vision","text":"<p>A field of AI focusing on enabling machines to interpret and understand visual information from the world. Example: Tasks include image classification, object detection, and image segmentation.</p>"},{"location":"glossary/#conditional-gan","title":"Conditional GAN","text":"<p>A GAN variant where both the generator and discriminator receive additional context information, such as class labels. Example: Conditional GANs can generate images conditioned on textual descriptions like \u201ccats\u201d or \u201cdogs.\u201d</p>"},{"location":"glossary/#confusion-matrix","title":"Confusion Matrix","text":"<p>A tabular layout displaying the number of correct and incorrect predictions grouped by class. Example: Rows typically represent actual classes, while columns represent predicted classes, making misclassifications visible.</p>"},{"location":"glossary/#conda-environment","title":"Conda Environment","text":"<p>A virtual environment manager provided by Anaconda, allowing users to isolate Python packages and dependencies. Example: Students create a separate conda environment to manage a specific ML project\u2019s libraries without interfering with system Python.</p>"},{"location":"glossary/#convolution-operation","title":"Convolution Operation","text":"<p>A mathematical operation where a filter (kernel) is applied to input data (e.g., an image) to detect features. Example: In CNNs, convolution layers learn filters that activate in response to relevant patterns.</p>"},{"location":"glossary/#cross-validation","title":"Cross-Validation","text":"<p>A technique for evaluating model performance by partitioning data into multiple folds, cycling through training and validation sets. Example: k-fold cross-validation helps assess how well the model generalizes to unseen data.</p>"},{"location":"glossary/#cyclegan","title":"CycleGAN","text":"<p>A GAN-based architecture for unpaired image-to-image translation. Example: CycleGAN can transform horse images into zebra images without needing paired training examples.</p>"},{"location":"glossary/#data-acquisition","title":"Data Acquisition","text":"<p>The process of gathering relevant datasets from various sources, such as APIs, sensors, or public repositories. Example: In a project, data acquisition might involve scraping websites or querying a medical imaging database.</p>"},{"location":"glossary/#data-annotation","title":"Data Annotation","text":"<p>Labeling or tagging raw data (images, text, audio) with meaningful information to facilitate supervised learning. Example: Crowdworkers often annotate bounding boxes on images to prepare object detection datasets.</p>"},{"location":"glossary/#data-augmentation","title":"Data Augmentation","text":"<p>Techniques used to increase the diversity of a training set by applying transformations (e.g., flips, rotations) to existing samples. Example: Randomly flipping or cropping images to help CNNs generalize better.</p>"},{"location":"glossary/#data-cleaning","title":"Data Cleaning","text":"<p>The process of identifying and correcting incomplete, inaccurate, or irrelevant parts of a dataset. Example: Removing duplicates or fixing inconsistent labels in a CSV file before model training.</p>"},{"location":"glossary/#data-driven-ai","title":"Data-Driven AI","text":"<p>AI methodologies that rely heavily on large datasets and statistical learning rather than explicit symbolic rules. Example: Deep learning is a classic form of data-driven AI, requiring abundant labeled data for training.</p>"},{"location":"glossary/#data-ethics","title":"Data Ethics","text":"<p>The standards and principles guiding the responsible collection, storage, and use of data. Example: Ensuring personal data is handled with user consent and compliance with legal frameworks such as GDPR.</p>"},{"location":"glossary/#data-imbalance","title":"Data Imbalance","text":"<p>A condition where classes or categories in a dataset are not equally represented. Example: In fraud detection, \u201cfraud\u201d cases are often far fewer than \u201cnon-fraud\u201d cases, impacting model training.</p>"},{"location":"glossary/#data-parallelism","title":"Data Parallelism","text":"<p>Distributing subsets of the input data across multiple processors or machines to train a model faster. Example: Large mini-batches are split across GPUs, and gradients are aggregated at each step.</p>"},{"location":"glossary/#data-preprocessing","title":"Data Preprocessing","text":"<p>Preparatory steps taken on raw data, such as cleaning, normalization, or encoding. Example: Converting categorical variables into one-hot vectors or normalizing images to zero mean and unit variance.</p>"},{"location":"glossary/#data-profile","title":"Data Profile","text":"<p>A summary of key statistical characteristics and quality metrics of a dataset, including distributions, missing values, cardinality, and data types for each feature.</p> <p>Example: An MNIST data profile would reveal that each image is 28x28 pixels (784 features), pixel values range from 0-255, there are 10 distinct label classes with roughly equal distribution (~6000 samples each), and the dataset contains no missing values.  See </p>"},{"location":"glossary/#data-splits","title":"Data Splits","text":"<p>Partitioning data into subsets (e.g., training, validation, test) for model development and evaluation. Example: 80% of the data for training, 10% for validation, 10% for testing.</p>"},{"location":"glossary/#data-visualization","title":"Data Visualization","text":"<p>Graphical representation of data to reveal patterns, trends, or outliers. Example: Creating scatter plots in Matplotlib to explore relationships between features.</p>"},{"location":"glossary/#data-visualization-tools","title":"Data Visualization Tools","text":"<p>Software libraries and platforms that facilitate the creation of charts, graphs, and interactive plots. Example: Matplotlib, Seaborn, and Plotly are used in notebooks to illustrate key data insights.</p>"},{"location":"glossary/#deep-learning","title":"Deep Learning","text":"<p>Neural network designs featuring multiple hidden layers, enabling hierarchical feature extraction.</p> <p>Deep Learning is a subset of machine learning that focus on complex models.  Deep learning is used for tasks such as image classification, object detection, speech recognition, natural language processing and other problems that can't be solved by simpler models.</p> <p>Example: A typical deep learning system might have a dozen or more layers in a CNN.</p>"},{"location":"glossary/#deep-q-network-dqn","title":"Deep Q-Network (DQN)","text":"<p>A reinforcement learning approach where a deep neural network approximates the Q-function for action selection. Example: DQN was famously applied by DeepMind to play Atari games at a superhuman level.</p>"},{"location":"glossary/#deepseek","title":"Deepseek","text":"<p>An open-source large-langauge model with reasoning announced in 2025 that has superior performance.</p> <p>The Deepseek-R1 7B model is used on a local GPU for testing the tokens-per-second lab in this course.</p>"},{"location":"glossary/#densenet","title":"DenseNet","text":"<p>A CNN architecture where each layer is connected to every other layer in a feed-forward manner. Example: DenseNet alleviates the vanishing gradient problem by encouraging feature reuse.</p>"},{"location":"glossary/#depth-vs-width","title":"Depth Vs Width","text":"<p>The tradeoff between making a network deeper (more layers) or wider (more neurons per layer). Example: Deeper networks can learn more abstract features, whereas wider networks can capture more detail at each level.</p>"},{"location":"glossary/#denoising-diffusion","title":"Denoising Diffusion","text":"<p>A class of generative models where noise is incrementally added to data, then learned in reverse to generate samples. Example: Used in image generation tasks to progressively refine noise into realistic images.</p>"},{"location":"glossary/#diffusion-models","title":"Diffusion Models","text":"<p>Probabilistic models that learn to reverse a gradual noising process to produce new samples. Example: DALL\u00b7E 2 incorporates diffusion methods for high-quality image synthesis.</p>"},{"location":"glossary/#dimensionality-concepts","title":"Dimensionality Concepts","text":"<p>Principles and considerations related to the number of features or variables in a dataset (dimensionality). Example: High-dimensional data can be more difficult to visualize and may require techniques like PCA.</p>"},{"location":"glossary/#documentation","title":"Documentation","text":"<p>Written guides, explanations, or references describing software or processes. Example: Thorough documentation ensures that future team members can understand the codebase and its usage.</p>"},{"location":"glossary/#drug-discovery","title":"Drug Discovery","text":"<p>The application of AI to identify potential new medications by analyzing large chemical and biological datasets. Example: ML models predict molecular binding affinities to reduce trial-and-error in drug design.</p>"},{"location":"glossary/#dropout","title":"Dropout","text":"<p>A regularization method that randomly \u201cdrops\u201d or sets some neurons to zero during training to reduce overfitting. Example: Setting a 50% dropout rate in a dense layer to encourage robust feature learning.</p>"},{"location":"glossary/#early-stopping","title":"Early Stopping","text":"<p>A regularization technique that halts training when validation performance stops improving. Example: Prevents overfitting by not allowing the model to train excessively on one dataset.</p>"},{"location":"glossary/#edge-deployment","title":"Edge Deployment","text":"<p>Running AI models directly on edge devices (smartphones, IoT devices) rather than on centralized servers. Example: A small CNN for object recognition deployed on a mobile phone for real-time inference.</p>"},{"location":"glossary/#energy-based-models","title":"Energy-Based Models","text":"<p>A class of probabilistic models that define an energy function over configurations, and learning involves shaping this energy landscape. Example: Boltzmann machines are one type of energy-based model used for representation learning.</p>"},{"location":"glossary/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Quantitative measures to assess model performance, guiding model selection and tuning. Example: Accuracy, precision, recall, and F1 score are common metrics for classification tasks.</p>"},{"location":"glossary/#explainable-ai-xai","title":"Explainable AI (XAI)","text":"<p>Methods designed to make AI system decisions interpretable by humans. Example: Feature attribution maps that highlight which parts of an image influenced a CNN\u2019s classification.</p>"},{"location":"glossary/#exploration-vs-exploitation","title":"Exploration Vs Exploitation","text":"<p>The balance in reinforcement learning between trying new actions (exploration) and using known rewarding actions (exploitation). Example: An RL agent might explore different states early on, then exploit the best actions discovered.</p>"},{"location":"glossary/#exploding-gradient-control","title":"Exploding Gradient Control","text":"<p>Techniques to prevent gradients from becoming excessively large during backpropagation. Example: Gradient clipping is a common way to avoid unstable updates in RNN training.</p>"},{"location":"glossary/#exploding-gradients","title":"Exploding Gradients","text":"<p>A situation where gradients grow uncontrollably during training, causing large parameter updates and potential instability. Example: Deep RNNs may experience exploding gradients unless special measures (like clipping) are applied.</p>"},{"location":"glossary/#f1-score","title":"F1 Score","text":"<p>The harmonic mean of precision and recall, providing a single measure of a test\u2019s accuracy. Example: Used when you care about both false positives and false negatives in an imbalanced classification.</p>"},{"location":"glossary/#fairness","title":"Fairness","text":"<p>The principle of designing AI systems to avoid discriminatory outcomes or biased treatment. Example: Ensuring loan approval models do not systematically disadvantage applicants from certain demographics.</p>"},{"location":"glossary/#feature-engineering","title":"Feature Engineering","text":"<p>The process of creating or transforming input features to improve model performance. Example: Combining multiple text columns into a single normalized \u201cbag of words\u201d vector.</p>"},{"location":"glossary/#feature-space","title":"Feature Space","text":"<p>The multidimensional space where each dimension corresponds to a feature of the data. Example: Visualizing points in feature space helps understand how the model separates classes.</p>"},{"location":"glossary/#filters-and-kernels","title":"Filters And Kernels","text":"<p>Learnable convolution operators that detect features in images or other spatial data. Example: Early layers might learn edge detectors, while deeper layers capture more complex patterns.</p>"},{"location":"glossary/#financial-forecasting","title":"Financial Forecasting","text":"<p>Using AI/ML to predict market movements, asset prices, or economic indicators. Example: LSTM networks analyzing historical stock data for next-day price predictions.</p>"},{"location":"glossary/#flow-based-models","title":"Flow-Based Models","text":"<p>Generative models that transform noise into data samples via a series of invertible transformations. Example: RealNVP or Glow architectures produce exact likelihood estimates and allow sampling.</p>"},{"location":"glossary/#forward-propagation","title":"Forward Propagation","text":"<p>The process of passing input data through a neural network to get an output prediction. Example: In a feedforward network, data flows from the input layer through hidden layers to the output.</p>"},{"location":"glossary/#frozen-in-time","title":"Frozen in Time","text":"<p>The term used to describe that the knowledge base that a LLM is trained on has a cutoff date which can often omit current events in the past year.</p>"},{"location":"glossary/#fully-connected-layers","title":"Fully Connected Layers","text":"<p>Layers where every neuron is connected to every neuron in the next layer, typically appearing after convolution blocks. Example: After convolution and pooling, the extracted features might go into a fully connected classifier.</p>"},{"location":"glossary/#gan-basics","title":"GAN Basics","text":"<p>The foundational idea of Generative Adversarial Networks, involving a generator and a discriminator in a minimax game. Example: A generator tries to create realistic images, while the discriminator attempts to distinguish them from real images.</p>"},{"location":"glossary/#generative-models","title":"Generative Models","text":"<p>Models that learn the joint probability distribution of data, enabling them to generate new, synthetic samples. Example: Models like GANs, VAEs, and diffusion models can create realistic images or text.</p>"},{"location":"glossary/#generator-vs-discriminator","title":"Generator Vs Discriminator","text":"<p>Two components of a GAN: the generator synthesizes data, and the discriminator classifies whether data is real or generated. Example: Training alternates between improving the generator\u2019s realism and tightening the discriminator\u2019s detection.</p>"},{"location":"glossary/#gpt-overview","title":"GPT Overview","text":"<p>Highlights of the Generative Pre-trained Transformer series, focusing on autoregressive language modeling at scale. Example: GPT-3 can generate coherent paragraphs of text based on a given prompt.</p>"},{"location":"glossary/#graph-neural-networks-gnns","title":"Graph Neural Networks (GNNs)","text":"<p>Networks designed to process graph-structured data, learning node or edge representations through message passing. Example: GNNs can predict molecular properties by treating atoms as nodes and bonds as edges.</p>"},{"location":"glossary/#gradient-descent","title":"Gradient Descent","text":"<p>An iterative optimization method that updates parameters in the opposite direction of the gradient of the loss function. Example: Simple gradient descent uses the entire dataset to compute gradients each iteration.</p>"},{"location":"glossary/#gru-units","title":"GRU Units","text":"<p>Gated Recurrent Unit cells that manage hidden state transitions without a separate cell state. Example: GRUs are often computationally simpler yet comparable to LSTMs for sequence tasks.</p>"},{"location":"glossary/#grid-search","title":"Grid Search","text":"<p>A hyperparameter tuning method that exhaustively tries every combination of a specified parameter grid. Example: Searching over different learning rates and regularization strengths for logistic regression.</p>"},{"location":"glossary/#gpu-acceleration","title":"GPU Acceleration","text":"<p>Utilizing graphics processing units to speed up parallelizable operations in neural network training. Example: Matrix multiplications in backprop are greatly accelerated on GPUs.</p>"},{"location":"glossary/#he-initialization","title":"He Initialization","text":"<p>Weight initialization method adapted for ReLU-like activation functions, aiming to maintain variance across layers. Example: Also known as Kaiming initialization, used to stabilize training in deep ReLU networks.</p>"},{"location":"glossary/#high-dimensional-data","title":"High-Dimensional Data","text":"<p>Data with many features, which can lead to the \u201ccurse of dimensionality\u201d and sparse observations in feature space. Example: Text data with thousands of unique terms is inherently high-dimensional.</p>"},{"location":"glossary/#history-of-ai","title":"History Of AI","text":"<p>The chronological development and milestones of AI, from symbolic systems to modern deep learning breakthroughs. Example: Tracing from early logic-based AI in the 1950s to recent achievements in large-scale neural models.</p>"},{"location":"glossary/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>The process of finding optimal values for parameters not directly learned during training, like learning rates or layer sizes. Example: Using cross-validation to compare multiple hyperparameter configurations.</p>"},{"location":"glossary/#imagenet","title":"ImageNet","text":"<p>A large-scale image dataset widely used as a benchmark for deep learning, especially in computer vision. Example: Models that excel on ImageNet often generalize well to a range of vision tasks.</p>"},{"location":"glossary/#inception-modules","title":"Inception Modules","text":"<p>Architectural blocks that perform convolutions of different sizes in parallel, then concatenate outputs. Example: GoogLeNet (Inception v1) introduced inception modules for more efficient resource usage.</p>"},{"location":"glossary/#initialization-methods","title":"Initialization Methods","text":"<p>Techniques for setting initial weight values in neural networks to aid stable convergence. Example: He or Xavier initialization is often chosen based on the activation function used.</p>"},{"location":"glossary/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Interactive web-based computational tools that combine code, visualizations, and text. Example: Commonly used in teaching ML, allowing students to experiment step by step and visualize results.</p>"},{"location":"glossary/#key-figures","title":"Key Figures","text":"<p>Influential researchers or pioneers in AI who contributed foundational theories or breakthroughs. Example: Alan Turing, Marvin Minsky, Geoffrey Hinton, Yoshua Bengio, and Yann LeCun.</p>"},{"location":"glossary/#language-modeling","title":"Language Modeling","text":"<p>Predicting the likelihood of a sequence of words, forming the basis for many NLP tasks. Example: A language model might predict the next word in a sentence or evaluate the fluency of generated text.</p>"},{"location":"glossary/#large-language-models-llms","title":"Large Language Models (LLMs)","text":"<p>Very large neural network-based models trained on massive text corpora to perform complex language tasks. Example: GPT-3, BERT, and PaLM can perform text generation, question answering, and zero-shot tasks.</p>"},{"location":"glossary/#latent-space","title":"Latent Space","text":"<p>The lower-dimensional internal representation of data learned by a model, especially in generative methods. Example: In a VAE, points in the latent space can be sampled and decoded into new images.</p>"},{"location":"glossary/#layer-stacking","title":"Layer Stacking","text":"<p>Arranging multiple layers in a network to build deeper models capable of complex feature extraction. Example: Adding layers in a CNN to capture higher-level abstractions of the input.</p>"},{"location":"glossary/#learning-rate","title":"Learning Rate","text":"<p>A hyperparameter controlling the step size in gradient-based optimization. Example: A learning rate that\u2019s too high might overshoot minima, while one that\u2019s too low could prolong training.</p>"},{"location":"glossary/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>Strategies to adjust the learning rate over time, typically reducing it as training progresses. Example: Step decay, exponential decay, and warm restarts are all scheduling techniques.</p>"},{"location":"glossary/#lime","title":"LIME","text":"<p>Local Interpretable Model-agnostic Explanations; explains predictions of any classifier by approximating it locally with an interpretable model. Example: LIME can highlight text snippets that most influenced a sentiment classifier\u2019s decision.</p>"},{"location":"glossary/#lstm-units","title":"LSTM Units","text":"<p>Long Short-Term Memory cells that maintain and control access to an internal cell state for long-range dependencies in sequences. Example: LSTMs are popular in NLP for capturing context across lengthy texts.</p>"},{"location":"glossary/#likelihood-estimation","title":"Likelihood Estimation","text":"<p>The process of determining parameters of a probabilistic model by maximizing the likelihood of observed data. Example: In generative models, training often involves maximizing log-likelihood.</p>"},{"location":"glossary/#local-minima","title":"Local Minima","text":"<p>Points in the loss landscape where no small move decreases the loss, but they may not be the global optimum. Example: Neural networks often rely on large parameter spaces that can have many local minima.</p>"},{"location":"glossary/#loss-functions","title":"Loss Functions","text":"<p>Mathematical functions quantifying the difference between predictions and targets, guiding parameter updates. Example: Mean Squared Error is a loss function for regression; Cross-Entropy is common for classification.</p>"},{"location":"glossary/#loss-surface","title":"Loss Surface","text":"<p>The multidimensional space defined by network parameters on which the loss function value is plotted. Example: Gradient descent methods navigate the loss surface to find minima.</p>"},{"location":"glossary/#manifold-hypothesis","title":"Manifold Hypothesis","text":"<p>The assumption that high-dimensional data points lie on lower-dimensional manifolds embedded in the input space. Example: Techniques like t-SNE or UMAP attempt to uncover these manifolds for visualization.</p>"},{"location":"glossary/#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>A mathematical framework for sequential decision-making, characterized by states, actions, rewards, and transition probabilities. Example: Reinforcement learning often models environments as MDPs for algorithmic exploration.</p>"},{"location":"glossary/#matplotlib-basics","title":"Matplotlib Basics","text":"<p>Fundamental features of Matplotlib, a Python library for creating static, animated, and interactive visualizations. Example: Plotting histograms, line graphs, and scatter plots to understand dataset distributions.</p>"},{"location":"glossary/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>A regression loss function that averages the squared differences between predictions and targets. Example: MSE is minimized during training of linear regression or certain autoencoders.</p>"},{"location":"glossary/#medical-imaging","title":"Medical Imaging","text":"<p>The application of AI to interpret and analyze medical scans such as MRIs, CTs, and X-rays. Example: CNNs detecting tumors in MRI scans to assist radiologists.</p>"},{"location":"glossary/#memory-management","title":"Memory Management","text":"<p>Techniques to efficiently allocate and handle data in GPU/CPU memory during model training. Example: Gradient checkpointing helps reduce memory usage in very deep networks.</p>"},{"location":"glossary/#mini-batch-training","title":"Mini-Batch Training","text":"<p>A compromise between batch and stochastic training, updating parameters after a small subset of samples. Example: Common in practice for balancing computational efficiency with stable gradient estimates.</p>"},{"location":"glossary/#ml-definition","title":"ML Definition","text":"<p>A subfield of AI that focuses on algorithms learning patterns from data rather than being explicitly programmed. Example: Classification and regression tasks where models improve their performance as they process more data.</p>"},{"location":"glossary/#mlops","title":"MLOps","text":"<p>A set of practices for deploying and maintaining machine learning models in production, analogous to DevOps. Example: Automated CI/CD pipelines that retrain and redeploy models as new data arrives.</p>"},{"location":"glossary/#model-compression","title":"Model Compression","text":"<p>Techniques to reduce model size and inference costs, such as pruning, quantization, or knowledge distillation. Example: Compressing a large CNN so it can run on edge devices with limited memory.</p>"},{"location":"glossary/#model-interpretability","title":"Model Interpretability","text":"<p>The clarity with which a human can understand a model\u2019s internal processes and decisions. Example: Using saliency maps to see which pixels in an image were key for a CNN\u2019s classification.</p>"},{"location":"glossary/#model-parallelism","title":"Model Parallelism","text":"<p>Splitting a model\u2019s layers or parameters across different computational units. Example: Large language models can be distributed across multiple GPUs, each storing only part of the network.</p>"},{"location":"glossary/#momentum-optimizer","title":"Momentum Optimizer","text":"<p>An extension of gradient descent that accumulates velocity from past updates to damp oscillations and accelerate learning. Example: Typically combined with a learning rate schedule to converge faster.</p>"},{"location":"glossary/#multi-layer-perceptron","title":"Multi-Layer Perceptron","text":"<p>A fully connected feedforward network with one or more hidden layers. Example: Often used as a baseline for tabular data classification or regression tasks.</p>"},{"location":"glossary/#natural-language-processing-nlp","title":"Natural Language Processing (NLP)","text":"<p>AI techniques enabling machines to understand, interpret, and generate human language. Example: Chatbots, machine translation, and sentiment analysis are common NLP applications.</p>"},{"location":"glossary/#neural-network","title":"Neural Network","text":"<p>A network of calculations designed to mimic the neurons in the human brain.</p>"},{"location":"glossary/#neural-machine-translation","title":"Neural Machine Translation","text":"<p>An approach to automated translation that uses deep neural networks, often encoder-decoder architectures. Example: Systems like Google Translate or DeepL rely on neural machine translation to handle multiple languages.</p>"},{"location":"glossary/#neural-network-origins","title":"Neural Network Origins","text":"<p>The historical foundation of neural networks, tracing back to early perceptron models and Hebbian learning. Example: The perceptron\u2019s creation in the 1950s laid groundwork for modern deep learning.</p>"},{"location":"glossary/#nobel-prizes","title":"Nobel Prizes","text":"<p>Prestigious awards that, while not commonly granted specifically for AI, have occasionally been given for foundational contributions relevant to AI fields. Example: John Nash\u2019s work in game theory influenced multi-agent AI, recognized with a Nobel in Economics.</p>"},{"location":"glossary/#nonlinear-embeddings","title":"Nonlinear Embeddings","text":"<p>Mapping high-dimensional data into a lower-dimensional space using nonlinear transformations. Example: t-SNE and UMAP produce nonlinear embeddings to visualize clusters.</p>"},{"location":"glossary/#numpy-basics","title":"NumPy Basics","text":"<p>Core functionalities of the NumPy library for handling n-dimensional arrays and performing vectorized operations. Example: Creating and reshaping arrays, broadcasting, and using universal functions like <code>np.exp()</code>.</p>"},{"location":"glossary/#object-detection","title":"Object Detection","text":"<p>Identifying and localizing objects within images or video frames, typically returning bounding boxes and class labels. Example: A YOLO-based model scanning real-time video to detect pedestrians and vehicles.</p>"},{"location":"glossary/#ollama","title":"Ollama","text":"<p>A community-driven project that democratizes access to powerful LLMs developed by various organizations.</p> <p>Ollama was founded by Michael Chiang and Jeffrey Morgan in Palo Alto, CA, and is an independent startup that participated in the W21 batch of Y Combinator.</p> <p>Ollama is not directly associated with Meta or the development of the Llama models.</p> <p>Ollama makes it easy to run LLMs on local hardware such as a consumer-grade GPU.  Installation can be done with two simple UNIX shell commands.</p> <ul> <li>Ollama website</li> </ul>"},{"location":"glossary/#one-hot-encoded-format","title":"One-hot Encoded Format","text":"<p>A data preprocessing technique that represents categorical variables as binary vectors where exactly one element is 1 (hot) and all others are 0 (cold).</p> <p>Example: In MNIST digit classification, each target label (0-9) is encoded as a 10-dimensional binary vector: the digit '3' becomes [0,0,0,1,0,0,0,0,0,0], where only the fourth position (index 3) contains a 1.</p>"},{"location":"glossary/#overfitting-vs-underfitting","title":"Overfitting Vs Underfitting","text":"<p>Overfitting occurs when a model learns spurious details in the training data, while underfitting fails to capture underlying trends. Example: A deep network might overfit a small dataset; a simple linear model might underfit a complex dataset.</p>"},{"location":"glossary/#pca","title":"PCA","text":"<p>Principal Component Analysis; a linear dimensionality reduction method finding directions of maximum variance. Example: PCA can compress a 100-dimensional dataset into a handful of principal components for visualization.</p>"},{"location":"glossary/#pandas-basics","title":"Pandas Basics","text":"<p>Common operations in the Pandas library, such as DataFrame creation, cleaning, merging, and analysis. Example: Using <code>pandas.read_csv()</code> to load a dataset and <code>DataFrame.describe()</code> for summary statistics.</p>"},{"location":"glossary/#perceptron-model","title":"Perceptron Model","text":"<p>A single-layer linear classifier that outputs a binary decision based on a weighted sum of inputs. Example: Historically important as the earliest form of a neural network unit.</p>"},{"location":"glossary/#policy-gradient","title":"Policy Gradient","text":"<p>A family of RL algorithms that optimize a parameterized policy by ascending an estimate of the gradient of expected reward. Example: REINFORCE is a basic policy gradient method using sampled trajectories.</p>"},{"location":"glossary/#pooling-layers","title":"Pooling Layers","text":"<p>Layers that reduce spatial dimensions by combining nearby feature responses, helping control overfitting. Example: Max pooling or average pooling in a CNN to halve the width and height of feature maps.</p>"},{"location":"glossary/#positional-encoding","title":"Positional Encoding","text":"<p>A technique used in transformer architectures to inject information about sequence order without recurrence. Example: Sine and cosine functions of different frequencies are added to word embeddings in Transformers.</p>"},{"location":"glossary/#precision","title":"Precision","text":"<p>The fraction of predicted positives that are truly positive, measuring correctness among positive predictions. Example: High precision in a medical test means few healthy people are incorrectly diagnosed as sick.</p>"},{"location":"glossary/#pruning","title":"Pruning","text":"<p>Removing weights or neurons from a trained model to reduce complexity and size, often with minimal accuracy loss. Example: Pruning can eliminate near-zero weights, speeding up inference in production.</p>"},{"location":"glossary/#privacy","title":"Privacy","text":"<p>The right or requirement that individuals control their personal data and how it is used. Example: Differential privacy in ML ensures training data cannot be reverse-engineered from model parameters.</p>"},{"location":"glossary/#prompt-engineering","title":"Prompt Engineering","text":"<p>The practice of carefully designing prompts or instructions to guide large language models for desired outputs. Example: Providing explicit context and constraints in a GPT prompt to extract factual answers.</p>"},{"location":"glossary/#protein-folding","title":"Protein Folding","text":"<p>The computational prediction of a protein\u2019s 3D structure from its amino acid sequence, aided by AI breakthroughs. Example: DeepMind\u2019s AlphaFold made significant progress in accurately modeling protein configurations.</p>"},{"location":"glossary/#project-scoping","title":"Project Scoping","text":"<p>Defining the objectives, resources, and deliverables of a machine learning or AI project. Example: Determining the data needed, performance targets, and timeline for a sentiment analysis project.</p>"},{"location":"glossary/#pytorch-intro","title":"PyTorch Intro","text":"<p>An overview of the PyTorch deep learning framework, emphasizing its dynamic computation graph. Example: PyTorch\u2019s imperative style makes debugging simpler and is favored by many researchers.</p>"},{"location":"glossary/#python-setup","title":"Python Setup","text":"<p>Basic installation and configuration of Python environments, packages, and tools. Example: Installing Python 3.x, pip, and setting up a virtual environment on a workstation.</p>"},{"location":"glossary/#q-learning","title":"Q-Learning","text":"<p>A model-free RL algorithm that learns an action-value function predicting future rewards. Example: The algorithm updates a Q-table or Q-network using Bellman equations and experiences from the environment.</p>"},{"location":"glossary/#quantization","title":"Quantization","text":"<p>Reducing the precision of model parameters (e.g., from 32-bit floating-point to 8-bit integers) to improve efficiency. Example: Quantized networks often run faster on edge devices with limited compute power.</p>"},{"location":"glossary/#random-search","title":"Random Search","text":"<p>A hyperparameter tuning method selecting parameter combinations randomly within predefined ranges. Example: Often more efficient than grid search when the parameter space is large.</p>"},{"location":"glossary/#recall","title":"Recall","text":"<p>The fraction of actual positives correctly identified by the model, measuring completeness. Example: In a medical test, recall indicates the percentage of sick patients who are correctly diagnosed.</p>"},{"location":"glossary/#recommender-systems","title":"Recommender Systems","text":"<p>Algorithms designed to suggest items, such as products or media, to users based on preferences or similarities. Example: Matrix factorization or deep collaborative filtering for movie recommendations.</p>"},{"location":"glossary/#regression-vs-classification","title":"Regression Vs Classification","text":"<p>Distinguishes between predicting a continuous value (regression) and predicting a discrete label (classification). Example: House price prediction is regression, while determining spam vs non-spam email is classification.</p>"},{"location":"glossary/#regularization-overview","title":"Regularization Overview","text":"<p>Techniques (like weight decay, dropout) that constrain model complexity to prevent overfitting. Example: Adding L2 regularization to a neural network\u2019s cost function penalizes large weights.</p>"},{"location":"glossary/#requirement-analysis","title":"Requirement Analysis","text":"<p>Identifying the scope, constraints, and success criteria for an AI project before implementation. Example: Determining user needs, regulatory constraints, and performance thresholds for a medical imaging system.</p>"},{"location":"glossary/#residual-networks-resnet","title":"Residual Networks (ResNet)","text":"<p>Deep neural networks with skip connections that help mitigate vanishing gradients by enabling identity mappings. Example: ResNet-50 is a popular architecture for ImageNet classification tasks.</p>"},{"location":"glossary/#responsible-ai","title":"Responsible AI","text":"<p>AI development and deployment aligned with legal, ethical, and societal values. Example: Designing systems that respect user privacy, avoid bias, and are transparent.</p>"},{"location":"glossary/#reward-shaping","title":"Reward Shaping","text":"<p>Modifying or adding auxiliary rewards in RL to guide the agent toward desired behaviors. Example: Giving an agent a small reward for each time step it stays on track in a self-driving simulation.</p>"},{"location":"glossary/#rl-definition","title":"RL Definition","text":"<p>Reinforcement Learning is a subfield of AI where agents learn optimal behaviors through trial-and-error feedback from an environment. Example: An RL agent in a game environment receives rewards (scores) for achieving objectives.</p>"},{"location":"glossary/#robotics","title":"Robotics","text":"<p>The intersection of engineering and AI focusing on designing and controlling physical machines that perform tasks autonomously or semi-autonomously. Example: Automated robotic arms in manufacturing lines guided by computer vision.</p>"},{"location":"glossary/#scaling-and-normalization","title":"Scaling And Normalization","text":"<p>Rescaling features to standardized ranges or distributions for more stable training. Example: Applying min-max scaling to each feature so that all values lie between 0 and 1.</p>"},{"location":"glossary/#score-matching","title":"Score Matching","text":"<p>A technique for training generative models by matching the score function (gradient of log-density) of the model to that of the data. Example: Used in diffusion-based methods to iteratively denoise samples.</p>"},{"location":"glossary/#scipy-basics","title":"SciPy Basics","text":"<p>Fundamental capabilities of SciPy for scientific computations, including optimization, integration, and statistics. Example: Using <code>scipy.optimize</code> to implement advanced fitting procedures in an ML pipeline.</p>"},{"location":"glossary/#scikit-learn-overview","title":"Scikit-Learn Overview","text":"<p>Core features of the scikit-learn library, offering high-level APIs for classical ML algorithms, preprocessing, and validation. Example: Using <code>GridSearchCV</code> for hyperparameter tuning on a random forest model.</p>"},{"location":"glossary/#security","title":"Security","text":"<p>Protective measures ensuring AI systems resist unauthorized access, tampering, or malicious attacks. Example: Hardening model APIs against adversarial inputs that might reveal sensitive data.</p>"},{"location":"glossary/#self-attention","title":"Self-Attention","text":"<p>A mechanism in transformer models enabling each position in a sequence to attend to every other position for context. Example: BERT\u2019s self-attention layers capture dependencies in a sentence without relying on recurrence.</p>"},{"location":"glossary/#sequence-modeling","title":"Sequence Modeling","text":"<p>Approaches to handle sequential data (text, time series, etc.), capturing dependencies across timesteps. Example: LSTMs, GRUs, and Transformers are common sequence modeling architectures.</p>"},{"location":"glossary/#sequence-to-sequence","title":"Sequence-To-Sequence","text":"<p>Neural architectures mapping an input sequence (e.g., a sentence in English) to an output sequence (e.g., a sentence in French). Example: Used extensively in neural machine translation and speech recognition tasks.</p>"},{"location":"glossary/#shap","title":"SHAP","text":"<p>SHapley Additive exPlanations; a method based on Shapley values to interpret predictions by attributing contributions of each feature. Example: Generating a SHAP plot to see which features most influence a credit-scoring model\u2019s decisions.</p>"},{"location":"glossary/#skip-connections","title":"Skip Connections","text":"<p>Links that bypass one or more layers, helping gradients flow more easily in deep networks. Example: ResNet\u2019s skip connections add layer outputs directly to subsequent layers to mitigate vanishing gradients.</p>"},{"location":"glossary/#speech-recognition","title":"Speech Recognition","text":"<p>The task of converting spoken language into text using acoustic and language models. Example: Voice assistants transcribe user commands in real-time for further processing.</p>"},{"location":"glossary/#stochastic-training","title":"Stochastic Training","text":"<p>Updating model parameters after each individual sample or a small random subset (mini-batch). Example: Known as SGD, helps the model converge faster with more frequent updates.</p>"},{"location":"glossary/#stride-and-padding","title":"Stride And Padding","text":"<p>Parameters in convolution operations that determine how filters slide over input data and whether border regions are preserved. Example: Padding=\u201csame\u201d ensures output dimensions remain the same as input for certain CNN layers.</p>"},{"location":"glossary/#style-transfer","title":"Style Transfer","text":"<p>Neural technique that reworks the style of one image onto the content of another. Example: Merging a photo with the painting style of Van Gogh\u2019s \u201cStarry Night.\u201d</p>"},{"location":"glossary/#supervised-vs-unsupervised","title":"Supervised Vs Unsupervised","text":"<p>A distinction between learning with labeled data (supervised) vs. discovering patterns in unlabeled data (unsupervised). Example: Classifying labeled images is supervised; clustering unlabeled images is unsupervised.</p>"},{"location":"glossary/#symbolic-ai","title":"Symbolic AI","text":"<p>AI methods using explicit, human-readable representations of problems, logic, and rules. Example: Expert systems that encode domain knowledge in symbolic form.</p>"},{"location":"glossary/#team-collaboration","title":"Team Collaboration","text":"<p>Collective effort where multiple individuals share responsibilities and knowledge to complete AI projects. Example: Data engineers, data scientists, and software developers working together on a production pipeline.</p>"},{"location":"glossary/#t-sne","title":"t-SNE","text":"<p>t-Distributed Stochastic Neighbor Embedding; a non-linear dimensionality reduction method for visualization. Example: Often used to plot high-dimensional data (like embeddings) in 2D or 3D.</p>"},{"location":"glossary/#tensorflow-intro","title":"TensorFlow Intro","text":"<p>An overview of the TensorFlow framework emphasizing its computational graph, eager mode, and ecosystem tools. Example: Students build and train neural networks with Keras, a high-level TensorFlow API.</p>"},{"location":"glossary/#text-generation","title":"Text Generation","text":"<p>The task of producing coherent text sequences, often using language models. Example: GPT-based systems can generate paragraphs of natural-sounding text from a prompt.</p>"},{"location":"glossary/#text-generation-models","title":"Text Generation Models","text":"<p>Architectures specialized in producing novel text, typically via learned probability distributions over tokens. Example: LSTM-based decoders or transformer-based language models for writing summaries or creative content.</p>"},{"location":"glossary/#text-to-image","title":"Text-To-Image","text":"<p>Models that generate images based on textual input descriptions. Example: DALL\u00b7E variants produce custom images from user prompts like \u201can armchair shaped like an avocado.\u201d</p>"},{"location":"glossary/#time-series-forecasting","title":"Time-Series Forecasting","text":"<p>Techniques to predict future values of a sequence based on past observations. Example: LSTMs or Prophet library used for forecasting stock prices or energy demands.</p>"},{"location":"glossary/#training-validation-test","title":"Training-Validation-Test","text":"<p>Standard data splitting strategy: a training set for model fitting, a validation set for hyperparameter tuning, and a test set for final evaluation. Example: 60% train, 20% validation, 20% test split for a typical classification dataset.</p>"},{"location":"glossary/#transfer-learning-basics","title":"Transfer Learning Basics","text":"<p>Techniques for leveraging pre-trained models on new tasks with limited additional data. Example: Using a pre-trained ResNet on ImageNet as a feature extractor for a custom dataset.</p>"},{"location":"glossary/#transformers","title":"Transformers","text":"<p>An architecture for training deep neural networks that can be parallelized by GPUs.</p>"},{"location":"glossary/#tpu-acceleration","title":"TPU Acceleration","text":"<p>Using Tensor Processing Units (custom ASICs by Google) to speed up large-scale model training. Example: TPUs can be accessed on Google Cloud to train large transformer models more efficiently than GPUs.</p>"},{"location":"glossary/#umap","title":"UMAP","text":"<p>Uniform Manifold Approximation and Projection; a non-linear technique for dimensionality reduction. Example: Faster and often better at preserving global structure compared to t-SNE on large datasets.</p>"},{"location":"glossary/#value-functions","title":"Value Functions","text":"<p>In reinforcement learning, functions estimating expected future rewards from a given state (or state-action pair). Example: The critic in actor-critic algorithms learns a value function to guide the actor\u2019s updates.</p>"},{"location":"glossary/#vanilla-rnn","title":"Vanilla RNN","text":"<p>A basic recurrent neural network that uses hidden states to process sequences one step at a time. Example: Applied to simple sequence tasks but prone to vanishing/exploding gradient issues for long sequences.</p>"},{"location":"glossary/#vanishing-gradients","title":"Vanishing Gradients","text":"<p>A phenomenon where gradients diminish in magnitude through backprop, hindering learning in deep networks. Example: Sigmoid activations can exacerbate vanishing gradients as the network depth grows.</p>"},{"location":"glossary/#version-control","title":"Version Control","text":"<p>Systems that track changes to code over time, allowing collaboration and reversion if needed. Example: Git and GitHub store historical commits and manage parallel development branches.</p>"},{"location":"glossary/#version-control-workflow","title":"Version Control Workflow","text":"<p>Best practices and procedures for using version control systems, including branching, merging, and reviewing. Example: A feature branch workflow ensures code is tested and reviewed before merging into <code>main</code>.</p>"},{"location":"glossary/#variational-autoencoders-vae","title":"Variational Autoencoders (VAE)","text":"<p>Generative models that learn a latent distribution of data using an encoder-decoder framework with a KL-divergence term. Example: VAEs create smooth latent spaces, enabling interpolation between different generated samples.</p>"},{"location":"glossary/#vision-transformers-vit","title":"Vision Transformers (ViT)","text":"<p>Transformer-based architectures adapted for computer vision tasks by splitting images into patches. Example: ViT processes each patch as a token, leveraging self-attention for image classification.</p>"},{"location":"glossary/#weights-and-biases","title":"Weights And Biases","text":"<p>Trainable parameters in a neural network that transform inputs into outputs. Example: A linear layer with 10 inputs and 5 outputs would have 10\u00d75 weights plus 5 bias terms.</p>"},{"location":"glossary/#weight-decay","title":"Weight Decay","text":"<p>A regularization method applying an L2 penalty on weights to encourage smaller parameter values. Example: Often implemented as a parameter in optimizers like SGD or Adam (e.g., <code>weight_decay=1e-4</code>).</p>"},{"location":"glossary/#xavier-initialization","title":"Xavier Initialization","text":"<p>A weight initialization method maintaining variance in both forward and backward passes for certain activation functions. Example: Often used with tanh or sigmoid activations to keep gradients stable early in training.</p>"},{"location":"glossary/#text-to-image_1","title":"Text-to-Image","text":"<p>(Duplicate check \u2014 We already covered \"Text-To-Image\" at concept 110. Ensuring no duplication. It's already listed. We\u2019ll ignore any duplicates.)</p> <p>(No more new Z or Y terms, so we\u2019re done.)</p> <p>This completes the alphabetical glossary of all 200 concepts.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":"<ol> <li>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"chapters/activation-functions/","title":"Activation Functions in Deep Learning","text":""},{"location":"chapters/activation-functions/#introduction","title":"Introduction","text":"<p>Activation functions are mathematical equations that determine the output of a neural network. They are crucial components that introduce non-linearity into the network, allowing it to learn complex patterns and relationships in data. Without activation functions, neural networks would simply be a series of linear transformations, severely limiting their ability to solve complex problems.</p> <p> FIGURE 1: Basic neuron diagram showing input, weighted sum, activation function, and output</p>"},{"location":"chapters/activation-functions/#core-concepts","title":"Core Concepts","text":"<p>The activation function takes the weighted sum of inputs and biases, then transforms this value into an output signal. This transformation serves several critical purposes:</p> <ol> <li>Non-linearity: Enables the network to approximate complex functions</li> <li>Output bounding: Many activation functions constrain outputs to specific ranges</li> <li>Gradient properties: Influences how effectively the network can learn through backpropagation</li> </ol>"},{"location":"chapters/activation-functions/#common-activation-functions","title":"Common Activation Functions","text":""},{"location":"chapters/activation-functions/#1-sigmoid-logistic-function","title":"1. Sigmoid (Logistic Function)","text":"\\[ f(x) = 1 / (1 + e^(-x)) \\] <p>Pros: - Outputs bounded between 0 and 1 - Smooth gradient - Clear probability interpretation - Historically significant</p> <p>Cons: - Suffers from vanishing gradient problem - Output not zero-centered - Computationally expensive (exponential function) - Saturates and kills gradients</p> <p>Computational Cost: High (requires exponential calculation)</p>"},{"location":"chapters/activation-functions/#2-hyperbolic-tangent-tanh","title":"2. Hyperbolic Tangent (tanh)","text":"\\[ f(x) = (e^x - e^(-x)) / (e^x + e^(-x)) \\] <p>Pros: - Zero-centered outputs (-1 to 1) - Stronger gradients than sigmoid - Often works better than sigmoid in practice</p> <p>Cons: - Still suffers from vanishing gradient - Computationally expensive - Saturates</p> <p>Computational Cost: High (requires exponential calculations)</p>"},{"location":"chapters/activation-functions/#3-rectified-linear-unit-relu","title":"3. Rectified Linear Unit (ReLU)","text":"\\[ f(x) = max(0, x) \\] <p>Pros: - Computationally efficient - No vanishing gradient for positive values - Sparse activation (natural regularization) - Faster convergence - Biologically inspired</p> <p>Cons: - \"Dying ReLU\" problem - Not zero-centered - Unbounded positive activation</p> <p>Computational Cost: Very Low (simple max operation)</p>"},{"location":"chapters/activation-functions/#4-leaky-relu","title":"4. Leaky ReLU","text":"\\[ f(x) = max(\u03b1x, x), where \u03b1 is typically 0.01 \\] <p>Pros: - Prevents dying ReLU problem - All benefits of ReLU - Allows for negative gradients</p> <p>Cons: - Results not consistent across problems - Additional hyperparameter (\u03b1) - Not zero-centered</p> <p>Computational Cost: Very Low (simple max operation with scalar multiplication)</p>"},{"location":"chapters/activation-functions/#5-exponential-linear-unit-elu","title":"5. Exponential Linear Unit (ELU)","text":"\\[ f(x) = x if x &gt; 0 f(x) = \u03b1(e^x - 1) if x \u2264 0 \\] <p>Pros: - Reduces bias shift - Natural gradient - Robust to noise - Better handling of negative values</p> <p>Cons: - Computationally more expensive than ReLU - Additional hyperparameter (\u03b1) - Slightly more complex derivative</p> <p>Computational Cost: Moderate (exponential calculation only for negative values)</p>"},{"location":"chapters/activation-functions/#computational-efficiency-and-scale","title":"Computational Efficiency and Scale","text":"<p>When building large-scale deep learning models, the computational efficiency of activation functions becomes crucial. Here's a comparison of relative computational costs:</p> <ol> <li>ReLU/Leaky ReLU: ~1x (baseline)</li> <li>ELU: ~2-3x</li> <li>Sigmoid/Tanh: ~4-5x</li> </ol> <p>For a model with millions or billions of parameters, these differences can significantly impact: - Training time - Inference latency - Energy consumption - Hardware requirements</p>"},{"location":"chapters/activation-functions/#modern-usage-patterns","title":"Modern Usage Patterns","text":"<p>In contemporary deep learning practice, activation functions are typically chosen as follows:</p>"},{"location":"chapters/activation-functions/#1-default-choice-relu","title":"1. Default Choice: ReLU","text":"<ul> <li>Used in ~80% of deep learning applications</li> <li>Standard for most convolutional neural networks</li> <li>Excellent balance of performance and efficiency</li> </ul>"},{"location":"chapters/activation-functions/#2-common-alternatives","title":"2. Common Alternatives:","text":"<ul> <li>Leaky ReLU: When dying ReLU is a concern</li> <li>ELU: When negative values need better handling</li> <li>Tanh: In RNNs and some specialized architectures</li> </ul>"},{"location":"chapters/activation-functions/#3-specific-use-cases","title":"3. Specific Use Cases:","text":"<ul> <li>Sigmoid: Final layer of binary classification</li> <li>Softmax: Final layer of multi-class classification</li> <li>Linear: Final layer of regression tasks</li> </ul>"},{"location":"chapters/activation-functions/#conclusion","title":"Conclusion","text":"<p>The choice of activation function can significantly impact both model performance and computational efficiency. While ReLU dominates modern architectures due to its simplicity and effectiveness, understanding the full range of options allows practitioners to make informed decisions based on their specific use cases and constraints.</p> <p>For large-scale deep learning projects, the computational efficiency of ReLU makes it particularly attractive, as the activation function computation can represent a significant portion of the overall computational budget. However, practitioners should remain aware of potential issues like the dying ReLU problem and be prepared to switch to alternatives when necessary.</p> <p> FIGURE 7: Comparative visualization of all activation functions discussed]</p>"},{"location":"chapters/activation-functions/#practice-questions","title":"Practice Questions","text":"<ol> <li>Why can't neural networks only use linear transformations without activation functions?</li> <li>What makes ReLU computationally efficient compared to sigmoid and tanh?</li> <li>In what scenarios might you choose ELU over ReLU despite its higher computational cost?</li> <li>How does the choice of activation function impact the training of very deep networks?</li> </ol>"},{"location":"chapters/cnns/","title":"Convolutional Neural Networks for Image Processing","text":""},{"location":"chapters/cnns/#chapter-outline","title":"Chapter Outline","text":""},{"location":"chapters/cnns/#1-introduction-to-cnns","title":"1. Introduction to CNNs","text":"<ul> <li>Historical context and development</li> <li>Comparison with traditional neural networks</li> <li>Key advantages for image processing tasks</li> <li>Overview of CNN applications in computer vision</li> </ul>"},{"location":"chapters/cnns/#2-fundamental-cnn-components","title":"2. Fundamental CNN Components","text":"<ul> <li>Convolutional layers</li> <li>Filters and kernels</li> <li>Feature maps</li> <li>Stride and padding options</li> <li>Pooling layers</li> <li>Max pooling</li> <li>Average pooling</li> <li>Purpose in reducing spatial dimensions</li> <li>Activation functions</li> <li>ReLU and variants</li> <li>Importance of non-linearity</li> <li>Fully connected layers</li> </ul>"},{"location":"chapters/cnns/#3-cnn-architectures","title":"3. CNN Architectures","text":"<ul> <li>LeNet</li> <li>AlexNet</li> <li>VGG</li> <li>Inception/GoogLeNet</li> <li>ResNet</li> <li>DenseNet</li> <li>Vision Transformers (ViT)</li> </ul>"},{"location":"chapters/cnns/#4-training-cnns","title":"4. Training CNNs","text":"<ul> <li>Loss functions for image tasks</li> <li>Backpropagation through convolutional layers</li> <li>Regularization techniques</li> <li>Dropout</li> <li>Batch normalization</li> <li>Data augmentation</li> <li>Optimization strategies</li> <li>Learning rate scheduling</li> <li>Weight initialization methods</li> </ul>"},{"location":"chapters/cnns/#5-transfer-learning-with-cnns","title":"5. Transfer Learning with CNNs","text":"<ul> <li>Pre-trained models on ImageNet</li> <li>Feature extraction approaches</li> <li>Fine-tuning strategies</li> <li>Domain adaptation challenges</li> </ul>"},{"location":"chapters/cnns/#6-advanced-cnn-applications","title":"6. Advanced CNN Applications","text":"<ul> <li>Object detection</li> <li>R-CNN family</li> <li>YOLO</li> <li>SSD</li> <li>Semantic segmentation</li> <li>FCN</li> <li>U-Net</li> <li>Instance segmentation</li> <li>Mask R-CNN</li> <li>Image generation and style transfer</li> </ul>"},{"location":"chapters/cnns/#7-explainable-cnns","title":"7. Explainable CNNs","text":"<ul> <li>Visualization techniques</li> <li>Feature maps</li> <li>Activation maximization</li> <li>Grad-CAM</li> <li>Interpretability approaches</li> <li>Ethical considerations</li> </ul>"},{"location":"chapters/cnns/#8-optimizing-cnn-performance","title":"8. Optimizing CNN Performance","text":"<ul> <li>Model compression techniques</li> <li>Pruning</li> <li>Quantization</li> <li>Hardware considerations</li> <li>GPU acceleration</li> <li>Edge deployment</li> <li>Model architecture search</li> </ul>"},{"location":"chapters/cnns/#9-case-studies","title":"9. Case Studies","text":"<ul> <li>Medical imaging analysis</li> <li>Autonomous driving perception</li> <li>Face recognition systems</li> <li>Industrial quality control</li> </ul>"},{"location":"chapters/cnns/#10-future-directions","title":"10. Future Directions","text":"<ul> <li>Hybrid architectures (CNN + Transformer)</li> <li>Self-supervised learning for CNNs</li> <li>Multimodal learning</li> <li>Neuromorphic approaches</li> </ul>"},{"location":"chapters/cnns/#11-practical-implementations","title":"11. Practical Implementations","text":"<ul> <li>CNN implementations in popular frameworks</li> <li>Deployment considerations</li> <li>Best practices for CNN development</li> </ul>"},{"location":"chapters/cnns/#12-glossary-and-resources","title":"12. Glossary and Resources","text":"<ul> <li>Key terminology</li> <li>Recommended reading</li> <li>Online courses and tutorials</li> <li>Datasets for practice</li> </ul>"},{"location":"chapters/cnns/cnn-microsim-ideas/","title":"CNN MicroSim Ideas","text":""},{"location":"chapters/cnns/cnn-microsim-ideas/#interactive-microsims-for-learning-cnn-concepts","title":"Interactive MicroSims for Learning CNN Concepts","text":"<p>Prompt</p> <p>A MicroSim is an in-browser interactive simulation written by a program such as p5.js that is used to illustrate a complex concept.  What topics in the above outline could be learned with an interactive simulation where the student run a small simulation they must interact with.  The simulation should have a start/stop/reset button, radio buttons and sliders in a control area that change drawing.  </p> <p>For each possible MicroSim, return the following:</p> <p>Clever Name Description Input Controls Learning Objective Assessment</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#response","title":"Response","text":"<p>Based on the chapter outline for Convolutional Neural Networks, here are several interactive MicroSim concepts that would effectively illustrate key concepts through simulation:</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#1-filtervision","title":"1. FilterVision","text":"<p>Clever Name: FilterVision</p> <p>Description: An interactive simulation that demonstrates how convolutional filters work on real images. Users can apply different filters (edge detection, blurring, sharpening) to sample images and see the resulting feature maps in real-time.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Slider for filter size (3x3, 5x5, 7x7)</li> <li>Slider for filter intensity</li> <li>Radio buttons for filter types (Sobel, Gaussian blur, emboss, custom)</li> <li>Custom filter value inputs for advanced users</li> </ul> <p>Learning Objective: Understand how different convolutional filters extract various features from images and how filter parameters affect the output.</p> <p>Assessment: Ask students to identify which filter would best detect specific features (edges, textures, corners) in an image, and to explain why filter size affects the feature detection capability.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#2-poolingplayground","title":"2. PoolingPlayground","text":"<p>Clever Name: PoolingPlayground</p> <p>Description: A visualization that shows how max pooling and average pooling affect feature maps. Users can see the original image section, apply convolutions, and then observe how pooling reduces dimensions while preserving important information.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Radio buttons for pooling type (Max, Average)</li> <li>Slider for pooling window size (2x2, 3x3, 4x4)</li> <li>Slider for stride value</li> <li>Toggle for displaying numerical values</li> </ul> <p>Learning Objective: Understand the differences between pooling types and how they preserve information while reducing computational requirements.</p> <p>Assessment: Have students predict the output dimensions after applying different pooling operations and explain why max pooling might be preferred for certain types of features.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#3-activationstation","title":"3. ActivationStation","text":"<p>Clever Name: ActivationStation</p> <p>Description: A simulation that visualizes how different activation functions transform input signals in neural networks, with special emphasis on how they affect feature maps in CNNs.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Radio buttons for activation functions (ReLU, Leaky ReLU, Sigmoid, Tanh)</li> <li>Slider for function parameters (e.g., leak rate for Leaky ReLU)</li> <li>Toggle for before/after comparison</li> <li>Slider to adjust input values</li> </ul> <p>Learning Objective: Understand how activation functions introduce non-linearity and how this impacts feature detection in CNNs.</p> <p>Assessment: Ask students to identify which activation function would work best for specific scenarios and explain why ReLU has become the standard in most CNN architectures.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#4-layerviz","title":"4. LayerViz","text":"<p>Clever Name: LayerViz</p> <p>Description: An interactive model of a small CNN where users can see how an image transforms as it passes through multiple layers. The simulation shows activations at each layer and allows slowing down the forward pass.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Slider for network depth (number of layers)</li> <li>Radio buttons for different sample images</li> <li>Toggle switches to enable/disable specific layers</li> <li>Slider for \"animation speed\" of forward propagation</li> </ul> <p>Learning Objective: Visualize the hierarchical feature extraction in CNNs, from low-level features in early layers to high-level concepts in deeper layers.</p> <p>Assessment: Have students describe what types of features are detected at different depths of the network and explain why deeper networks can recognize more complex patterns.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#5-backproptracker","title":"5. BackpropTracker","text":"<p>Clever Name: BackpropTracker</p> <p>Description: A visualization of how gradients flow backward through a CNN during training, showing which parts of the network are most affected by errors and how weights update.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Slider for learning rate</li> <li>Slider for batch size</li> <li>Radio buttons for different loss functions</li> <li>Toggle to show/hide gradient magnitudes</li> </ul> <p>Learning Objective: Understand the backpropagation process in CNNs and how parameter updates occur during training.</p> <p>Assessment: Ask students to explain what happens when learning rates are too high or too low, and how the gradient flow differs in early versus deep layers.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#6-architectbuilder","title":"6. ArchitectBuilder","text":"<p>Clever Name: ArchitectBuilder</p> <p>Description: A drag-and-drop interface for building simple CNN architectures, with real-time feedback on parameter count, computational complexity, and simulated performance on basic tasks.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Drag-and-drop layer components</li> <li>Sliders for layer parameters (filters, neurons, etc.)</li> <li>Radio buttons for different datasets (MNIST, CIFAR-10)</li> <li>Toggle for skip connections</li> </ul> <p>Learning Objective: Learn the tradeoffs in CNN design and how different architectural choices affect model performance and computational requirements.</p> <p>Assessment: Challenge students to design the smallest network that can achieve a target accuracy on a simple dataset, explaining their design decisions.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#7-transferinsight","title":"7. TransferInsight","text":"<p>Clever Name: TransferInsight</p> <p>Description: A simulation demonstrating transfer learning by showing feature activations from pre-trained models and how they can be repurposed for new tasks with limited data.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Radio buttons for pre-trained models (VGG, ResNet, etc.)</li> <li>Slider for freezing/unfreezing layers</li> <li>Slider for new dataset size</li> <li>Toggle to compare from-scratch vs. transfer learning</li> </ul> <p>Learning Objective: Understand how and why transfer learning works, and when to freeze or fine-tune pre-trained layers.</p> <p>Assessment: Have students determine which layers should be frozen versus fine-tuned for specific target tasks, with justification based on the visualization.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#8-explainablecnn","title":"8. ExplainableCNN","text":"<p>Clever Name: ExplainableCNN</p> <p>Description: An interactive tool showing various CNN interpretability methods like activation maps, Grad-CAM, and feature visualization on sample images, helping students understand what the network \"sees.\"</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Radio buttons for visualization technique</li> <li>Slider for layer selection</li> <li>Radio buttons for sample images</li> <li>Toggle for superimposing visualizations on original image</li> </ul> <p>Learning Objective: Learn techniques for making CNN decisions more transparent and interpretable.</p> <p>Assessment: Ask students to use different visualization techniques to determine why a CNN might misclassify specific examples and how the network's \"attention\" differs across classes.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#9-optimizenet","title":"9. OptimizeNet","text":"<p>Clever Name: OptimizeNet</p> <p>Description: A simulation demonstrating model optimization techniques like pruning and quantization, showing the effects on model size, inference speed, and accuracy.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Slider for pruning threshold</li> <li>Slider for quantization level (32-bit to 4-bit)</li> <li>Radio buttons for optimization strategy</li> <li>Toggle to show accuracy-vs-size tradeoff</li> </ul> <p>Learning Objective: Understand methods for making CNNs more efficient for deployment on resource-constrained devices.</p> <p>Assessment: Challenge students to optimize a model to fit within specific memory constraints while minimizing accuracy loss, explaining the tradeoffs they encounter.</p>"},{"location":"chapters/cnns/cnn-microsim-ideas/#10-augmentationstation","title":"10. AugmentationStation","text":"<p>Clever Name: AugmentationStation</p> <p>Description: A hands-on tool demonstrating how data augmentation expands training datasets and improves CNN robustness, showing transformations applied to images and their impact on training.</p> <p>Input Controls:</p> <ul> <li>Start/Stop/Reset buttons</li> <li>Toggle switches for different augmentation types (rotation, flip, crop, color)</li> <li>Sliders for augmentation parameters (rotation angle, crop size, etc.)</li> <li>Radio buttons for base image selection</li> <li>Slider for augmentation intensity</li> </ul> <p>Learning Objective: Understand how data augmentation helps prevent overfitting and improves model generalization.</p> <p>Assessment: Have students design an augmentation strategy for specific scenarios (limited data, class imbalance, etc.) and explain how their choices would help the model generalize better.</p>"},{"location":"chapters/counting-model-parameters/","title":"Counting Model Parameters","text":"<p>Many discussion on deep learning models describe a model based on how many \"parameters\" it has.  The bigger the model, the more parameters it has.  But what exactly is a model parameter and how do we calculate the number of parameters for a deep neural network?</p> <p>Let's take a look at the formula for calculating the total parameters in a fully connected neural network:</p> <ol> <li>For weights:</li> <li>Each connection between two layers needs a weight parameter</li> <li>For each pair of adjacent layers: (nodes in previous layer \u00d7 nodes in current layer)</li> <li>Since input layer doesn't have incoming connections, we multiply by (numLayers - 1)</li> <li> <p>Total weight parameters = neuronsPerLayer \u00d7 neuronsPerLayer \u00d7 (numLayers - 1)</p> </li> <li> <p>For biases:</p> </li> <li>Each neuron (except in input layer) has one bias parameter</li> <li> <p>Total bias parameters = neuronsPerLayer \u00d7 (numLayers - 1)</p> </li> <li> <p>Total parameters = Total weight parameters + Total bias parameters</p> </li> </ol> <p>For example, in a network with 3 layers (including input) and 4 neurons per layer:</p> <ul> <li>Weight parameters: 4 \u00d7 4 \u00d7 (3-1) = 32</li> <li>Bias parameters: 4 \u00d7 (3-1) = 8</li> <li>Total parameters: 32 + 8 = 40</li> </ul> <p>We have created an interactive visualization where you can: 1. Adjust the number of layers (2-7) 2. Adjust the neurons per layer (2-10) 3. See the network structure 4. Get a breakdown of weight parameters, bias parameters, and total parameters</p> <p>Would you like me to add any additional features to the visualization?</p>"},{"location":"labs/","title":"Deep Learning Labs","text":"<p>There is a list of hands-on interactive labs that students can do to learn more about Deep Learning.</p> <p>Calculating Tokens Per Second on a Local GPU</p>"},{"location":"labs/calculating-tokens-per-second/","title":"Calculating Tokens Per Second","text":"<p>How quickly a model returns text is a key metric.  Here is a sample program that calculates the number of tokens per second for the Deepseek-r1:7b running in an Ollama framework.  This test was run on my local GPU which is a NVIDIA RTX 2080 Ti with 12GB RAM running CUDA 12.6.  The size of the model was 4.7GB which fits well within the 12GB ram of the GPU.</p> <p>To time the performance of a model we do the following:</p>"},{"location":"labs/calculating-tokens-per-second/#1-record-the-time-before-the-model-runs-with","title":"1. Record the time before the model runs with","text":"<pre><code>start_time = time.time()\n</code></pre>"},{"location":"labs/calculating-tokens-per-second/#2-record-the-end-time-and-calculate-the-elapsed-time","title":"2. Record the end time and calculate the elapsed time","text":"<pre><code>end_time = time.time()\nelapsed_time = end_time - start_time\n</code></pre>"},{"location":"labs/calculating-tokens-per-second/#3-count-the-total-number-of-tokens-in-the-result-and-calculate-the-tokens-per-second","title":"3. Count the total number of tokens in the result and calculate the tokens per second","text":"<pre><code>token_count = len(result.split())\ntokens_per_second = token_count / elapsed_time if elapsed_time &gt; 0 else 0\n</code></pre> <p>Note that we check to make sure that elapsed_time is greater than zero to avoid a divide-by-zero error.</p>"},{"location":"labs/calculating-tokens-per-second/#complete-program","title":"Complete Program","text":"<pre><code>import time\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_ollama.llms import OllamaLLM\n\n# Define your prompt template.\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\n# Initialize the Ollama model.\nmodel = OllamaLLM(model=\"llama3.1\")\n\n# Chain the prompt and the model.\nchain = prompt | model\n\n# Record the start time.\nstart_time = time.time()\n\n# Invoke the chain with your question.\nresult = chain.invoke({\"question\": \"What are the 50 most imporant concepts in a college level course on deep learning?\"})\n\n# Record the end time.\nend_time = time.time()\nelapsed_time = end_time - start_time\n\n# Count tokens in the output.\n# Here we assume tokens are separated by whitespace.\n# (For more accurate token counts, consider using the model's specific tokenizer.)\ntoken_count = len(result.split())\n\n# Calculate tokens per second.\ntokens_per_second = token_count / elapsed_time if elapsed_time &gt; 0 else 0\n\n# Print the result and statistics.\nprint(\"Result:\")\nprint(result)\nprint(\"\\nStatistics:\")\nprint(f\"Token Count: {token_count}\")\nprint(f\"Time Elapsed: {elapsed_time:.2f} seconds\")\nprint(f\"Tokens per Second: {tokens_per_second:.2f}\")\n</code></pre>"},{"location":"labs/calculating-tokens-per-second/#result","title":"Result","text":"<p>What a fascinating question!</p> <p>To tackle this, let's break down the process into manageable steps. Here's a suggested approach:</p>"},{"location":"labs/calculating-tokens-per-second/#step-1-identify-key-topics-in-deep-learning","title":"Step 1: Identify Key Topics in Deep Learning","text":"<p>At a high level, deep learning is a subset of machine learning that focuses on complex models inspired by the structure and function of the brain. The field encompasses various techniques for tasks such as image classification, object detection, speech recognition, natural language processing, and more.</p> <p>Some key topics to consider when identifying important concepts include:</p> <ol> <li>Neural network architectures (e.g., convolutional neural networks, recurrent neural networks)</li> <li>Deep learning frameworks (e.g., TensorFlow, PyTorch)</li> <li>Optimization algorithms (e.g., stochastic gradient descent, Adam)</li> <li>Regularization techniques (e.g., dropout, L1 and L2 regularization)</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#step-2-consider-mathematical-foundations","title":"Step 2: Consider Mathematical Foundations","text":"<p>Deep learning relies heavily on mathematical concepts from linear algebra, calculus, probability theory, and statistics.</p> <p>Some essential math concepts to cover include:</p> <ol> <li>Vector spaces and transformations</li> <li>Matrix operations (e.g., multiplication, inversion)</li> <li>Eigenvalue decomposition and singular value decomposition</li> <li>Probability distributions (e.g., Gaussian, Bernoulli)</li> <li>Expectation and variance</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#step-3-explore-practical-aspects-of-deep-learning","title":"Step 3: Explore Practical Aspects of Deep Learning","text":"<p>This includes the implementation, evaluation, and deployment of deep learning models.</p> <p>Some important practical concepts to consider:</p> <ol> <li>Data preprocessing and augmentation techniques</li> <li>Model selection and hyperparameter tuning</li> <li>Overfitting and regularization strategies</li> <li>Model interpretability and explainability</li> <li>Deployment and scalability in production environments</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#step-4-review-advanced-topics-and-emerging-trends","title":"Step 4: Review Advanced Topics and Emerging Trends","text":"<p>As the field of deep learning continues to evolve, it's essential to stay up-to-date with recent developments.</p> <p>Some advanced topics and emerging trends include:</p> <ol> <li>Transfer learning and pre-trained models</li> <li>Adversarial attacks and defenses</li> <li>Graph neural networks and graph-based techniques</li> <li>Attention mechanisms and transformer architectures</li> <li>Quantum computing and its potential applications in deep learning</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#step-5-refine-the-list-of-important-concepts","title":"Step 5: Refine the List of Important Concepts","text":"<p>Considering the topics identified above, let's prioritize them based on their relevance to a college-level course on deep learning.</p> <p>Here are the top 50 most important concepts:</p>"},{"location":"labs/calculating-tokens-per-second/#mathematical-foundations-1-10","title":"Mathematical Foundations (1-10)","text":"<ol> <li>Vector spaces and transformations</li> <li>Matrix operations (e.g., multiplication, inversion)</li> <li>Eigenvalue decomposition and singular value decomposition</li> <li>Probability distributions (e.g., Gaussian, Bernoulli)</li> <li>Expectation and variance</li> <li>Calculus basics (e.g., gradients, Hessians)</li> <li>Linear algebra review (e.g., determinants, eigenvectors)</li> <li>Probability and statistics review</li> <li>Optimization algorithms (e.g., gradient descent)</li> <li>Regularization techniques (e.g., L1, L2)</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#neural-network-architectures-11-20","title":"Neural Network Architectures (11-20)","text":"<ol> <li>Convolutional neural networks (CNNs)</li> <li>Recurrent neural networks (RNNs)</li> <li>Long short-term memory (LSTM) networks</li> <li>Gated recurrent units (GRUs)</li> <li>Residual connections and skip connections</li> <li>Autoencoders and variational autoencoders</li> <li>U-Net architectures for image segmentation</li> <li>Transformers and self-attention mechanisms</li> <li>Graph neural networks (GNNs)</li> <li>Other specialized architectures (e.g., capsule networks)</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#deep-learning-frameworks-and-tools-21-25","title":"Deep Learning Frameworks and Tools (21-25)","text":"<ol> <li>TensorFlow and Keras APIs</li> <li>PyTorch and Lightning-PyTorch</li> <li>Deep learning frameworks for GPU acceleration (e.g., cuDNN)</li> <li>Model serving and deployment tools (e.g., Docker, Kubernetes)</li> <li>Deep learning software development kits (SDKs) and libraries</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#practical-aspects-of-deep-learning-26-35","title":"Practical Aspects of Deep Learning (26-35)","text":"<ol> <li>Data preprocessing and augmentation techniques</li> <li>Model selection and hyperparameter tuning</li> <li>Overfitting and regularization strategies</li> <li>Model interpretability and explainability</li> <li>Deployment and scalability in production environments</li> <li>Model evaluation metrics (e.g., accuracy, precision)</li> <li>Common pitfalls and debugging techniques</li> <li>Data efficiency and transfer learning strategies</li> <li>Regularization techniques for large models</li> <li>Distributed training and parallelization</li> </ol>"},{"location":"labs/calculating-tokens-per-second/#advanced-topics-and-emerging-trends-36-50","title":"Advanced Topics and Emerging Trends (36-50)","text":"<ol> <li>Transfer learning and pre-trained models</li> <li>Adversarial attacks and defenses</li> <li>Graph neural networks and graph-based techniques</li> <li>Attention mechanisms and transformer architectures</li> <li>Quantum computing and its potential applications in deep learning</li> <li>Exponential family distributions and link functions</li> <li>Causal inference and counterfactual reasoning</li> <li>Generative models (e.g., GANs, VAEs)</li> <li>Time series analysis with LSTM networks</li> <li>Text classification and sentiment analysis</li> <li>Image recognition and object detection</li> <li>Speech recognition and natural language processing</li> <li>Reinforcement learning and deep Q-networks (DQN)</li> <li>Multi-agent systems and distributed decision-making</li> <li>Explainability techniques for complex models</li> </ol> <p>Of course, this list is not exhaustive, but it should give you a solid starting point for creating a comprehensive college-level course on deep learning.</p> <p>How's that?</p>"},{"location":"labs/calculating-tokens-per-second/#inference-statistics","title":"Inference Statistics","text":"<p>Token Count: 687 Time Elapsed: 13.33 seconds Tokens per Second: 51.55</p> <p>Note</p> <p>Depending on your model's tokenization, you might need a more precise token counter   (e.g., using the <code>tiktoken</code> library for models like GPT).</p>"},{"location":"labs/calculating-tokens-per-second/#model-metadata","title":"Model Metadata","text":"<p>Knowing about the structure of a model is key to understanding its performance.</p> <p>Here is the information that ollama provided about deepseek-r1:</p> <pre><code>$ ollama show deepseek-r1\n  Model                          \n    arch                qwen2            \n    parameters          7.6B             \n    quantization        Q4_K_M           \n    context length      131072           \n    embedding length    3584             \n</code></pre> <p>Let's do a deep dive into each of these model metadata fields.</p>"},{"location":"labs/calculating-tokens-per-second/#1-arch-architecture","title":"1.  arch (Architecture):","text":"<ul> <li>What it means: This parameter indicates the underlying neural network architecture on which the model is based.</li> <li>In this case: The model uses the qwen2 architecture. This tells you which design or blueprint the model follows (e.g., similar to transformer-based architectures like GPT or BERT), which influences how it processes input data and generates responses.</li> </ul>"},{"location":"labs/calculating-tokens-per-second/#2-parameters-number-of-parameters","title":"2.  parameters (Number of Parameters):","text":"<ul> <li>What it means: This shows the total number of learnable weights (and biases) in the model. The size of this number is often used as a rough proxy for the model's capacity to learn and represent complex patterns.</li> <li>In this case: The model has 7.6B (7.6 billion) parameters. More parameters generally mean a higher capacity model, though they also require more memory and computational resources during inference.</li> </ul>"},{"location":"labs/calculating-tokens-per-second/#3-quantization","title":"3.  quantization:","text":"<ul> <li>What it means: Quantization refers to reducing the numerical precision of the model's parameters. This process converts high-precision weights (e.g., 32-bit floats) into lower-precision representations (e.g., 4-bit integers) to reduce model size and speed up computations with a minimal loss in accuracy.</li> <li>In this case: The value Q4_K_M indicates that a 4-bit quantization scheme is used. The \"Q4\" part tells you that the weights are represented with 4-bit precision, and \"K_M\" likely refers to the specific quantization method or variant implemented. This balance helps the model run more efficiently while retaining as much performance as possible.</li> </ul>"},{"location":"labs/calculating-tokens-per-second/#4-context-length","title":"4.  context length:","text":"<ul> <li>What it means: This parameter defines the maximum number of tokens the model can process in a single input prompt (or conversation). In transformer-based models, the context length determines how much text the model can consider at one time.</li> <li> <p>In this case: The model can handle a context of up to 131072 tokens. This is an exceptionally long context compared to most language models, which typically support only a few thousand tokens. It enables the model to process very large documents or maintain extended conversations.</p> </li> <li> <p>embedding length:</p> </li> <li> <p>What it means: This is the size (or dimensionality) of the vector used to represent each token in the model's internal computations. In other words, every token in the input is mapped to a vector of this length, which the model uses to capture semantic and syntactic information.</p> </li> <li>In this case: An embedding length of 3584 means that each token is converted into a 3584-dimensional vector. A higher embedding dimension can allow for richer representations but also increases the model's computational complexity.</li> </ul>"},{"location":"labs/calculating-tokens-per-second/#summary","title":"Summary","text":"<ul> <li>Architecture (arch): Defines the model's design (here, qwen2).</li> <li>Parameters: Indicates the model's size in terms of learnable weights (7.6B parameters).</li> <li>Quantization: Shows how the model's weights are stored (using 4-bit precision with the specific scheme Q4_K_M).</li> <li>Context Length: The maximum number of tokens the model can process at once (131072 tokens).</li> <li>Embedding Length: The dimensionality of token representations within the model (3584).</li> </ul> <p>Each of these parameters provides insight into the model's design, capacity, efficiency, and the scale at which it can process input data.</p>"},{"location":"labs/mnist/","title":"MINST","text":"<p>The MNIST (Modified National Institute of Standards and Technology) dataset is the widely-used dataset of handwritten digits that serves as a standard  benchmark for building and testing machine learning models, particularly deep learning models. It contains 60,000 training images  and 10,000 testing images of handwritten digits from 0-9, with each  image being 28x28 pixels in grayscale. </p> <p>The MNIST dataset is often used as a \"Hello World\" project for getting  started with deep learning and neural networks.</p> <p>Here's a detailed walkthrough of building a deep learning model using the MNIST dataset:</p> <p>Project Title: MNIST Handwritten Digit Classification Time Required: 2-3 hours Difficulty Level: Beginner-Intermediate</p>"},{"location":"labs/mnist/#introduction","title":"Introduction","text":"<p>The MNIST project is a fundamental introduction to deep learning where we build a neural network to recognize handwritten digits. This project demonstrates core concepts like data preprocessing, model architecture, training, and evaluation.</p> <p> Sample of MNIST images of grayscale digits (0-9)</p>"},{"location":"labs/mnist/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.x</li> <li>TensorFlow or PyTorch</li> <li>NumPy</li> <li>Matplotlib</li> <li>Basic understanding of neural networks</li> </ul>"},{"location":"labs/mnist/#steps","title":"Steps","text":""},{"location":"labs/mnist/#step-1-setup-conda-environment-and-libs","title":"Step 1: Setup Conda Environment and Libs","text":"<ol> <li>Create a conda environment called \"mnist\" and activate it</li> <li>Import the MNIST dataset through your chosen deep learning framework</li> </ol> <p>Here is a sample UNIX shell script to setup your conda environment and load the appropriate python libraries.</p> <pre><code>conda deactivate\nconda create --name mnist python=3.10\nconda activate mnist\nconda install matplotlib\nconda install numpy\nconda install seaborn\nconda install scikit-learn\n</code></pre>"},{"location":"labs/mnist/#step-2-data-profiling","title":"Step 2: Data Profiling","text":"<p>Data Profiling is the process of understanding what data you have as the distribution of data.  Having a good understanding of your input data is a key aspect to building a useful deep learning model.</p> <p>Without going into too much detail, here is a summary of the MNIST dataset</p> <p>The MNIST dataset contains:    - 60,000 training images (28x28 pixels each)    - 10,000 testing images    - Labels from 0-9 for each image</p> <p> Bar chart showing the distribution of digits in training set to demonstrate data balance Digit Distribution Python Code</p>"},{"location":"labs/mnist/#step-3-data-preprocessing","title":"Step 3: Data Preprocessing","text":"<ol> <li>Normalize pixel values from [0, 255] to [0, 1]</li> <li>Reshape images from 28x28 to 784-dimensional vectors</li> <li>Convert labels to one-hot encoded format</li> <li>Split training data into training and validation sets (80-20 split)</li> </ol> <p>In MNIST digit classification, each target label (0-9) is encoded as a 10-dimensional binary vector: the digit '3' becomes [0,0,0,1,0,0,0,0,0,0], where only the fourth position (index 3) contains a 1.</p> <p> MNIST Data preprocessing steps, showing original image \u2192 normalized image \u2192 flattened vector Mermaid Flowchart</p>"},{"location":"labs/mnist/#step-4-model-architecture","title":"Step 4: Model Architecture","text":"<p>Build a simple neural network with:</p> <ol> <li>Input layer (784 neurons)</li> <li>Hidden layer 1 (128 neurons, ReLU activation)</li> <li>Hidden layer 2 (64 neurons, ReLU activation)</li> <li>Output layer (10 neurons, softmax activation)</li> </ol> <p> Network architecture diagram showing layers, connections, and dimensions.</p>"},{"location":"labs/mnist/#step-5-model-training","title":"Step 5: Model Training","text":"<p>Configure training parameters:</p> <ul> <li>Loss function: Categorical cross entropy</li> <li>Optimizer: Adam</li> <li>Learning rate: 0.001</li> <li>Batch size: 32</li> <li>Epochs: 10</li> </ul>"},{"location":"labs/mnist/#step-6-train-the-model-while-monitoring","title":"Step 6: Train the model while monitoring:","text":"<ul> <li>Training accuracy</li> <li>Validation accuracy</li> <li>Training loss</li> <li>Validation loss</li> </ul> <p>[FIGURE DESCRIPTION: Two line plots showing training/validation accuracy and loss over epochs]</p>"},{"location":"labs/mnist/#step-7-model-evaluation","title":"Step 7: Model Evaluation","text":"<ol> <li>Evaluate model on test set</li> <li>Generate confusion matrix</li> <li>Calculate key metrics:<ol> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1-score</li> </ol> </li> </ol> <p>[FIGURE DESCRIPTION: Confusion matrix heatmap showing prediction results across all digits]</p>"},{"location":"labs/mnist/#step-8-making-predictions","title":"Step 8: Making Predictions","text":"<ol> <li>Select random test images</li> <li>Make predictions</li> <li>Display results with:<ul> <li>Original image</li> <li>Predicted digit</li> <li>Confidence score</li> </ul> </li> </ol> <p>[FIGURE DESCRIPTION: Grid of sample predictions showing original images, predicted digits, and confidence scores]</p>"},{"location":"labs/mnist/#common-challenges-and-solutions","title":"Common Challenges and Solutions","text":"<ol> <li> <p>Overfitting</p> <ul> <li>Solution: Add dropout layers</li> <li>Solution: Use data augmentation</li> </ul> </li> <li> <p>Poor Convergence</p> <ul> <li>Solution: Adjust learning rate</li> <li>Solution: Modify batch size</li> </ul> </li> <li> <p>Low Accuracy</p> <ul> <li>Solution: Deepen network</li> <li>Solution: Increase training time</li> </ul> </li> </ol>"},{"location":"labs/mnist/#tips-for-improvement","title":"Tips for Improvement","text":"<ol> <li> <p>Experiment with different architectures:</p> <ul> <li>Convolutional Neural Networks (CNNs)</li> <li>Different numbers of layers</li> <li>Various activation functions</li> </ul> </li> <li> <p>Try data augmentation:</p> <ul> <li>Random rotations</li> <li>Slight shifts</li> <li>Small zooms</li> </ul> </li> <li> <p>Implement regularization:</p> <ul> <li>Dropout</li> <li>L1/L2 regularization</li> <li>Batch normalization</li> </ul> </li> </ol>"},{"location":"labs/mnist/#project-extensions","title":"Project Extensions","text":"<ol> <li>Build a web interface for real-time digit recognition</li> <li>Implement different model architectures (CNN, ResNet)</li> <li>Create a mobile app for digit recognition</li> <li>Compare performance across different frameworks</li> </ol> <p>[FIGURE DESCRIPTION: Comparison chart showing accuracy vs. complexity for different model architectures]</p>"},{"location":"labs/mnist/#expected-results","title":"Expected Results","text":"<ul> <li>Training accuracy: &gt;98%</li> <li>Test accuracy: &gt;97%</li> <li>Training time: ~5 minutes on CPU</li> <li>Model size: ~5MB</li> </ul> <p>This project serves as an excellent introduction to deep learning, covering essential concepts while providing a solid foundation for more complex projects. The visual nature of the problem makes it easy to understand and debug, while the relatively small dataset size allows for quick experimentation with different approaches.</p>"},{"location":"labs/mnist/#references","title":"References","text":"<p>Claude Transcript</p>"},{"location":"prompts/","title":"Generative AI Prompts","text":"<p>Course Description</p>"},{"location":"prompts/02-bloom-taxonomy-description/","title":"Bloom Taxonomy Description","text":"<p>Prompt</p> <p>Give the course description, apply the Bloom Taxonomy to rewrite the course description into a set of precise levels.</p> <p>Leverage Bloom\u2019s Taxonomy to structure the course, detailing the knowledge, skills, and applications students will develop at each cognitive level (remember, understand, apply, analyze, evaluate, and create).</p> <p>In the description, highlight how generative AI will serve as a primary teaching tool in the following ways:</p> <p>Metaphors and Analogies: Explain how generative AI will dynamically create metaphors that help students understand complex circuits and system concepts intuitively, for example, comparing [XXX] to [YYY] different contexts.</p> <p>Stories and Lessons: Illustrate how AI-generated narratives will guide students through historical milestones in circuit design, with characters like pioneering engineers or scientists to add context and enhance engagement.</p> <p>Content Generation: Describe how AI will develop interactive content\u2014such as real-time quizzes, customized practice problems, and detailed walkthroughs of circuit diagrams\u2014that align with students' progress and areas of improvement.</p> <p>Simulations and Animations: Outline how AI-powered simulations will allow students to interact with virtual circuit models, adjusting parameters and observing outcomes in real-time, and how animations will visually represent phenomena like current flow and voltage changes.</p> <p>Learning Activities and Projects: Explain how generative AI will generate scaffolded project suggestions based on Bloom\u2019s higher-order objectives, like creating their own circuit designs or troubleshooting complex systems. AI should provide formative feedback throughout each step to foster critical thinking.</p> <p>Assessment and Feedback: Describe how AI-driven assessments will go beyond traditional grading, providing personalized feedback loops that prompt self-reflection, track learning progress, and guide students through iterative problem-solving exercises.</p> <p>The course description should aim to inspire curiosity and convey how AI enhances interactivity and personalizes the learning journey, supporting students in achieving a deep and practical understanding of circuits and systems.</p> <p>Give the course description, apply the Bloom Taxonomy to rewrite the course description into a set of precise levels.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-description","title":"Course Description","text":"<p>Organization: University of Minnesota Department of Electrical Engineering and Computer Design (EECD) Course Identifier: EE 4940 Course Title: : Introduction to Deep Learning Offering Date: Spring 2025</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-instructor","title":"Course Instructor","text":"<p>Sang-Hyun Oh, Professor of ECE, graduate faculty in neuroscience, physics, biomedical eng. Email: sang@umn.edu Office: 5-119 Keller Hall Teaching Assistant: Ike Goetzman (PhD candidate in ECE; email: goetz219@umn.edu) Lecture (Jan 22 \u2013 May 5, 2024): Monday/Wednesday 11:15am-12:30pm @ Appleby Hall 3</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-summary","title":"Course Summary","text":"<p>Machine learning, especially deep learning based on neural networks, has shown transformative impacts across life sciences, engineering, and medicine, as recently highlighted by the 2024 Nobel Prizes: </p> <ul> <li>The Physics Nobel was awarded to John Hopfield and Geoffrey Hinton for pioneering contributions to neural networks that enable machine learning</li> <li>Demis Hassabis, John Jumper, and David Baker received the Chemistry Nobel for advancing computational protein design.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#course-overview","title":"Course Overview","text":"<p>EE 4940: Introduction to Deep Learning aims to provide students with both fundamental and applied perspectives on these advances. Beginning with an overview of AI, the course introduces deep neural networks (DNNs) through practical, hands-on examples and guides students through specialized neural network architectures, including</p> <ul> <li>convolutional neural networks (CNNs) for diagnostic applications and </li> <li>recurrent neural networks (RNNs) for time series analysis.</li> </ul> <p>Emphasis is placed on applying neural networks to tackle real-world problems in classification, regression, image analysis, and time-series prediction. Scientific applications are demonstrated as well, such as using DNNs to solve equations.</p> <p>The course concludes with dimensionality reduction techniques, followed by advanced topics such as generative AI, and an introduction to reinforcement learning. </p> <p>Emphasis will be on practical experience with Python-based coding over mathematical theory.</p> <p>By the end of the course, students will have gained familiarity with Python-driven deep learning, hands-on skills, and a working knowledge of deep learning\u2019s potential for a wide range of problems.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-prerequisites","title":"Course Prerequisites","text":"<p>EE 4940 - Special Topics in Electrical and Computer Engineering is recommended but not required.</p> <p>This course is designed to be accessible and introductory, with essential concepts and coding skills introduced as needed.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#expected-workload","title":"Expected Workload","text":"<p>This course (EE 4940) is a 3-credit, lecture format course. The expected workload for this course this semester is, on average, 45 hours per credit, or just over 3 hours per credit hour per week (including lecture). The total workload of 9 hours per week.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-goals-and-objectives","title":"Course Goals and Objectives","text":"<p>EE 4940 aims to equip students with a foundation in deep learning, focusing on core architectures like deep neural networks (DNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).</p> <p>Through hands-on applications, students will learn to apply these models to solve practical problems using real-world datasets, such as cancer diagnostics and time-series analysis. The primary goal of the course is to develop students's ability to implement and troubleshoot neural network models, gaining some practical experiences, and be aware of potential pitfalls/artifacts while using DNNs.</p> <p>By the course's end, students will have a working knowledge of AI\u2019s role in scientific problem-solving, dimensionality reduction techniques, and a simple introduction to generative models and reinforcement learning (RL).</p>"},{"location":"prompts/02-bloom-taxonomy-description/#topics-to-be-covered","title":"Topics To Be Covered","text":""},{"location":"prompts/02-bloom-taxonomy-description/#1-introduction-to-ai-and-deep-learning","title":"1. Introduction to AI and deep learning","text":"<ul> <li>Brief history of AI, neural networks and recent Nobel-winning breakthroughs in physics and chemistry</li> <li>Course setup, tools, and brief introduction to Python for deep learning</li> <li>The universality theorem and the manifold hypothesis</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#2-fundamentals-of-deep-neural-networks-dnns","title":"2. Fundamentals of Deep Neural Networks (DNNs)","text":"<ul> <li>Architecture of DNNs: layers, activation functions, and backpropagation</li> <li>Training, validation, and testing sets;</li> <li>Loss functions, gradient descent, mini-batches</li> <li>Hands-on coding</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#3-convolutional-neural-networks-cnns-applications-in-diagnostic-imaging","title":"3. Convolutional Neural Networks (CNNs) - applications in diagnostic imaging","text":"<ul> <li>CNN architecture</li> <li>Applying CNNs, with MHIST as a case study for cancer diagnosis</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#4-recurrent-neural-networks-rnns-and-time-series-analysis","title":"4. Recurrent Neural Networks (RNNs) and time series analysis","text":"<ul> <li>Introduction to RNNs, LSTMs, for sequency data analysis.</li> <li>Hands-on practice with real-world data.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#5-chips-and-ai","title":"5. Chips and AI","text":"<p>Briefly explore AI chips, neuromorphic devices, and future directions.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#6-ai-for-scientific-problem-solving-using-neural-networks-to-approximate-solutions-for","title":"6. AI for scientific problem solving: Using neural networks to approximate solutions for","text":"<p>Maxwell\u2019s equations</p>"},{"location":"prompts/02-bloom-taxonomy-description/#7-dimensionality-reduction-and-autoencoders","title":"7. Dimensionality reduction and autoencoders","text":""},{"location":"prompts/02-bloom-taxonomy-description/#8-advanced-topics","title":"8. Advanced topics","text":""},{"location":"prompts/02-bloom-taxonomy-description/#course-format","title":"Course Format","text":"<ul> <li>Classroom lectures only</li> <li>Canvas LMS link will be provided</li> <li>Textbook and Required Materials: none</li> <li>Based on lecture notes (uploaded in canvas 1-2 days before class)</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#homework-assignments-and-mini-projects","title":"Homework Assignments and \u201cMini-projects\u201d:","text":"<ul> <li>Regularly-scheduled weekly or biweekly homework assignments and 4 multi-week \u201cmini-projects.\u201d The former will be designed to exercise basic concepts and modest extensions of the technical or computational tools introduced in class; the latter will be crafted to allow students to take a \u201cdeeper dive\u201d by applying techniques developed in the course to data sets arising in various aspects of ECE and life sciences.</li> <li>You are allowed (in fact, encouraged) to work with others when solving the homework and mini project problems. However, solutions submitted for grading should be formulated by each student independently! Late homework and mini-project submissions will be accepted, but without a legitimate (e.g., illness-based) justification, will result in a maximum grade of 80% for that assignment. Specifically, unjustified late assignments will be graded according to the same rubric as on-time assignments, but the corresponding score will be multiplied by 0.8 when entered into the gradebook.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#exams","title":"Exams","text":"<p>There will be one midterm exam and one final exam. The midterm exam will be open book/notes. The final exam will be open book/notes and will be due no earlier than the final exam time specified by the Registrar. Either or both may be take-home exams.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#grading","title":"Grading","text":"<p>The grade distributions for the course variations are provided below.</p> <p>Note: distributions for homework assignments and mini-projects are total; individual submissions will be equally weighted.</p> Deliverable Weight Homework Assignments 40% Mini-projects 40% Midterm Exam 10% Final Exam 10%"},{"location":"prompts/02-bloom-taxonomy-description/#software-requirements","title":"Software Requirements","text":"<p>Through the homework assignments and mini-projects, students will be required to perform a number of computer exercises to become familiar with the computational and practical aspects of deep learning. This will typically entail utilization of Python, often in concert with existing toolboxes such as NumPy, SciPy, Matplotlib, Scikit-learn, and PyTorch (or TensorFlow), all of which are freely available for Windows, Mac, and Linux operating systems.</p> <p>As a perhaps preferred alternative, students may wish to perform all (Python) programming computations using a free cloud-based account (i.e., Google Colab), which will be discussed and established at the beginning of the semester. Large-scale deep learning computations necessary for the final \u201cmini-project\u201d will utilize these resources.</p> <p>Note</p> <p>The Required University Policy Statements were provided in the original PDF of the course description.  They have been removed to allow the generative AI tools to focus on the concept enumeration.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#result","title":"Result","text":"<p>Chat GPT o1 Thought about deep learning course structure for 42 seconds</p> <p>Below is a revised course description for EE 4940: Introduction to Deep Learning structured according to Bloom's Taxonomy. Each level highlights the knowledge, skills, and applications students will develop, as well as how generative AI will serve as a primary teaching tool---through metaphor/analogy generation, narrative stories, adaptive content creation, real-time simulations, scaffolded projects, and personalized assessment.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#course-information","title":"Course Information","text":"<ul> <li>Organization: University of Minnesota Department of Electrical Engineering and Computer Design (EECD)</li> <li>Course Identifier: EE 4940</li> <li>Course Title: Introduction to Deep Learning</li> <li>Offering Date: Spring 2025</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#instructor-and-teaching-assistant","title":"Instructor and Teaching Assistant","text":"<ul> <li>Instructor:     Sang-Hyun Oh, Professor of ECE (graduate faculty in neuroscience, physics, biomedical engineering)     Email: sang@umn.edu Office: 5-119 Keller Hall</li> <li>Teaching Assistant:     Ike Goetzman (PhD candidate in ECE)     Email: goetz219@umn.edu</li> <li>Lecture Schedule (Jan 22 -- May 5, 2024):     Monday/Wednesday 11:15am-12:30pm @ Appleby Hall 3</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#course-summary_1","title":"Course Summary","text":"<p>Machine learning---particularly deep learning---is transforming fields from life sciences to engineering and medicine. Inspired by recent Nobel Prize recognitions (e.g., John Hopfield and Geoffrey Hinton in Physics for neural network breakthroughs, Demis Hassabis, John Jumper, and David Baker in Chemistry for computational protein design), this course provides an introductory yet hands-on exploration of deep neural networks (DNNs), covering CNNs for image analysis, RNNs for time-series data, dimensionality reduction, an introduction to generative AI, and a glimpse of reinforcement learning.</p> <p>Students will gain practical coding experience in Python with toolkits such as PyTorch or TensorFlow, applying deep learning techniques to real-world datasets (e.g., cancer diagnostics, time-series forecasting).</p>"},{"location":"prompts/02-bloom-taxonomy-description/#prerequisites-format-and-workload","title":"Prerequisites, Format, and Workload","text":"<ul> <li>Prerequisites: EE 4940 - Special Topics in Electrical and Computer Engineering recommended but not required.</li> <li>Format: In-person lectures; no required textbook---lectures and notes via Canvas LMS.</li> <li>Workload: 3-credit lecture course (~9 hours/week total).<ul> <li>Weekly/biweekly homework.</li> <li>4 \"mini-projects\" involving deeper applications of course concepts.</li> <li>One midterm exam (open book/notes) and one final exam (open book/notes).</li> </ul> </li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#course-goals-and-objectives_1","title":"Course Goals and Objectives","text":"<p>By the end of the course, students will:</p> <ul> <li>Have a working knowledge of deep neural network architectures (DNNs, CNNs, RNNs).</li> <li>Understand training, validation, and testing processes, gradient descent, backpropagation.</li> <li>Apply models to diagnostic imaging, time-series forecasting, and other real-world problems.</li> <li>Explore dimensionality reduction and see how DNNs can solve scientific problems (e.g., approximating solutions to Maxwell's equations).</li> <li>Gain a basic introduction to generative AI and reinforcement learning.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#blooms-taxonomy-levels-and-the-role-of-generative-ai","title":"Bloom's Taxonomy Levels and the Role of Generative AI","text":"<p>Below is how this course---and its use of generative AI---aligns with each cognitive level of Bloom's Taxonomy: Remember, Understand, Apply, Analyze, Evaluate, and Create.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#1-remember","title":"1. Remember","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Recall fundamental definitions of artificial intelligence, neural networks, activation functions, backpropagation, and key historical milestones (e.g., Hopfield networks, Hinton's contributions).</li> <li>Identify standard Python libraries and deep learning frameworks (NumPy, SciPy, Matplotlib, PyTorch, TensorFlow).</li> </ul> <p>Generative AI in Action</p> <ul> <li>Metaphors and Analogies: The AI tool will generate simple, memorable analogies (e.g., \"A neural network layer is like a network of interconnected 'filters'---just like the lenses in glasses that each focus on a different part of the scene.\"). These real-time analogies help students quickly remember and anchor new terminology.</li> <li>Content Generation: AI-powered flashcards and micro-quizzes will be automatically created to help students memorize key terms, layer types, and function names.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#2-understand","title":"2. Understand","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Interpret how DNNs learn internal representations of data.</li> <li>Summarize the logic behind training/validation/test splits and how loss functions guide model optimization.</li> <li>Explain the significance of convolutional filters in CNNs and gating mechanisms in RNNs.</li> </ul> <p>Generative AI in Action</p> <ul> <li>Stories and Lessons: AI-generated narratives and role-play scenarios (e.g., \"Geoffrey Hinton explains to a young Demis Hassabis how hidden layers revolutionized speech recognition\") immerse students in historical contexts. These stories make key concepts in deep learning more relatable, fostering deeper understanding.</li> <li>Real-Time Clarifications: When students pose questions, AI can craft new explanations or analogies on the spot---ensuring that confusing topics are re-framed until fully understood.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#3-apply","title":"3. Apply","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Use Python and popular libraries to build, train, and test basic DNN architectures.</li> <li>Implement CNNs for image-based classification tasks and RNNs for time-series predictions.</li> <li>Employ data preprocessing techniques (normalization, reshaping) and iterative model refinement in mini-projects.</li> </ul> <p>Generative AI in Action</p> <ul> <li>Content Generation: Interactive practice problems and short coding challenges adapt to each student's progress. If a learner struggles with CNN kernel concepts, AI will generate targeted exercises.</li> <li>Simulations and Animations: Students use AI-driven virtual labs to manipulate hyperparameters and watch real-time changes in learning curves, explore \"zoomed-in\" views of convolutional filters, or tweak RNN cell parameters to see how model predictions shift---bridging theory and practice.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#4-analyze","title":"4. Analyze","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Compare different network architectures (DNN, CNN, RNN) for specific applications, examining performance metrics (accuracy, precision, recall, F1 scores).</li> <li>Investigate model failures or anomalies, diagnosing potential issues like overfitting or underfitting.</li> <li>Break down real-world datasets (e.g., MHIST for cancer diagnosis) to identify patterns and design suitable learning strategies.</li> </ul> <p>Generative AI in Action</p> <ul> <li>Learning Activities and Projects: AI proposes incremental \"mini-projects\" that require critical data analysis---such as comparing performance across multiple model architectures or investigating feature maps in CNNs. The AI offers real-time hints, ensuring students learn systematic analysis methods.</li> <li>Adaptive Feedback: As students debug their models, AI-powered suggestions highlight where and why training might be failing, prompting reflection on data splits, loss function choices, or architectural mismatches.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#5-evaluate","title":"5. Evaluate","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Assess the reliability, interpretability, and ethical considerations of deep learning solutions.</li> <li>Critique model architecture decisions, optimization techniques, and the trade-offs between computational cost and accuracy.</li> <li>Determine when alternative approaches (e.g., simpler regressions or decision trees) might be more suitable than deep networks.</li> </ul> <p>Generative AI in Action</p> <ul> <li>Assessment and Feedback: AI-driven grading provides personalized feedback loops that go beyond numeric scores. Students receive suggestions (e.g., \"Try re-evaluating your dropout strategy. Are you balancing variance and bias properly?\"), enabling them to evaluate and refine their approach.</li> <li>Peer Review Simulations: Generative AI simulates a peer-review environment, providing constructive critiques of project proposals or final results---students practice giving and receiving evaluations in a supportive, simulated setting.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#6-create","title":"6. Create","text":"<p>Knowledge &amp; Skills</p> <ul> <li>Design and implement original neural network architectures for novel data domains.</li> <li>Experiment with generative models (e.g., Variational Autoencoders or Generative Adversarial Networks) and propose new applications for them.</li> <li>Formulate and conduct reinforcement learning experiments for simple control or decision-making problems.</li> </ul> <p>Generative AI in Action</p> <ul> <li>Learning Activities and Projects: The AI tool auto-generates scaffolded project suggestions that encourage students to develop new solutions---\"Propose a CNN variant for biomedical image segmentation,\" or \"Design an RNN-based text predictor with user-defined constraints.\"</li> <li>Formative Feedback: Throughout the creative process, the AI continually checks code, results, and design decisions, prompting deeper critical thinking. This fosters an iterative cycle of hypothesis-testing and refinement---helping students truly create and innovate.</li> </ul>"},{"location":"prompts/02-bloom-taxonomy-description/#how-generative-ai-personalizes-the-learning-journey","title":"How Generative AI Personalizes the Learning Journey","text":"<ol> <li> <p>Metaphors and Analogies</p> <ul> <li>Ongoing generation of simple and relatable comparisons for abstract concepts, ensuring students never get stuck on jargon.</li> <li> <p>Stories and Lessons</p> </li> <li> <p>AI-developed narratives infuse historical milestones (Hopfield nets, Hinton's backpropagation breakthroughs, Hassabis's achievements) with character-driven storytelling.</p> </li> <li> <p>Content Generation</p> </li> <li> <p>Adaptive quizzes, customized practice problems, and circuit-like diagram walk-throughs (adapted to deep learning diagrams) grow alongside the student's evolving competence.</p> </li> <li> <p>Simulations and Animations</p> </li> <li> <p>AI-powered interfaces let students see real-time \"inside\" changes as a model learns---activations, gradients, confusion matrices---supporting interactive exploration.</p> </li> <li> <p>Learning Activities and Projects</p> </li> <li> <p>Scaffolded tasks guide novices from basic classification examples to more advanced challenges---like building an autoencoder for dimensionality reduction or a basic reinforcement learning agent.</p> </li> <li> <p>Assessment and Feedback</p> </li> <li> <p>Detailed, iterative feedback loops ensure students understand why a concept or code block fails, equipping them to self-reflect, iterate, and improve.</p> </li> </ul> </li> </ol>"},{"location":"prompts/02-bloom-taxonomy-description/#grading-overview","title":"Grading Overview","text":"Deliverable Weight Homework Assignments 40% --- --- Mini-projects 40% Midterm Exam 10% Final Exam 10% <p>Note: Late submissions (without legitimate justification) receive up to 80% credit. Exams are open-book/notes and may include take-home components.</p>"},{"location":"prompts/02-bloom-taxonomy-description/#conclusion","title":"Conclusion","text":"<p>By integrating Bloom's Taxonomy with generative AI-powered tools, EE 4940: Introduction to Deep Learning aspires to spark curiosity and deep engagement. Students will remember core principles, understand architectural choices, apply techniques to real data, analyze model results, evaluate ethical and practical considerations, and ultimately create new solutions in the rapidly evolving field of deep learning. This comprehensive and interactive approach---fueled by dynamic AI metaphors, narratives, simulations, and personalized feedback---ensures that each learner can explore and master the foundational and forward-looking aspects of modern neural network technology.</p> <p>o1</p> <p>O</p>"},{"location":"prompts/03-concept-enumeration/","title":"Concept Enumeration","text":"<p>Prompt</p> <p>Please generate a list of the 200 core concepts that need to be covered in this course and the order in which they should be taught.</p> <p>Return numbered list of these core concepts using a single line per concept.  For each concept, just return a short concept name and use acronyms or abbreviations when appropriate.  They will be used as labels in a graph network. Make sure that the concepts are distinct not duplicated in the list.</p> <p>Use title case for the concept names.</p> <p>Start with the most basic foundational concepts and then proceed to the most complex concepts.</p>"},{"location":"prompts/03-concept-enumeration/#workflow","title":"Workflow","text":"<p>Setting the stage</p> <p>I'm compiling a list of 200 unique core concepts for a deep learning course. The order will start with basic concepts and gradually advance to more complex ones.</p> <p>Expanding the list</p> <p>I'm building upon the course outline to include topics like generative AI and RL, ensuring each is distinct and enumerated correctly while avoiding duplicates.</p> <p>Organizing topics</p> <p>Structuring the course into categories like basic foundations, Python tools, and advanced topics such as generative AI and domain-specific architectures to ensure a systematic progression.</p> <p>Finalizing concepts</p> <p>Wrapping up the course with distinctions like \"AI Definition\" and \"ML Definition,\" ensuring clarity with concise, title-case concepts in 200 carefully enumerated lines.</p> <p>Ensuring clarity</p> <p>I'm refining concepts like \"AI Definition\" and \"ML Definition,\" combining title case and potentially using acronyms for distinct clarity. The goal is to maintain precision and avoid redundancy.</p> <p>Detailed line distribution</p> <p>I'm assigning 10-line chunks to various topics, such as Python, ML fundamentals, neural networks, and advanced architectures, ensuring comprehensive coverage across all 200 lines.</p> <p>Progressing through topics</p> <p>I'm charting our course with concise lines to cover \"NN Fundamentals\" and \"Training Processes,\" ensuring distinct and systematic progression across 200 meticulously controlled entries.</p> <p>Venturing into complexity</p> <p>I'm exploring advanced topics in neural networks, including special architectures like Capsule Networks and Tree-Augmented Naive Bayes, as well as Transformer techniques for natural language processing.</p> <p>Unraveling generative AI</p> <p>I'm analyzing advanced generative models and reinforcement learning techniques. This involves delving into diffusion models, denoising diffusion, principal components, and ethical considerations like fairness and privacy.</p> <p>Mapping deployment and application</p> <p>I'm covering GPU and TPU acceleration, memory management, model compression, and practical applications like computer vision and autonomous driving. This completes the cycle, ensuring comprehensive coverage.</p>"},{"location":"prompts/03-concept-enumeration/#result","title":"Result","text":"<p>Thought about deep learning curriculum concepts for 1m 42s</p> <pre><code>AI Definition\nML Definition\nHistory Of AI\nNobel Prizes\nKey Figures\nAI Branches\nSymbolic AI\nData-Driven AI\nNN Origins\nBiological Inspiration\nPython Setup\nConda Environment\nJupyter Notebooks\nNumPy Basics\nPandas Basics\nMatplotlib Basics\nSciPy Basics\nScikit-Learn Overview\nPyTorch Intro\nTensorFlow Intro\nSupervised Vs Unsupervised\nRegression Vs Classification\nOverfitting Vs Underfitting\nTraining-Validation-Test\nBias-Variance Tradeoff\nRegularization Overview\nCross-Validation\nEvaluation Metrics\nLoss Functions\nGradient Descent\nData Preprocessing\nScaling And Normalization\nData Augmentation\nFeature Engineering\nDimensionality Concepts\nData Splits\nData Visualization\nData Cleaning\nData Imbalance\nData Ethics\nPerceptron Model\nMulti-Layer Perceptron\nActivation Functions\nForward Propagation\nBackpropagation\nWeights And Biases\nBatch Training\nMini-Batch Training\nStochastic Training\nLearning Rate\nLoss Surface\nLocal Minima\nVanishing Gradients\nExploding Gradients\nInitialization Methods\nHe Initialization\nXavier Initialization\nMomentum Optimizer\nAdaGrad\nAdam Optimizer\nConvolution Operation\nFilters And Kernels\nPooling Layers\nStride And Padding\nFully Connected Layers\nBatch Normalization\nCNN Architectures\nImageNet\nTransfer Learning Basics\nObject Detection\nSequence Modeling\nVanilla RNN\nLSTM Units\nGRU Units\nBackprop Through Time (BPTT)\nExploding Gradient Control\nLanguage Modeling\nText Generation\nTime-Series Forecasting\nAttention Mechanism\nDeep Architecture\nLayer Stacking\nDepth Vs Width\nResidual Networks (ResNet)\nInception Modules\nDenseNet\nSkip Connections\nDropout\nBatchNorm\nWeight Decay\nCapsule Networks\nGraph Neural Networks (GNNs)\nTransformers Intro\nVision Transformers (ViT)\nSelf-Attention\nPositional Encoding\nBERT Overview\nGPT Overview\nSequence-To-Sequence\nNeural Machine Translation\nGenerative Models\nAutoencoders (AE)\nVariational Autoencoders (VAE)\nGAN Basics\nGenerator Vs Discriminator\nLatent Space\nConditional GAN\nCycleGAN\nStyle Transfer\nText-To-Image\nDiffusion Models\nDenoising Diffusion\nScore Matching\nLikelihood Estimation\nEnergy-Based Models\nFlow-Based Models\nPrompt Engineering\nText Generation Models\nLarge Language Models (LLMs)\nChatGPT Concept\nPCA\nt-SNE\nUMAP\nAutoencoder DR\nManifold Hypothesis\nFeature Space\nPrincipal Components\nNonlinear Embeddings\nHigh-Dimensional Data\nData Visualization Tools\nRL Definition\nMarkov Decision Process (MDP)\nValue Functions\nPolicy Gradient\nQ-Learning\nDeep Q-Network (DQN)\nActor-Critic\nReward Shaping\nExploration Vs Exploitation\nBaseline Algorithms\nModel Interpretability\nExplainable AI (XAI)\nSHAP\nLIME\nFairness\nBias Mitigation\nPrivacy\nSecurity\nAdversarial Examples\nResponsible AI\nGPU Acceleration\nTPU Acceleration\nMemory Management\nModel Compression\nQuantization\nPruning\nEdge Deployment\nCloud Services\nAutoML\nMLOps\nHyperparameter Tuning\nGrid Search\nRandom Search\nBayesian Optimization\nEarly Stopping\nLearning Rate Scheduling\nData Parallelism\nModel Parallelism\nCheckpointing\nVersion Control\nAccuracy\nPrecision\nRecall\nF1 Score\nAUC-ROC\nConfusion Matrix\nTop-1 Vs Top-5 Accuracy\nBLEU Score\nPerplexity\nMean Squared Error (MSE)\nComputer Vision\nNatural Language Processing (NLP)\nSpeech Recognition\nMedical Imaging\nDrug Discovery\nProtein Folding\nAutonomous Driving\nRobotics\nFinancial Forecasting\nRecommender Systems\nProject Scoping\nRequirement Analysis\nData Acquisition\nData Annotation\nResource Allocation\nTeam Collaboration\nVersion Control Workflow\nAgile Methods\nCode Review\nDocumentation\n</code></pre>"},{"location":"prompts/03-concept-enumeration/#getting-the-csv","title":"Getting the CSV","text":"<p>Prompt</p> <p>please return this list in a single csv file with the first column as ConceptID and the second column as ConceptLabel</p> <pre><code>import pandas as pd\n\n# Define the list of concepts\nconcepts = [\n    \"AI Definition\", \"ML Definition\", \"History Of AI\", \"Nobel Prizes\", \"Key Figures\", \"AI Branches\", \"Symbolic AI\",\n    \"Data-Driven AI\", \"NN Origins\", \"Biological Inspiration\", \"Python Setup\", \"Conda Environment\", \"Jupyter Notebooks\",\n    \"NumPy Basics\", \"Pandas Basics\", \"Matplotlib Basics\", \"SciPy Basics\", \"Scikit-Learn Overview\", \"PyTorch Intro\",\n    \"TensorFlow Intro\", \"Supervised Vs Unsupervised\", \"Regression Vs Classification\", \"Overfitting Vs Underfitting\",\n    \"Training-Validation-Test\", \"Bias-Variance Tradeoff\", \"Regularization Overview\", \"Cross-Validation\",\n    \"Evaluation Metrics\", \"Loss Functions\", \"Gradient Descent\", \"Data Preprocessing\", \"Scaling And Normalization\",\n    \"Data Augmentation\", \"Feature Engineering\", \"Dimensionality Concepts\", \"Data Splits\", \"Data Visualization\",\n    \"Data Cleaning\", \"Data Imbalance\", \"Data Ethics\", \"Perceptron Model\", \"Multi-Layer Perceptron\",\n    \"Activation Functions\", \"Forward Propagation\", \"Backpropagation\", \"Weights And Biases\", \"Batch Training\",\n    \"Mini-Batch Training\", \"Stochastic Training\", \"Learning Rate\", \"Loss Surface\", \"Local Minima\",\n    \"Vanishing Gradients\", \"Exploding Gradients\", \"Initialization Methods\", \"He Initialization\",\n    \"Xavier Initialization\", \"Momentum Optimizer\", \"AdaGrad\", \"Adam Optimizer\", \"Convolution Operation\",\n    \"Filters And Kernels\", \"Pooling Layers\", \"Stride And Padding\", \"Fully Connected Layers\", \"Batch Normalization\",\n    \"CNN Architectures\", \"ImageNet\", \"Transfer Learning Basics\", \"Object Detection\", \"Sequence Modeling\",\n    \"Vanilla RNN\", \"LSTM Units\", \"GRU Units\", \"Backprop Through Time (BPTT)\", \"Exploding Gradient Control\",\n    \"Language Modeling\", \"Text Generation\", \"Time-Series Forecasting\", \"Attention Mechanism\", \"Deep Architecture\",\n    \"Layer Stacking\", \"Depth Vs Width\", \"Residual Networks (ResNet)\", \"Inception Modules\", \"DenseNet\",\n    \"Skip Connections\", \"Dropout\", \"BatchNorm\", \"Weight Decay\", \"Capsule Networks\", \"Graph Neural Networks (GNNs)\",\n    \"Transformers Intro\", \"Vision Transformers (ViT)\", \"Self-Attention\", \"Positional Encoding\", \"BERT Overview\",\n    \"GPT Overview\", \"Sequence-To-Sequence\", \"Neural Machine Translation\", \"Generative Models\", \"Autoencoders (AE)\",\n    \"Variational Autoencoders (VAE)\", \"GAN Basics\", \"Generator Vs Discriminator\", \"Latent Space\", \"Conditional GAN\",\n    \"CycleGAN\", \"Style Transfer\", \"Text-To-Image\", \"Diffusion Models\", \"Denoising Diffusion\", \"Score Matching\",\n    \"Likelihood Estimation\", \"Energy-Based Models\", \"Flow-Based Models\", \"Prompt Engineering\", \"Text Generation Models\",\n    \"Large Language Models (LLMs)\", \"ChatGPT Concept\", \"PCA\", \"t-SNE\", \"UMAP\", \"Autoencoder DR\", \"Manifold Hypothesis\",\n    \"Feature Space\", \"Principal Components\", \"Nonlinear Embeddings\", \"High-Dimensional Data\", \"Data Visualization Tools\",\n    \"RL Definition\", \"Markov Decision Process (MDP)\", \"Value Functions\", \"Policy Gradient\", \"Q-Learning\",\n    \"Deep Q-Network (DQN)\", \"Actor-Critic\", \"Reward Shaping\", \"Exploration Vs Exploitation\", \"Baseline Algorithms\",\n    \"Model Interpretability\", \"Explainable AI (XAI)\", \"SHAP\", \"LIME\", \"Fairness\", \"Bias Mitigation\", \"Privacy\",\n    \"Security\", \"Adversarial Examples\", \"Responsible AI\", \"GPU Acceleration\", \"TPU Acceleration\", \"Memory Management\",\n    \"Model Compression\", \"Quantization\", \"Pruning\", \"Edge Deployment\", \"Cloud Services\", \"AutoML\", \"MLOps\",\n    \"Hyperparameter Tuning\", \"Grid Search\", \"Random Search\", \"Bayesian Optimization\", \"Early Stopping\",\n    \"Learning Rate Scheduling\", \"Data Parallelism\", \"Model Parallelism\", \"Checkpointing\", \"Version Control\",\n    \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC-ROC\", \"Confusion Matrix\", \"Top-1 Vs Top-5 Accuracy\",\n    \"BLEU Score\", \"Perplexity\", \"Mean Squared Error (MSE)\", \"Computer Vision\", \"Natural Language Processing (NLP)\",\n    \"Speech Recognition\", \"Medical Imaging\", \"Drug Discovery\", \"Protein Folding\", \"Autonomous Driving\", \"Robotics\",\n    \"Financial Forecasting\", \"Recommender Systems\", \"Project Scoping\", \"Requirement Analysis\", \"Data Acquisition\",\n    \"Data Annotation\", \"Resource Allocation\", \"Team Collaboration\", \"Version Control Workflow\", \"Agile Methods\",\n    \"Code Review\", \"Documentation\"\n]\n\n# Create a DataFrame with ConceptID and ConceptLabel\ndf = pd.DataFrame({\n    \"ConceptID\": range(1, len(concepts) + 1),\n    \"ConceptLabel\": concepts\n})\n\n# Save the DataFrame to a CSV file\nfile_path = \"/mnt/data/EE4940_Core_Concepts.csv\"\ndf.to_csv(file_path, index=False)\n\nfile_path\n</code></pre>"},{"location":"prompts/03-concept-enumeration/#result_1","title":"Result","text":"<p>Deep Learning Concepts</p>"},{"location":"prompts/04-concept-dependencies/","title":"Concept Dependencies","text":"<p>Prompt</p> <p>Below is a CSV file of concepts in a course on Deep Learning for junior or senior college students.  Please use this CSV file of 200 concepts for the next step.</p> <p>Next, please create a fully connected concept dependency graph for the concepts in this course. In this step, for each Concept in the CSV file, create a new list of other concepts on which it depends.  If a concept depends on a specific concept that is not listed, then we will need to add it as a new concept. This will help us determine the teaching order of the Concepts.</p> <p>Return the new list as a CSV file using the following format:</p> <ol> <li>The first column in the CSV file is a ConceptID you assigned in the list above.</li> <li>The second column is the ConceptLabel</li> <li>The third column is called Dependencies.  It's format is a pipe-delimited list of the Concept IDs that the concept depends upon.</li> </ol> <p>Check your results to make sure that all concepts are connected to at least one other concept.</p> <p>Foundation Concepts are concepts that have no dependant concepts in this set.  They have an empty third column of Dependencies.</p> <p>In this example, the following concepts have no dependencies (empty Dependencies column). They serve as possible \u201cstarting points\u201d in the curriculum:</p> <ul> <li>1: AI Definition</li> <li>3: History Of AI</li> <li>10: Biological Inspiration</li> <li>11: Python Setup</li> <li>191: Project Scoping</li> </ul> <p>All other concepts depend on at least one earlier concept\u2014ensuring every concept is connected in one unified graph. Feel free to modify or expand these dependencies to match your exact course structure.</p>"},{"location":"prompts/04-concept-dependencies/#response","title":"Response","text":"<p>Foundation Concepts</p>"},{"location":"prompts/05-concept-taxonomy/","title":"Concept Taxonomy","text":"<p>Prompt</p> <p>Given the list of concepts, your next task is to organize these concepts into a taxonomy by grouping them into appropriate categories. Each category should include concepts that share similar characteristics, functions, or applications.</p> <p>Suggest 10-12 categories for the taxonomy and assign each concept to one or more categories. Ensure that the categories cover a range of concept types, including abstract ideas, practical applications, Python libraries, common misunderstandings, and real-world examples.</p> <p>Avoid redundancy and ensure that each category is distinct.</p> <p>Your first category is \"Foundational Concepts\", your second is \"Terms and Definitions\".   A final category might be \"Goals\".</p>"},{"location":"prompts/05-concept-taxonomy/#workflow","title":"Workflow","text":"<p>ChatGPT o1 thought about course taxonomy and categories for 3m 36s</p> <p>Crafting a taxonomy</p> <p>I'm organizing 200 concepts into 10-12 categories, including foundational concepts, terms and definitions, and goals. Ensuring a variety of concept types without redundancy.</p> <p>Shaping the categories</p> <p>I'm organizing the taxonomy with 10-12 categories, focusing on topics like foundational concepts, key tools, data engineering, core ML, model architectures, optimization, advanced topics, and real-world examples.</p> <p>Clarifying \"Goals\"</p> <p>Hmm, I'm thinking about whether \"Goals\" means measuring success, project scoping, or responsible AI like interpretability. Each concept will fit into one of the 10 categories.</p> <p>Shaping the framework</p> <p>I'm honing in on Python tools, data engineering, Neural Network Architectures, Reinforcement Learning, and integration with interpretation, ethics, fairness, MLOps, and project workflow.</p> <p>Mapping the categories</p> <p>I'm working through merging \"Interpretation, Ethics, &amp; Fairness\" with \"Goals,\" but it seems intricate and possibly ineffective. It's interesting to see these elements might be better handled separately.</p> <p>Refining the taxonomy</p> <p>I'm merging \"Sequence &amp; Natural Language\" with \"Advanced &amp; Specialized Topics,\" and integrating \"Reinforcement Learning\" into the revised category. This results in a sleeker, more coherent framework.</p> <p>Finalizing the categories</p> <p>I'm looking at a streamlined taxonomy with 11 categories, including Foundational Concepts, Python Tools, Data Engineering, Advanced Topics, and Real-World Applications. Assigning concepts to these categories now.</p> <p>Voucher assignment</p> <p>Deciding whether to include \"Nobel Prizes\" in \"Foundational Concepts\" or \"History of AI\" is tricky. I'll weigh the typical context for each category to make a balanced decision.</p> <p>Piecing together the foundation</p> <p>I'm carefully grouping concepts like AI and ML definitions, AI history, Nobel Prizes, key figures, AI branches, and neural networks into foundational concepts.</p> <p>Mapping out key concepts</p> <p>I'm digging into foundational concepts like AI and ML definitions, AI history, Nobel Prizes, key figures, AI branches, and neural network origins, ensuring a balanced and comprehensive approach.</p> <p>Clarifying definitions</p> <p>I'm thinking through terms for \"Terms and Definitions,\" focusing on clear, concise definitions like Supervised vs. Unsupervised learning, Regression vs. Classification, and evaluation metrics.</p> <p>Mapping concepts</p> <p>Pulling together Python tools, data engineering, neural network architectures, and advanced topics like transformers and generative models. Assigning terms systematically to refine clarity and avoid redundancy.</p> <p>Refining category alignment</p> <p>Consider placing \"Attention Mechanism\" under \"Neural Network Architectures.\" \"Reinforcement Learning\" and \"Advanced Recognition Techniques\" seem to fit well inside \"Advanced &amp; Specialized Topics.\"</p> <p>Merging concepts</p> <p>I'm contemplating combining \"Reinforcement Learning\" with \"Advanced &amp; Specialized Topics,\" and \"Project Scoping\" seems to fit better in \"MLOps &amp; Project Workflow\" or as \"Goals.\"</p> <p>Mapping real applications</p> <p>I'm organizing concepts on ethical AI, MLOps, and project workflow, including resource allocation, version control, and agile methods. Next up: integrating \"Project Scoping\" and refining \"Goals.\"</p> <p>Clarifying project metrics</p> <p>I'm thinking about including \"Evaluation Metrics\" and \"Project Scoping\" under \"Goals,\" refining their roles to ensure they align cohesively within the broader context.</p> <p>Integrating distinct elements</p> <p>I'm honing the \"Goals\" category with evaluation metrics, precision, recall, and other metrics. For dimensionality reduction and data visualization tools, they seem to fit better in \"Data Preparation &amp; Engineering\" or \"Training &amp; Optimization.\"</p> <p>Mapping concepts</p> <p>I'm integrating dimensionality reduction and data visualization tools into \"Data Preparation &amp; Engineering,\" and placing \"Prompt Engineering\" in \"Advanced &amp; Specialized Topics\" or \"Sequence &amp; Natural Language.\"</p> <p>Mapping as I go</p> <p>Integrating concepts like \"Prompt Engineering\" into \"Advanced &amp; Specialized Topics\" and aligning \"Attention Mechanism\" with \"Neural Network Architectures.\" Also, I'm finalizing the assignment of various ML and data concepts.</p> <p>Mapping out assignments</p> <p>There's a clear progression: integrating \"Data Ethics\" into \"Interpretation, Ethics, &amp; Fairness,\" placing \"ImageNet\" in \"Neural Network Architectures,\" and categorizing \"Text Generation\" with \"Advanced &amp; Specialized Topics.\"</p> <p>Organizing concepts</p> <p>I'm grouping \"Language Modeling\" and \"Text Generation\" into \"Advanced &amp; Specialized Topics,\" and aligning \"Attention Mechanism\" with \"Neural Network Architectures\". Robotics fits best in \"Real-World Applications.\"</p> <p>Organizing and refining</p> <p>Assigning concepts like \"Loss Surface\" and \"Data Annotation\" to \"Training &amp; Optimization\" and \"Data Prep &amp; MLOps\" respectively. \"Robotics\" fits well in \"Real-World Applications.\"</p> <p>Finalizing concepts</p> <p>I'm wrapping up by mapping \"ChatGPT Concept\" and \"Adversarial Examples\" to \"Advanced &amp; Specialized Topics\" and \"Interpretation, Ethics, &amp; Fairness,\" ensuring each concept is logically categorized.</p> <p>Finalizing mappings</p> <p>Assigning concepts like \"Quantization,\" \"Pruning,\" and \"Edge Deployment\" to \"Advanced &amp; Specialized Topics,\" and ensuring all concepts fit logically within the intended categories.</p> <p>Finalizing concepts</p> <p>I'm completing the mapping of advanced concepts like \"DQN,\" \"Actor-Critic,\" and \"Reward Shaping\" to \"Advanced &amp; Specialized Topics,\" and ensuring all terms are logically categorized.</p> <p>Piecing together assignments</p> <p>I'm ensuring \"SciPy\" fits into \"Python Tools,\" aligning \"Precision,\" \"Recall,\" \"F1,\" and \"AUC-ROC\" with \"Goals,\" and placing \"Data Splits\" and \"Data Visualization Tools\" under \"Data Prep &amp; Engineering.\"</p> <p>Aligning the framework</p> <p>Aligning \"GPU Acceleration\" and \"TPU Acceleration\" with \"Advanced &amp; Specialized Topics,\" \"Project Scoping\" and \"Version Control Workflow\" with \"MLOps &amp; Project Workflow.\"</p> <p>Refining concept categories</p> <p>I'm evolving \"Adam Optimizer\" and \"Hyperparameter Tuning\" within \"Training &amp; Optimization.\" \"Data Visualization\" and \"PCA\" fit snugly under \"Data Prep &amp; Engineering.\"</p> <p>Finalizing taxonomy</p> <p>OK, let me see. I'm ensuring each concept lands in the right category, refining concepts like \"Weight Decay\" and \"Evaluation Metrics.\" I'm setting a clear, concise list of 11 categories.</p> <p>Mapping out preferences</p> <p>I'm factoring in the need for brevity and the user's desire for a range of concept types. Considering a summarized 11-category list with multi-category assignments, focusing on practical applications and Python libraries.</p>"},{"location":"prompts/05-concept-taxonomy/#result","title":"Result","text":"<p>Below is an example taxonomy of the 200 concepts, grouped into 11 distinct categories. Each concept may appear in more than one category if it makes sense pedagogically (e.g., a concept can be both a \"term/definition\" and also relevant to \"training &amp; optimization\").</p> <p>In practice, you might refine these groupings further or redistribute concepts according to your specific course design. The primary goals are (1) to ensure that each concept is accounted for at least once, (2) to keep categories mutually distinct, and (3) to cover a wide range of types (abstract ideas, practical tools, potential pitfalls, real-world use cases, etc.).</p>"},{"location":"prompts/05-concept-taxonomy/#1-foundational-concepts","title":"1. Foundational Concepts","text":"<p>These are broad, high-level ideas or historical touchpoints on which other topics build.</p> <ul> <li>(1) AI Definition</li> <li>(2) ML Definition</li> <li>(3) History Of AI</li> <li>(4) Nobel Prizes</li> <li>(5) Key Figures</li> <li>(6) AI Branches</li> <li>(7) Symbolic AI</li> <li>(8) Data-Driven AI</li> <li>(9) NN Origins</li> <li>(10) Biological Inspiration</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#2-terms-and-definitions","title":"2. Terms and Definitions","text":"<p>Key terminology and conceptual distinctions often introduced early to clarify the field's language.</p> <ul> <li>(21) Supervised Vs Unsupervised</li> <li>(22) Regression Vs Classification</li> <li>(23) Overfitting Vs Underfitting</li> <li>(25) Bias-Variance Tradeoff</li> <li>(35) Dimensionality Concepts</li> <li>(39) Data Imbalance</li> <li>(51) Loss Surface</li> <li>(52) Local Minima</li> <li>(62) Filters And Kernels</li> <li>(100) Neural Machine Translation</li> <li>(118) Text Generation Models</li> <li>(131) RL Definition</li> <li>(139) Exploration Vs Exploitation</li> </ul> <p>(Note that some of these also appear under other categories if they are central to training, optimization, or advanced topics.)</p>"},{"location":"prompts/05-concept-taxonomy/#3-python-tools-libraries","title":"3. Python Tools &amp; Libraries","text":"<p>Practical software tools used throughout the course for coding, data manipulation, and experimentation.</p> <ul> <li>(11) Python Setup</li> <li>(12) Conda Environment</li> <li>(13) Jupyter Notebooks</li> <li>(14) NumPy Basics</li> <li>(15) Pandas Basics</li> <li>(16) Matplotlib Basics</li> <li>(17) SciPy Basics</li> <li>(18) Scikit-Learn Overview</li> <li>(19) PyTorch Intro</li> <li>(20) TensorFlow Intro</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#4-data-preparation-engineering","title":"4. Data Preparation &amp; Engineering","text":"<p>Concepts and techniques for acquiring, cleaning, organizing, and transforming data before (and during) model development.</p> <ul> <li>(31) Data Preprocessing</li> <li>(32) Scaling And Normalization</li> <li>(33) Data Augmentation</li> <li>(34) Feature Engineering</li> <li>(36) Data Splits</li> <li>(37) Data Visualization</li> <li>(38) Data Cleaning</li> <li>(40) Data Ethics (also appears in Ethics/Ethical Use)</li> <li>(121) PCA</li> <li>(122) t-SNE</li> <li>(123) UMAP</li> <li>(124) Autoencoder DR</li> <li>(125) Manifold Hypothesis</li> <li>(126) Feature Space</li> <li>(127) Principal Components</li> <li>(128) Nonlinear Embeddings</li> <li>(129) High-Dimensional Data</li> <li>(130) Data Visualization Tools</li> <li>(193) Data Acquisition</li> <li>(194) Data Annotation</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#5-training-optimization","title":"5. Training &amp; Optimization","text":"<p>How models learn, common pitfalls, and methods to optimize performance.</p> <ul> <li>(23) Overfitting Vs Underfitting</li> <li>(24) Training-Validation-Test</li> <li>(25) Bias-Variance Tradeoff</li> <li>(26) Regularization Overview</li> <li>(27) Cross-Validation</li> <li>(29) Loss Functions</li> <li>(30) Gradient Descent</li> <li>(43) Activation Functions</li> <li>(44) Forward Propagation</li> <li>(45) Backpropagation</li> <li>(46) Weights And Biases</li> <li>(47) Batch Training</li> <li>(48) Mini-Batch Training</li> <li>(49) Stochastic Training</li> <li>(50) Learning Rate</li> <li>(52) Local Minima</li> <li>(53) Vanishing Gradients</li> <li>(54) Exploding Gradients</li> <li>(58) Momentum Optimizer</li> <li>(59) AdaGrad</li> <li>(60) Adam Optimizer</li> <li>(76) Exploding Gradient Control</li> <li>(90) Weight Decay</li> <li>(161) Hyperparameter Tuning</li> <li>(162) Grid Search</li> <li>(163) Random Search</li> <li>(164) Bayesian Optimization</li> <li>(165) Early Stopping</li> <li>(166) Learning Rate Scheduling</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#6-neural-network-architectures","title":"6. Neural Network Architectures","text":"<p>Core structures and design choices in building various types of deep neural networks.</p> <ul> <li>(41) Perceptron Model</li> <li>(42) Multi-Layer Perceptron</li> <li>(55) Initialization Methods</li> <li>(56) He Initialization</li> <li>(57) Xavier Initialization</li> <li>(61) Convolution Operation</li> <li>(63) Pooling Layers</li> <li>(64) Stride And Padding</li> <li>(65) Fully Connected Layers</li> <li>(66) Batch Normalization</li> <li>(67) CNN Architectures</li> <li>(68) ImageNet</li> <li>(69) Transfer Learning Basics</li> <li>(70) Object Detection (also a real-world CV application)</li> <li>(71) Sequence Modeling</li> <li>(72) Vanilla RNN</li> <li>(73) LSTM Units</li> <li>(74) GRU Units</li> <li>(75) Backprop Through Time (BPTT)</li> <li>(79) Time-Series Forecasting</li> <li>(80) Attention Mechanism</li> <li>(81) Deep Architecture</li> <li>(82) Layer Stacking</li> <li>(83) Depth Vs Width</li> <li>(84) Residual Networks (ResNet)</li> <li>(85) Inception Modules</li> <li>(86) DenseNet</li> <li>(87) Skip Connections</li> <li>(88) Dropout</li> <li>(89) BatchNorm (duplicate of 66's concept family)</li> <li>(91) Capsule Networks</li> <li>(92) Graph Neural Networks (GNNs)</li> <li>(93) Transformers Intro</li> <li>(94) Vision Transformers (ViT)</li> <li>(95) Self-Attention</li> <li>(96) Positional Encoding</li> <li>(97) BERT Overview</li> <li>(98) GPT Overview</li> <li>(99) Sequence-To-Sequence</li> <li>(101) Generative Models</li> <li>(102) Autoencoders (AE)</li> <li>(103) Variational Autoencoders (VAE)</li> <li>(104) GAN Basics</li> <li>(105) Generator Vs Discriminator</li> <li>(106) Latent Space</li> <li>(107) Conditional GAN</li> <li>(108) CycleGAN</li> <li>(109) Style Transfer</li> <li>(110) Text-To-Image</li> <li>(111) Diffusion Models</li> <li>(112) Denoising Diffusion</li> <li>(113) Score Matching</li> <li>(114) Likelihood Estimation</li> <li>(115) Energy-Based Models</li> <li>(116) Flow-Based Models</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#7-advanced-specialized-topics","title":"7. Advanced &amp; Specialized Topics","text":"<p>Cutting-edge methods, hardware accelerators, reinforcement learning, and larger-scale automation.</p> <ul> <li>(71) Sequence Modeling (repeated if focusing on advanced sequences)</li> <li>(77) Language Modeling</li> <li>(78) Text Generation</li> <li>(117) Prompt Engineering</li> <li>(119) Large Language Models (LLMs)</li> <li>(120) ChatGPT Concept</li> <li>(131) RL Definition</li> <li>(132) Markov Decision Process (MDP)</li> <li>(133) Value Functions</li> <li>(134) Policy Gradient</li> <li>(135) Q-Learning</li> <li>(136) Deep Q-Network (DQN)</li> <li>(137) Actor-Critic</li> <li>(138) Reward Shaping</li> <li>(140) Baseline Algorithms</li> <li>(151) GPU Acceleration</li> <li>(152) TPU Acceleration</li> <li>(153) Memory Management</li> <li>(154) Model Compression</li> <li>(155) Quantization</li> <li>(156) Pruning</li> <li>(157) Edge Deployment</li> <li>(158) Cloud Services</li> <li>(159) AutoML</li> <li>(167) Data Parallelism</li> <li>(168) Model Parallelism</li> <li>(169) Checkpointing</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#8-real-world-applications","title":"8. Real-World Applications","text":"<p>Domains and example uses that illustrate how AI/ML solutions are deployed.</p> <ul> <li>(70) Object Detection (also in CNNs)</li> <li>(181) Computer Vision</li> <li>(182) Natural Language Processing (NLP)</li> <li>(183) Speech Recognition</li> <li>(184) Medical Imaging</li> <li>(185) Drug Discovery</li> <li>(186) Protein Folding</li> <li>(187) Autonomous Driving</li> <li>(188) Robotics</li> <li>(189) Financial Forecasting</li> <li> <p>(190) Recommender Systems</p> </li> <li> <p>Interpretation, Ethics, &amp; Fairness</p> </li> </ul> <p>Ensuring AI systems are transparent, fair, secure, and used responsibly.</p> <ul> <li>(40) Data Ethics</li> <li>(141) Model Interpretability</li> <li>(142) Explainable AI (XAI)</li> <li>(143) SHAP</li> <li>(144) LIME</li> <li>(145) Fairness</li> <li>(146) Bias Mitigation</li> <li>(147) Privacy</li> <li>(148) Security</li> <li>(149) Adversarial Examples</li> <li>(150) Responsible AI</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#10-mlops-project-workflow","title":"10. MLOps &amp; Project Workflow","text":"<p>Practical, organizational, and life-cycle aspects of putting ML/DL models into production and managing teams.</p> <ul> <li>(160) MLOps</li> <li>(170) Version Control</li> <li>(191) Project Scoping</li> <li>(192) Requirement Analysis</li> <li>(193) Data Acquisition (also in Data Prep)</li> <li>(194) Data Annotation (also in Data Prep)</li> <li>(195) Resource Allocation</li> <li>(196) Team Collaboration</li> <li>(197) Version Control Workflow</li> <li>(198) Agile Methods</li> <li>(199) Code Review</li> <li>(200) Documentation</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#11-goals","title":"11. Goals","text":"<p>Metrics and broader objectives that guide model selection, validation, and success criteria.</p> <ul> <li>(28) Evaluation Metrics</li> <li>(171) Accuracy</li> <li>(172) Precision</li> <li>(173) Recall</li> <li>(174) F1 Score</li> <li>(175) AUC-ROC</li> <li>(176) Confusion Matrix</li> <li>(177) Top-1 Vs Top-5 Accuracy</li> <li>(178) BLEU Score</li> <li>(179) Perplexity</li> <li>(180) Mean Squared Error (MSE)</li> </ul>"},{"location":"prompts/05-concept-taxonomy/#notes-on-overlaps","title":"Notes on Overlaps","text":"<ul> <li>Some items (e.g., \"Overfitting Vs Underfitting\") appear in both Terms and Definitions and Training &amp; Optimization, because they are foundational distinctions and deeply tied to optimization.</li> <li>\"Object Detection,\" \"Data Ethics,\" \"Data Acquisition,\" and several others also appear in more than one category when relevant.</li> </ul> <p>This multi-category approach reflects the reality that many deep learning concepts straddle theoretical and practical lines and can belong in more than one \"bucket.\" Depending on your course or textbook structure, you may wish to split them differently or provide cross-references.</p>"},{"location":"prompts/05-concept-taxonomy/#final-csv-file","title":"Final CSV File","text":"<p>Prompt</p> <p>That was excellent.  Now please regenerate the CSV file, but this time please add a new column for TaxonomyID.  Use the numbers for each category as the TaxonomyID.</p> <p>The format of the new file is:</p> <ol> <li>Column 1: ConceptID</li> <li>Column 2: ConceptLabel</li> <li>Column 3: Dependencies</li> <li>Column 4: TaxonomyID</li> </ol>"},{"location":"prompts/06-glossary-of-terms/","title":"Generating a Glossary","text":"<p>Prompt</p> <p>Next we need a glossary of terms.  For each of the 200 concepts, please create a ISO 11179 definition that is precise, concise, distinct and non-circular.  When appropriate, give an example of how this term is used in the course.  Return each concept label in Markdown level 4 and the definition for the term in the body text.  Use Example: before the example.</p> <p>Return the entire glossary of terms in a single markdown file.  Place \"# Glossary of Terms\" at the top of the file.  Return the terms in alphabetical order.</p>"},{"location":"prompts/06-glossary-of-terms/#result","title":"Result","text":""},{"location":"prompts/07-equations/","title":"Generating Commonly Used Deep Learning Equations","text":"<p>Prompt</p> <p>Create a list of the 100 most commonly used equations in a college level course in deep learning.</p> <p>For each equation, return the following:</p> <ol> <li>The name of the equation in markdown level 4.</li> <li>Within the body, return the equation in LaTeX format</li> <li>Surround the equation with dollar signs</li> <li>For each variable in the equation, add a \"where:\" that defines each variable</li> <li>Put blank lines around the list of variables</li> <li>Wrap the variables names in dollar signs</li> </ol> <p>Examples:</p>"},{"location":"prompts/07-equations/#slope-and-intercept","title":"Slope and Intercept","text":"<p>\\(y = m * x + b\\)</p> <p>where:</p> <ul> <li>\\(y\\) is the vertical axis</li> <li>\\(x\\) is the horizontal axis</li> <li>\\(m\\) is the slope</li> <li>\\(b\\) is the intercept</li> </ul>"},{"location":"prompts/07-equations/#area-of-a-circle","title":"Area of a Circle","text":"<p>\\(A = 2\\pi r^2\\) where:</p> <ul> <li>\\(A\\) is the area of the circle</li> <li>\\(r\\) is the radius</li> </ul>"},{"location":"prompts/07-equations/#response","title":"Response","text":"<p>Below are 100 commonly encountered equations in a college-level deep learning course, presented in alphabetical order. Each entry has:</p> <ol> <li>A heading in Markdown level 4.</li> <li>The equation in LaTeX, surrounded by dollar signs.</li> <li>A where: section defining each variable, with each variable in dollar signs and blank lines around the list.</li> </ol>"},{"location":"prompts/07-equations/#100-common-equations-in-deep-learning","title":"100 Common Equations in Deep Learning","text":"<p>Below is a curated list of 100 frequently encountered equations in a college-level Deep Learning course. Each entry includes:</p> <ul> <li>A name (in level 4 markdown).</li> <li>The equation (in LaTeX, surrounded by single dollar signs).</li> <li>A \u201cwhere:\u201d section explaining each variable.</li> </ul> <p>The equations are grouped loosely by theme (basic algebra, loss functions, optimization, activations, convolution, RNNs, attention mechanisms, etc.) but may appear in any Deep Learning curriculum.</p>"},{"location":"prompts/07-equations/#1-mean-squared-error-mse","title":"1. Mean Squared Error (MSE)","text":"<p>\\(L_{\\text{MSE}} = \\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)} - \\hat{y}^{(i)})^2\\)</p> <p>where:</p> <ul> <li>\\(N\\) is the number of samples</li> <li>\\(y^{(i)}\\) is the true label for the \\(i\\)-th sample</li> <li>\\(\\hat{y}^{(i)}\\) is the predicted value for the \\(i\\)-th sample</li> </ul>"},{"location":"prompts/07-equations/#2-cross-entropy-loss-binary","title":"2. Cross-Entropy Loss (Binary)","text":"<p>\\(L_{\\text{CE}} = -\\frac{1}{N}\\sum_{i=1}^{N}\\Big[y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\\Big]\\)</p> <p>where:</p> <ul> <li>\\(N\\) is the number of samples</li> <li>\\(y^{(i)} \\in \\{0, 1\\}\\) is the true binary label</li> <li>\\(\\hat{y}^{(i)}\\) is the predicted probability of the positive class</li> </ul>"},{"location":"prompts/07-equations/#3-cross-entropy-loss-multi-class","title":"3. Cross-Entropy Loss (Multi-Class)","text":"<p>\\(L_{\\text{CE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{c}^{(i)} \\log(\\hat{y}_{c}^{(i)})\\)</p> <p>where:</p> <ul> <li>\\(N\\) is the number of samples</li> <li>\\(C\\) is the number of classes</li> <li>\\(y_{c}^{(i)} \\in \\{0,1\\}\\) indicates the true class of sample \\(i\\)</li> <li>\\(\\hat{y}_{c}^{(i)}\\) is the predicted probability for class \\(c\\)</li> </ul>"},{"location":"prompts/07-equations/#4-softmax-function","title":"4. Softmax Function","text":"<p>\\(\\hat{y}_j = \\frac{\\exp(z_j)}{\\sum_{k=1}^{C}\\exp(z_k)}\\)</p> <p>where:</p> <ul> <li>\\(z_j\\) is the logit for class \\(j\\)</li> <li>\\(C\\) is the total number of classes</li> <li>\\(\\hat{y}_j\\) is the probability assigned to class \\(j\\)</li> </ul>"},{"location":"prompts/07-equations/#5-sigmoid-logistic-function","title":"5. Sigmoid (Logistic) Function","text":"<p>\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input or logit</li> <li>\\(\\sigma(z)\\) is the output between 0 and 1</li> </ul>"},{"location":"prompts/07-equations/#6-tanh-function","title":"6. Tanh Function","text":"<p>\\(\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> <li>\\(\\tanh(z)\\) outputs values in the range \\([-1, 1]\\)</li> </ul>"},{"location":"prompts/07-equations/#7-rectified-linear-unit-relu","title":"7. Rectified Linear Unit (ReLU)","text":"<p>\\(\\text{ReLU}(z) = \\max(0, z)\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> <li>\\(\\text{ReLU}(z)\\) is zero for negative \\(z\\) and \\(z\\) itself if \\(z &gt; 0\\)</li> </ul>"},{"location":"prompts/07-equations/#8-leaky-relu","title":"8. Leaky ReLU","text":"<p>\\(\\text{LeakyReLU}(z) =  \\begin{cases} z &amp; \\text{if } z \\ge 0\\\\ \\alpha \\, z &amp; \\text{if } z &lt; 0 \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> <li>\\(\\alpha\\) is a small positive slope (e.g., 0.01) for negative \\(z\\)</li> </ul>"},{"location":"prompts/07-equations/#9-weighted-sum-of-inputs","title":"9. Weighted Sum of Inputs","text":"<p>\\(z = \\sum_{i=1}^{d} w_i x_i + b\\)</p> <p>where:</p> <ul> <li>\\(d\\) is the number of input features</li> <li>\\(w_i\\) is the weight for feature \\(x_i\\)</li> <li>\\(x_i\\) is the \\(i\\)-th input</li> <li>\\(b\\) is the bias term</li> </ul>"},{"location":"prompts/07-equations/#10-neural-network-output-one-layer","title":"10. Neural Network Output (One Layer)","text":"<p>\\(\\hat{y} = f\\bigg(\\sum_{i=1}^{d} w_i x_i + b\\bigg)\\)</p> <p>where:</p> <ul> <li>\\(f(\\cdot)\\) is an activation function</li> <li>\\(w_i\\), \\(x_i\\), and \\(b\\) are defined as above</li> </ul>"},{"location":"prompts/07-equations/#11-gradient-descent-parameter-update","title":"11. Gradient Descent (Parameter Update)","text":"<p>\\(\\theta \\leftarrow \\theta - \\eta \\,\\nabla_\\theta L(\\theta)\\)</p> <p>where:</p> <ul> <li>\\(\\theta\\) represents parameters (weights, biases)</li> <li>\\(\\eta\\) is the learning rate</li> <li>\\(L(\\theta)\\) is the loss function</li> <li>\\(\\nabla_\\theta L(\\theta)\\) is the gradient of the loss w.r.t. parameters</li> </ul>"},{"location":"prompts/07-equations/#12-chain-rule-single-variable","title":"12. Chain Rule (Single-Variable)","text":"<p>\\(\\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x)\\)</p> <p>where:</p> <ul> <li>\\(f\\) and \\(g\\) are functions</li> <li>\\(f'\\) and \\(g'\\) are their derivatives</li> </ul>"},{"location":"prompts/07-equations/#13-chain-rule-multivariable","title":"13. Chain Rule (Multivariable)","text":"<p>\\(\\frac{\\partial}{\\partial x_i} L = \\sum_{j} \\frac{\\partial L}{\\partial z_j} \\frac{\\partial z_j}{\\partial x_i}\\)</p> <p>where:</p> <ul> <li>\\(L\\) is the loss function</li> <li>\\(z_j\\) are intermediate variables dependent on \\(x_i\\)</li> </ul>"},{"location":"prompts/07-equations/#14-momentum-update","title":"14. Momentum Update","text":"\\[ \\begin{aligned} v &amp;\\leftarrow \\beta v + (1-\\beta)\\nabla_\\theta L(\\theta), \\\\ \\theta &amp;\\leftarrow \\theta - \\eta\\, v \\end{aligned} \\] <p>where:</p> <ul> <li>\\(v\\) is the velocity term</li> <li>\\(\\beta\\) is the momentum hyperparameter (e.g., 0.9)</li> <li>\\(\\eta\\) is the learning rate</li> <li>\\(\\nabla_\\theta L(\\theta)\\) is the gradient</li> </ul>"},{"location":"prompts/07-equations/#15-nesterov-accelerated-gradient-nag","title":"15. Nesterov Accelerated Gradient (NAG)","text":"\\[ \\begin{aligned} v &amp;\\leftarrow \\beta v + (1-\\beta)\\nabla_\\theta L(\\theta - \\beta v),\\\\ \\theta &amp;\\leftarrow \\theta - \\eta\\, v \\end{aligned} \\] <p>where:</p> <ul> <li>\\(v\\) is the velocity term</li> <li>\\(\\beta\\) is the momentum factor</li> <li>\\(\\theta\\) are the parameters</li> <li>\\(\\eta\\) is the learning rate</li> </ul>"},{"location":"prompts/07-equations/#16-rmsprop-update","title":"16. RMSProp Update","text":"\\[ \\begin{aligned} E[g^2] &amp;\\leftarrow \\rho\\, E[g^2] + (1-\\rho) \\, (\\nabla_\\theta L(\\theta))^2, \\\\ \\theta &amp;\\leftarrow \\theta - \\frac{\\eta}{\\sqrt{E[g^2] + \\epsilon}} \\nabla_\\theta L(\\theta) \\end{aligned} \\] <p>where:</p> <ul> <li>\\(E[g^2]\\) is the running average of squared gradients</li> <li>\\(\\rho\\) is the decay rate (e.g., 0.9)</li> <li>\\(\\eta\\) is the learning rate</li> <li>\\(\\epsilon\\) is a small constant for numerical stability</li> </ul>"},{"location":"prompts/07-equations/#17-adam-optimizer","title":"17. Adam Optimizer","text":"\\[ \\begin{aligned} m &amp;\\leftarrow \\beta_1 m + (1 - \\beta_1)\\nabla_\\theta L(\\theta), \\\\ v &amp;\\leftarrow \\beta_2 v + (1 - \\beta_2)(\\nabla_\\theta L(\\theta))^2, \\\\ \\hat{m} &amp;\\leftarrow \\frac{m}{1 - \\beta_1^t}, \\quad \\hat{v} \\leftarrow \\frac{v}{1 - \\beta_2^t}, \\\\ \\theta &amp;\\leftarrow \\theta - \\eta \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon} \\end{aligned} \\] <p>where:</p> <ul> <li>\\(m\\) is the first moment estimate</li> <li>\\(v\\) is the second moment estimate</li> <li>\\(\\beta_1\\) and \\(\\beta_2\\) are exponential decay rates</li> <li>\\(\\eta\\) is the learning rate</li> <li>\\(t\\) is the current time step</li> <li>\\(\\epsilon\\) is a small constant</li> </ul>"},{"location":"prompts/07-equations/#18-l1-regularization","title":"18. L1 Regularization","text":"<p>\\(R_{L1} = \\lambda \\sum_{j} |w_j|\\)</p> <p>where:</p> <ul> <li>\\(\\lambda\\) is the regularization coefficient</li> <li>\\(w_j\\) are the weights</li> </ul>"},{"location":"prompts/07-equations/#19-l2-regularization","title":"19. L2 Regularization","text":"<p>\\(R_{L2} = \\frac{\\lambda}{2} \\sum_{j} w_j^2\\)</p> <p>where:</p> <ul> <li>\\(\\lambda\\) is the regularization strength</li> <li>\\(w_j\\) are the model\u2019s weights</li> <li>The factor \\(\\tfrac{1}{2}\\) is often included by convention</li> </ul>"},{"location":"prompts/07-equations/#20-weighted-cross-entropy-class-imbalance","title":"20. Weighted Cross-Entropy (Class Imbalance)","text":"<p>\\(L_{\\text{WCE}} = -\\frac{1}{N} \\sum_{i=1}^N \\Big[\\alpha \\, y^{(i)} \\log(\\hat{y}^{(i)}) + (1-\\alpha)\\,(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\Big]\\)</p> <p>where:</p> <ul> <li>\\(\\alpha\\) is a weight factor for the positive class</li> <li>\\(y^{(i)} \\in \\{0,1\\}\\) is the true label</li> <li>\\(\\hat{y}^{(i)}\\) is the predicted probability</li> </ul>"},{"location":"prompts/07-equations/#21-negative-log-likelihood-nll","title":"21. Negative Log-Likelihood (NLL)","text":"<p>\\(L_{\\text{NLL}} = -\\sum_{i=1}^N \\log \\, p(y^{(i)} | x^{(i)})\\)</p> <p>where:</p> <ul> <li>\\(p(y|x)\\) is the predicted probability of label \\(y\\) given input \\(x\\)</li> <li>\\(N\\) is the number of samples</li> </ul>"},{"location":"prompts/07-equations/#22-normal-equation-linear-regression","title":"22. Normal Equation (Linear Regression)","text":"<p>\\(\\theta = (X^\\top X)^{-1} X^\\top y\\)</p> <p>where:</p> <ul> <li>\\(X\\) is the design matrix</li> <li>\\(y\\) is the vector of targets</li> <li>\\(\\theta\\) is the vector of parameters</li> </ul>"},{"location":"prompts/07-equations/#23-coefficient-of-determination-r2","title":"23. Coefficient of Determination (\\(R^2\\))","text":"<p>\\(R^2 = 1 - \\frac{\\sum_{i=1}^N (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum_{i=1}^N (y^{(i)} - \\bar{y})^2}\\)</p> <p>where:</p> <ul> <li>\\(y^{(i)}\\) is the actual value</li> <li>\\(\\hat{y}^{(i)}\\) is the predicted value</li> <li>\\(\\bar{y}\\) is the mean of \\(y^{(i)}\\)</li> </ul>"},{"location":"prompts/07-equations/#24-batch-normalization-mean","title":"24. Batch Normalization (Mean)","text":"<p>\\(\\mu_B = \\frac{1}{m}\\sum_{i=1}^{m} x_i\\)</p> <p>where:</p> <ul> <li>\\(m\\) is the number of samples in a mini-batch</li> <li>\\(x_i\\) is the \\(i\\)-th activation in the batch</li> </ul>"},{"location":"prompts/07-equations/#25-batch-normalization-variance","title":"25. Batch Normalization (Variance)","text":"<p>\\(\\sigma_B^2 = \\frac{1}{m}\\sum_{i=1}^{m} (x_i - \\mu_B)^2\\)</p> <p>where:</p> <ul> <li>\\(x_i\\) is the \\(i\\)-th activation in the batch</li> <li>\\(\\mu_B\\) is the batch mean</li> <li>\\(\\sigma_B^2\\) is the batch variance</li> </ul>"},{"location":"prompts/07-equations/#26-batch-normalization-forward-pass","title":"26. Batch Normalization (Forward Pass)","text":"<p>\\(\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}, \\quad y_i = \\gamma \\, \\hat{x}_i + \\beta\\)</p> <p>where:</p> <ul> <li>\\(x_i\\) is the input activation</li> <li>\\(\\mu_B\\), \\(\\sigma_B^2\\) are batch statistics</li> <li>\\(\\epsilon\\) is a small constant</li> <li>\\(\\gamma\\), \\(\\beta\\) are learnable scale and shift parameters</li> </ul>"},{"location":"prompts/07-equations/#27-dropout-forward-pass","title":"27. Dropout (Forward Pass)","text":"<p>\\(\\tilde{h}^{(l)} = \\mathbf{m}^{(l)} \\odot h^{(l)}\\)</p> <p>where:</p> <ul> <li>\\(h^{(l)}\\) is the activation vector at layer \\(l\\)</li> <li>\\(\\mathbf{m}^{(l)}\\) is a random binary mask (each entry ~ Bernoulli(p))</li> <li>\\(\\odot\\) denotes elementwise multiplication</li> </ul>"},{"location":"prompts/07-equations/#28-2d-convolution-without-stridepadding","title":"28. 2D Convolution (Without Stride/Padding)","text":"<p>\\((I * K)(x,y) = \\sum_{u=-r}^{r} \\sum_{v=-s}^{s} I(x+u, y+v)\\, K(u,v)\\)</p> <p>where:</p> <ul> <li>\\(I\\) is the input image</li> <li>\\(K\\) is the kernel</li> <li>\\(r\\), \\(s\\) define kernel size offsets</li> </ul>"},{"location":"prompts/07-equations/#29-transposed-convolution","title":"29. Transposed Convolution","text":"<p>\\(\\text{ConvTranspose}(z, K) = \\sum_{u}\\sum_{v} z(u,v)\\, \\text{Upsample}(K, \\text{stride})\\)</p> <p>where:</p> <ul> <li>\\(z(u,v)\\) is an activation map</li> <li>\\(K\\) is the kernel</li> <li>\\(\\text{stride}\\) is the factor by which the output is upsampled</li> </ul>"},{"location":"prompts/07-equations/#30-max-pooling-2d","title":"30. Max Pooling (2D)","text":"<p>\\(\\text{MaxPool}(x,y) = \\max_{(p,q)\\in \\,\\mathcal{R}_{x,y}} I(p,q)\\)</p> <p>where:</p> <ul> <li>\\(I\\) is the input feature map</li> <li>\\(\\mathcal{R}_{x,y}\\) is the receptive field around location \\((x,y)\\)</li> </ul>"},{"location":"prompts/07-equations/#31-average-pooling-2d","title":"31. Average Pooling (2D)","text":"<p>\\(\\text{AvgPool}(x,y) = \\frac{1}{|\\mathcal{R}_{x,y}|}\\sum_{(p,q)\\in \\,\\mathcal{R}_{x,y}} I(p,q)\\)</p> <p>where:</p> <ul> <li>\\(\\mathcal{R}_{x,y}\\) is the region of pooling</li> <li>\\(|\\mathcal{R}_{x,y}|\\) is the size of that region</li> </ul>"},{"location":"prompts/07-equations/#32-rnn-hidden-state-update","title":"32. RNN Hidden State Update","text":"<p>\\(h_t = f(W_{hh} \\, h_{t-1} + W_{xh} \\, x_t + b_h)\\)</p> <p>where:</p> <ul> <li>\\(h_t\\) is the hidden state at time \\(t\\)</li> <li>\\(x_t\\) is the input at time \\(t\\)</li> <li>\\(W_{hh}, W_{xh}\\) are weight matrices</li> <li>\\(b_h\\) is a bias vector</li> <li>\\(f(\\cdot)\\) is an activation (e.g., tanh)</li> </ul>"},{"location":"prompts/07-equations/#33-lstm-input-gate","title":"33. LSTM Input Gate","text":"<p>\\(i_t = \\sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)\\)</p> <p>where:</p> <ul> <li>\\(i_t\\) is the input gate</li> <li>\\(x_t\\) is the current input</li> <li>\\(h_{t-1}\\) is the previous hidden state</li> <li>\\(W_{xi}, W_{hi}\\) are weight matrices</li> <li>\\(b_i\\) is the bias</li> <li>\\(\\sigma\\) is the sigmoid function</li> </ul>"},{"location":"prompts/07-equations/#34-lstm-forget-gate","title":"34. LSTM Forget Gate","text":"<p>\\(f_t = \\sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)\\)</p> <p>where:</p> <ul> <li>\\(f_t\\) is the forget gate</li> <li>\\(W_{xf}, W_{hf}\\) are weight matrices</li> <li>\\(b_f\\) is the bias</li> </ul>"},{"location":"prompts/07-equations/#35-lstm-output-gate","title":"35. LSTM Output Gate","text":"<p>\\(o_t = \\sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)\\)</p> <p>where:</p> <ul> <li>\\(o_t\\) is the output gate</li> <li>\\(W_{xo}, W_{ho}\\) are weight matrices</li> <li>\\(b_o\\) is the bias</li> </ul>"},{"location":"prompts/07-equations/#36-lstm-cell-state","title":"36. LSTM Cell State","text":"<p>\\(c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, \\quad \\tilde{c}_t = \\tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c)\\)</p> <p>where:</p> <ul> <li>\\(c_t\\) is the cell state</li> <li>\\(f_t, i_t\\) are forget and input gates</li> <li>\\(\\tilde{c}_t\\) is the candidate cell state</li> <li>\\(\\odot\\) is elementwise multiplication</li> </ul>"},{"location":"prompts/07-equations/#37-gru-update-gate","title":"37. GRU Update Gate","text":"<p>\\(z_t = \\sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z)\\)</p> <p>where:</p> <ul> <li>\\(z_t\\) is the update gate</li> <li>\\(x_t\\) is the input at time \\(t\\)</li> <li>\\(h_{t-1}\\) is the previous hidden state</li> <li>\\(W_{xz}, W_{hz}\\) are weight matrices</li> <li>\\(b_z\\) is the bias</li> </ul>"},{"location":"prompts/07-equations/#38-gru-reset-gate","title":"38. GRU Reset Gate","text":"<p>\\(r_t = \\sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r)\\)</p> <p>where:</p> <ul> <li>\\(r_t\\) is the reset gate</li> <li>\\(W_{xr}, W_{hr}\\) are weight matrices</li> <li>\\(b_r\\) is the bias</li> </ul>"},{"location":"prompts/07-equations/#39-gru-hidden-state","title":"39. GRU Hidden State","text":"<p>\\(h_t = z_t \\odot h_{t-1} + (1 - z_t) \\odot \\tilde{h}_t, \\quad \\tilde{h}_t = \\tanh(W_{xh} x_t + r_t \\odot (W_{hh} h_{t-1}))\\)</p> <p>where:</p> <ul> <li>\\(h_t\\) is the updated hidden state</li> <li>\\(z_t, r_t\\) are the update and reset gates</li> <li>\\(\\tilde{h}_t\\) is the candidate hidden state</li> </ul>"},{"location":"prompts/07-equations/#40-attention-score-dot-product","title":"40. Attention Score (Dot Product)","text":"<p>\\(e_{t,s} = h_t^\\top \\, s_s\\)</p> <p>where:</p> <ul> <li>\\(h_t\\) is the query vector</li> <li>\\(s_s\\) is the key vector</li> <li>\\(e_{t,s}\\) is the scalar score measuring alignment</li> </ul>"},{"location":"prompts/07-equations/#41-attention-weights-softmax","title":"41. Attention Weights (Softmax)","text":"<p>\\(\\alpha_{t,s} = \\frac{\\exp(e_{t,s})}{\\sum_{k=1}^{S} \\exp(e_{t,k})}\\)</p> <p>where:</p> <ul> <li>\\(e_{t,s}\\) is the attention score for query \\(t\\) and key \\(s\\)</li> <li>\\(S\\) is the total number of key vectors</li> </ul>"},{"location":"prompts/07-equations/#42-context-vector","title":"42. Context Vector","text":"<p>\\(c_t = \\sum_{s=1}^{S} \\alpha_{t,s}\\, v_s\\)</p> <p>where:</p> <ul> <li>\\(\\alpha_{t,s}\\) are attention weights</li> <li>\\(v_s\\) is the value vector associated with key \\(s\\)</li> <li>\\(c_t\\) is the resulting weighted sum</li> </ul>"},{"location":"prompts/07-equations/#43-scaled-dot-product-attention","title":"43. Scaled Dot-Product Attention","text":"<p>\\(\\text{Attention}(Q,K,V) = \\text{softmax}\\bigg(\\frac{QK^\\top}{\\sqrt{d_k}}\\bigg)V\\)</p> <p>where:</p> <ul> <li>\\(Q\\) is the matrix of query vectors</li> <li>\\(K\\) is the matrix of key vectors</li> <li>\\(V\\) is the matrix of value vectors</li> <li>\\(d_k\\) is the dimensionality of keys/queries</li> </ul>"},{"location":"prompts/07-equations/#44-multi-head-attention","title":"44. Multi-Head Attention","text":"<p>\\(\\text{MHA}(Q,K,V) = \\big[\\text{head}_1, \\dots, \\text{head}_h\\big]W^O\\)</p> <p>where:</p> <ul> <li>\\(\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\\)</li> <li>\\(W^O\\) is the output projection matrix</li> <li>\\(h\\) is the number of heads</li> </ul>"},{"location":"prompts/07-equations/#45-feed-forward-transformer","title":"45. Feed-Forward (Transformer)","text":"<p>\\(\\text{FFN}(x) = \\max(0, xW_1 + b_1) \\, W_2 + b_2\\)</p> <p>where:</p> <ul> <li>\\(x\\) is the input</li> <li>\\(W_1, b_1, W_2, b_2\\) are learned parameters</li> <li>\\(\\max(0,\\cdot)\\) indicates ReLU (or another activation)</li> </ul>"},{"location":"prompts/07-equations/#46-positional-encoding-sinusoidal","title":"46. Positional Encoding (Sinusoidal)","text":"<p>\\(\\text{PE}(pos, 2i) = \\sin\\big(pos / 10000^{2i/d_{\\text{model}}}\\big), \\quad \\text{PE}(pos, 2i+1) = \\cos\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\\)</p> <p>where:</p> <ul> <li>\\(pos\\) is the position index</li> <li>\\(i\\) is the dimension index</li> <li>\\(d_{\\text{model}}\\) is the model dimensionality</li> </ul>"},{"location":"prompts/07-equations/#47-perceptron-rule","title":"47. Perceptron Rule","text":"<p>\\(\\hat{y} =  \\begin{cases} 1 &amp; \\text{if } w^\\top x + b \\ge 0\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(w\\) is the weight vector</li> <li>\\(x\\) is the input vector</li> <li>\\(b\\) is the bias term</li> </ul>"},{"location":"prompts/07-equations/#48-general-activation-forward","title":"48. General Activation Forward","text":"<p>\\(a = f(z)\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the pre-activation (weighted sum)</li> <li>\\(f(\\cdot)\\) is an activation function (e.g., sigmoid, ReLU)</li> </ul>"},{"location":"prompts/07-equations/#49-derivative-of-relu","title":"49. Derivative of ReLU","text":"<p>\\(\\frac{d}{dz}\\text{ReLU}(z) = \\begin{cases} 1 &amp; z &gt; 0\\\\ 0 &amp; z \\le 0 \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> </ul>"},{"location":"prompts/07-equations/#50-derivative-of-sigmoid","title":"50. Derivative of Sigmoid","text":"<p>\\(\\sigma'(z) = \\sigma(z)\\big(1-\\sigma(z)\\big)\\)</p> <p>where:</p> <ul> <li>\\(\\sigma(z)\\) is the sigmoid function</li> </ul>"},{"location":"prompts/07-equations/#51-derivative-of-tanh","title":"51. Derivative of Tanh","text":"<p>\\(\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)\\)</p> <p>where:</p> <ul> <li>\\(\\tanh(z)\\) is the hyperbolic tangent function</li> </ul>"},{"location":"prompts/07-equations/#52-derivative-of-softmax-vector-form","title":"52. Derivative of Softmax (Vector Form)","text":"<p>\\(\\frac{\\partial \\hat{y}_j}{\\partial z_k} = \\hat{y}_j \\big(\\delta_{jk} - \\hat{y}_k\\big)\\)</p> <p>where:</p> <ul> <li>\\(\\hat{y}_j\\) is the softmax output for class \\(j\\)</li> <li>\\(z_k\\) is the logit for class \\(k\\)</li> <li>\\(\\delta_{jk}\\) is the Kronecker delta (1 if \\(j=k\\), 0 otherwise)</li> </ul>"},{"location":"prompts/07-equations/#53-derivative-of-cross-entropy-wrt-logits-softmax","title":"53. Derivative of Cross-Entropy w.r.t. Logits (Softmax)","text":"<p>\\(\\frac{\\partial L_{\\text{CE}}}{\\partial z_j} = \\hat{y}_j - y_j\\)</p> <p>where:</p> <ul> <li>\\(\\hat{y}_j\\) is the predicted probability (softmax output)</li> <li>\\(y_j\\) is the one-hot target</li> </ul>"},{"location":"prompts/07-equations/#54-l2-distance-euclidean","title":"54. L2 Distance (Euclidean)","text":"<p>\\(d_{\\text{Euclidean}}(x,y) = \\sqrt{\\sum_{i=1}^{d}(x_i - y_i)^2}\\)</p> <p>where:</p> <ul> <li>\\(x, y \\in \\mathbb{R}^d\\) are vectors</li> <li>\\(d\\) is the number of dimensions</li> </ul>"},{"location":"prompts/07-equations/#55-manhattan-distance-l1","title":"55. Manhattan Distance (L1)","text":"<p>\\(d_{\\text{Manhattan}}(x,y) = \\sum_{i=1}^d |x_i - y_i|\\)</p> <p>where:</p> <ul> <li>\\(x, y\\) are vectors in \\(\\mathbb{R}^d\\)</li> <li>\\(|\\,\\cdot\\,|\\) is the absolute value</li> </ul>"},{"location":"prompts/07-equations/#56-cosine-similarity","title":"56. Cosine Similarity","text":"<p>\\(\\text{cos\\_sim}(x, y) = \\frac{x \\cdot y}{\\|x\\| \\, \\|y\\|}\\)</p> <p>where:</p> <ul> <li>\\(x \\cdot y\\) is the dot product</li> <li>\\(\\|x\\|\\) is the norm of \\(x\\)</li> </ul>"},{"location":"prompts/07-equations/#57-dot-product","title":"57. Dot Product","text":"<p>\\(x \\cdot y = \\sum_{i=1}^{d} x_i \\, y_i\\)</p> <p>where:</p> <ul> <li>\\(x, y\\) are vectors in \\(\\mathbb{R}^d\\)</li> </ul>"},{"location":"prompts/07-equations/#58-matrix-multiplication","title":"58. Matrix Multiplication","text":"<p>\\((AB)_{ij} = \\sum_{k=1}^{n} A_{ik}\\, B_{kj}\\)</p> <p>where:</p> <ul> <li>\\(A\\) is a \\((m \\times n)\\) matrix</li> <li>\\(B\\) is a \\((n \\times p)\\) matrix</li> <li>\\((AB)\\) is a \\((m \\times p)\\) matrix</li> </ul>"},{"location":"prompts/07-equations/#59-determinant-of-a-22-matrix","title":"59. Determinant of a 2\u00d72 Matrix","text":"<p>\\(\\det\\begin{pmatrix} a &amp; b\\\\ c &amp; d \\end{pmatrix} = ad - bc\\)</p> <p>where:</p> <ul> <li>\\(a, b, c, d\\) are elements of the matrix</li> </ul>"},{"location":"prompts/07-equations/#60-matrix-inverse-22","title":"60. Matrix Inverse (2\u00d72)","text":"<p>\\(\\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}^{-1}  = \\frac{1}{ad - bc} \\begin{pmatrix} d &amp; -b \\\\ -c &amp; a\\end{pmatrix}\\)</p> <p>where:</p> <ul> <li>\\(a, b, c, d\\) are elements</li> <li>\\(ad - bc \\neq 0\\)</li> </ul>"},{"location":"prompts/07-equations/#61-frobenius-norm","title":"61. Frobenius Norm","text":"<p>\\(\\|A\\|_F = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n} |A_{ij}|^2}\\)</p> <p>where:</p> <ul> <li>\\(A\\) is an \\(m \\times n\\) matrix</li> <li>\\(A_{ij}\\) is the element in row \\(i\\), column \\(j\\)</li> </ul>"},{"location":"prompts/07-equations/#62-kl-divergence","title":"62. KL Divergence","text":"<p>\\(D_{\\text{KL}}(P \\| Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}\\)</p> <p>where:</p> <ul> <li>\\(P\\) and \\(Q\\) are discrete probability distributions</li> <li>The sum is over all possible outcomes \\(x\\)</li> </ul>"},{"location":"prompts/07-equations/#63-jensen-shannon-divergence","title":"63. Jensen-Shannon Divergence","text":"<p>\\(D_{\\text{JS}}(P\\|Q) = \\frac{1}{2} D_{\\text{KL}}\\big(P \\| M\\big) + \\frac{1}{2} D_{\\text{KL}}\\big(Q \\| M\\big)\\)</p> <p>where:</p> <ul> <li>\\(M = \\frac{1}{2}(P + Q)\\)</li> <li>\\(P\\) and \\(Q\\) are probability distributions</li> </ul>"},{"location":"prompts/07-equations/#64-bayes-rule","title":"64. Bayes\u2019 Rule","text":"<p>\\(P(A|B) = \\frac{P(B|A)\\,P(A)}{P(B)}\\)</p> <p>where:</p> <ul> <li>\\(A\\) and \\(B\\) are events</li> <li>\\(P(A)\\), \\(P(B)\\), \\(P(B|A)\\) are known or can be inferred</li> </ul>"},{"location":"prompts/07-equations/#65-covariance","title":"65. Covariance","text":"<p>\\(\\text{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])]\\)</p> <p>where:</p> <ul> <li>\\(E[\\cdot]\\) denotes the expectation</li> <li>\\(X, Y\\) are random variables</li> </ul>"},{"location":"prompts/07-equations/#66-pearson-correlation","title":"66. Pearson Correlation","text":"<p>\\(\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\)</p> <p>where:</p> <ul> <li>\\(\\sigma_X\\) and \\(\\sigma_Y\\) are standard deviations of \\(X\\) and \\(Y\\)</li> </ul>"},{"location":"prompts/07-equations/#67-pca-covariance-eigen-decomposition","title":"67. PCA Covariance Eigen-Decomposition","text":"<p>\\(C = U \\Lambda U^\\top\\)</p> <p>where:</p> <ul> <li>\\(C\\) is the covariance matrix</li> <li>\\(U\\) is the matrix of eigenvectors</li> <li>\\(\\Lambda\\) is the diagonal matrix of eigenvalues</li> </ul>"},{"location":"prompts/07-equations/#68-singular-value-decomposition-svd","title":"68. Singular Value Decomposition (SVD)","text":"<p>\\(A = U \\Sigma V^\\top\\)</p> <p>where:</p> <ul> <li>\\(A\\) is an \\((m \\times n)\\) matrix</li> <li>\\(U\\) and \\(V\\) are orthonormal matrices</li> <li>\\(\\Sigma\\) is the diagonal matrix of singular values</li> </ul>"},{"location":"prompts/07-equations/#69-gaussian-distribution-1d","title":"69. Gaussian Distribution (1D)","text":"<p>\\(\\mathcal{N}(x \\mid \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\Big(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\Big)\\)</p> <p>where:</p> <ul> <li>\\(\\mu\\) is the mean</li> <li>\\(\\sigma^2\\) is the variance</li> </ul>"},{"location":"prompts/07-equations/#70-probability-integral-discrete","title":"70. Probability Integral (Discrete)","text":"<p>\\(\\sum_{x} P(x) = 1\\)</p> <p>where:</p> <ul> <li>\\(x\\) ranges over all possible outcomes</li> <li>\\(P(x)\\) is a probability distribution</li> </ul>"},{"location":"prompts/07-equations/#71-probability-integral-continuous","title":"71. Probability Integral (Continuous)","text":"<p>\\(\\int_{-\\infty}^{\\infty} p(x)\\, dx = 1\\)</p> <p>where:</p> <ul> <li>\\(p(x)\\) is a probability density function</li> </ul>"},{"location":"prompts/07-equations/#72-exponential-family-general-form","title":"72. Exponential Family (General Form)","text":"<p>\\(p(x|\\theta) = h(x) \\exp\\big(\\eta(\\theta)^\\top T(x) - A(\\theta)\\big)\\)</p> <p>where:</p> <ul> <li>\\(h(x)\\), \\(T(x)\\) are known functions</li> <li>\\(\\eta(\\theta)\\) is the natural parameter</li> <li>\\(A(\\theta)\\) is the log-partition function</li> </ul>"},{"location":"prompts/07-equations/#73-weighted-adjacency-graph-neural-network","title":"73. Weighted Adjacency (Graph Neural Network)","text":"<p>\\(h_v^{(l+1)} = \\sigma\\Bigg(\\sum_{u \\in \\mathcal{N}(v)} \\alpha_{vu} \\, W^{(l)} h_u^{(l)}\\Bigg)\\)</p> <p>where:</p> <ul> <li>\\(h_v^{(l)}\\) is the hidden representation of node \\(v\\) at layer \\(l\\)</li> <li>\\(\\alpha_{vu}\\) is the attention or adjacency weight from node \\(u\\) to \\(v\\)</li> <li>\\(W^{(l)}\\) is a trainable weight matrix</li> <li>\\(\\sigma\\) is an activation function</li> </ul>"},{"location":"prompts/07-equations/#74-gnn-aggregation","title":"74. GNN Aggregation","text":"<p>\\(h_v^{(l+1)} = \\text{AGGREGATE}\\big(\\{\\,h_u^{(l)} : u \\in \\mathcal{N}(v)\\}\\big)\\)</p> <p>where:</p> <ul> <li>\\(\\mathcal{N}(v)\\) is the set of neighbors of node \\(v\\)</li> <li>\\(\\text{AGGREGATE}(\\cdot)\\) could be sum, mean, max, or an attention-based mechanism</li> </ul>"},{"location":"prompts/07-equations/#75-rmsprop-repeated-for-emphasis","title":"75. RMSProp (Repeated for Emphasis)","text":"<p>\\(\\theta \\leftarrow \\theta - \\frac{\\eta}{\\sqrt{E[g^2] + \\epsilon}} \\nabla_\\theta L(\\theta)\\)</p> <p>where:</p> <ul> <li>\\(\\theta\\) is the parameter vector</li> <li>\\(\\eta\\) is the learning rate</li> <li>\\(E[g^2]\\) is the running average of squared gradients</li> <li>\\(\\epsilon\\) prevents division by zero</li> </ul>"},{"location":"prompts/07-equations/#76-weighted-combination-for-multi-head","title":"76. Weighted Combination for Multi-Head","text":"<p>\\(\\text{head}_i = \\text{Attention}(QW_i^Q, \\; KW_i^K, \\; VW_i^V)\\)</p> <p>where:</p> <ul> <li>\\(Q, K, V\\) are query, key, and value matrices</li> <li>\\(W_i^Q, W_i^K, W_i^V\\) are parameter matrices for head \\(i\\)</li> </ul>"},{"location":"prompts/07-equations/#77-gradient-of-mse-wrt-weights","title":"77. Gradient of MSE w.r.t. Weights","text":"<p>\\(\\frac{\\partial L_{\\text{MSE}}}{\\partial w_j} = \\frac{1}{N}\\sum_{i=1}^N 2 \\,(\\hat{y}^{(i)} - y^{(i)}) \\, x_j^{(i)}\\)</p> <p>where:</p> <ul> <li>\\(w_j\\) is weight \\(j\\)</li> <li>\\(x_j^{(i)}\\) is the \\(j\\)-th feature of sample \\(i\\)</li> </ul>"},{"location":"prompts/07-equations/#78-gradient-of-binary-cross-entropy-wrt-weights","title":"78. Gradient of Binary Cross-Entropy w.r.t. Weights","text":"<p>\\(\\frac{\\partial L_{\\text{CE}}}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)})\\, x_j^{(i)}\\)</p> <p>where:</p> <ul> <li>\\(\\hat{y}^{(i)}\\) is the predicted probability</li> <li>\\(y^{(i)}\\) is the true label</li> </ul>"},{"location":"prompts/07-equations/#79-logistic-regression-decision-boundary","title":"79. Logistic Regression Decision Boundary","text":"<p>\\(\\hat{y} = \\sigma(w^\\top x + b) \\implies  \\text{Decision} = \\begin{cases} 1 &amp; \\text{if } w^\\top x + b \\ge 0\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(w, b\\) are parameters</li> <li>\\(x\\) is the input</li> <li>\\(\\sigma\\) is the sigmoid function</li> </ul>"},{"location":"prompts/07-equations/#80-balanced-cross-entropy","title":"80. Balanced Cross-Entropy","text":"<p>\\(L_{\\text{bal}} = -\\frac{1}{N} \\sum_{i=1}^N \\Big[\\beta \\, y^{(i)} \\log(\\hat{y}^{(i)}) + (1-\\beta)(1-y^{(i)}) \\log(1-\\hat{y}^{(i)})\\Big]\\)</p> <p>where:</p> <ul> <li>\\(\\beta\\) is a factor weighting the positive class</li> <li>\\(y^{(i)}, \\hat{y}^{(i)}\\) as before</li> </ul>"},{"location":"prompts/07-equations/#81-f1-score","title":"81. F1 Score","text":"<p>\\(\\text{F1} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\\)</p> <p>where:</p> <ul> <li>\\(\\text{precision} = \\frac{TP}{TP+FP}\\)</li> <li>\\(\\text{recall} = \\frac{TP}{TP+FN}\\)</li> </ul>"},{"location":"prompts/07-equations/#82-precision","title":"82. Precision","text":"<p>\\(\\text{precision} = \\frac{TP}{TP + FP}\\)</p> <p>where:</p> <ul> <li>\\(TP\\) = true positives</li> <li>\\(FP\\) = false positives</li> </ul>"},{"location":"prompts/07-equations/#83-recall","title":"83. Recall","text":"<p>\\(\\text{recall} = \\frac{TP}{TP + FN}\\)</p> <p>where:</p> <ul> <li>\\(TP\\) = true positives</li> <li>\\(FN\\) = false negatives</li> </ul>"},{"location":"prompts/07-equations/#84-auc-area-under-roc-curve","title":"84. AUC (Area Under ROC Curve)","text":"<p>\\(\\text{AUC} = \\int_0^1 TPR(\\text{FPR}^{-1}(x))\\, dx\\)</p> <p>where:</p> <ul> <li>\\(TPR\\) is the true positive rate</li> <li>\\(FPR\\) is the false positive rate</li> <li>The integral is conceptually the area under the ROC curve</li> </ul>"},{"location":"prompts/07-equations/#85-step-decay-learning-rate-scheduling","title":"85. Step Decay (Learning Rate Scheduling)","text":"<p>\\(\\eta_{\\text{new}} = \\eta_{\\text{old}} \\cdot \\gamma^{\\lfloor \\frac{\\text{epoch}}{k}\\rfloor}\\)</p> <p>where:</p> <ul> <li>\\(\\eta_{\\text{old}}\\) is the current learning rate</li> <li>\\(\\eta_{\\text{new}}\\) is the updated learning rate</li> <li>\\(\\gamma \\in (0,1)\\) is the decay factor</li> <li>\\(k\\) is the step size in epochs</li> </ul>"},{"location":"prompts/07-equations/#86-polynomial-kernel-svm","title":"86. Polynomial Kernel (SVM)","text":"<p>\\(K(x, x') = (x^\\top x' + c)^p\\)</p> <p>where:</p> <ul> <li>\\(x, x'\\) are feature vectors</li> <li>\\(c\\) is a constant (often 1)</li> <li>\\(p\\) is the polynomial degree</li> </ul>"},{"location":"prompts/07-equations/#87-radial-basis-function-kernel-rbf","title":"87. Radial Basis Function Kernel (RBF)","text":"<p>\\(K(x, x') = \\exp\\Big(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\Big)\\)</p> <p>where:</p> <ul> <li>\\(x, x'\\) are feature vectors</li> <li>\\(\\sigma\\) is the kernel width</li> </ul>"},{"location":"prompts/07-equations/#88-hinge-loss-svm","title":"88. Hinge Loss (SVM)","text":"<p>\\(L_{\\text{hinge}} = \\max\\big(0, 1 - y_i (w^\\top x_i + b)\\big)\\)</p> <p>where:</p> <ul> <li>\\(y_i \\in \\{-1, +1\\}\\) is the true label</li> <li>\\(w\\) is the weight vector</li> <li>\\(x_i\\) is the input</li> <li>\\(b\\) is the bias</li> </ul>"},{"location":"prompts/07-equations/#89-margin-hard-svm","title":"89. Margin (Hard-SVM)","text":"<p>\\(\\text{Margin} = \\frac{2}{\\|w\\|}\\)</p> <p>where:</p> <ul> <li>\\(w\\) is the normal vector to the decision boundary</li> <li>\\(\\|w\\|\\) is the Euclidean norm of \\(w\\)</li> </ul>"},{"location":"prompts/07-equations/#90-q-learning-update-rl","title":"90. Q-Learning Update (RL)","text":"<p>\\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a'} Q(s', a') - Q(s,a)\\Big]\\)</p> <p>where:</p> <ul> <li>\\(Q(s,a)\\) is the current Q-value</li> <li>\\(\\alpha\\) is the learning rate</li> <li>\\(r\\) is the reward</li> <li>\\(\\gamma\\) is the discount factor</li> <li>\\(s'\\) is the next state</li> <li>\\(a'\\) is the next action</li> </ul>"},{"location":"prompts/07-equations/#91-bellman-equation-value-function","title":"91. Bellman Equation (Value Function)","text":"<p>\\(V^\\pi(s) = \\mathbb{E}_{a\\sim \\pi}\\Big[r(s,a) + \\gamma V^\\pi(s')\\Big]\\)</p> <p>where:</p> <ul> <li>\\(V^\\pi(s)\\) is the value of state \\(s\\) under policy \\(\\pi\\)</li> <li>\\(r(s,a)\\) is the immediate reward</li> <li>\\(\\gamma\\) is the discount factor</li> <li>\\(s'\\) is the next state</li> </ul>"},{"location":"prompts/07-equations/#92-actor-critic-gradient","title":"92. Actor-Critic Gradient","text":"<p>\\(\\nabla_\\theta J(\\theta) = \\mathbb{E}\\big[\\nabla_\\theta \\log \\pi_\\theta(a|s)\\, (Q_w(s,a) - b(s))\\big]\\)</p> <p>where:</p> <ul> <li>\\(\\pi_\\theta\\) is the policy (actor)</li> <li>\\(Q_w(s,a)\\) is the critic\u2019s estimate of action-value</li> <li>\\(b(s)\\) is a baseline to reduce variance</li> </ul>"},{"location":"prompts/07-equations/#93-softplus-activation","title":"93. Softplus Activation","text":"<p>\\(\\text{Softplus}(z) = \\log(1 + e^z)\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> </ul>"},{"location":"prompts/07-equations/#94-elu-exponential-linear-unit","title":"94. ELU (Exponential Linear Unit)","text":"<p>\\(\\text{ELU}(z) = \\begin{cases} z &amp; z \\ge 0,\\\\ \\alpha(e^z - 1) &amp; z &lt; 0 \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(\\alpha\\) is a positive constant</li> </ul>"},{"location":"prompts/07-equations/#95-selu-scaled-elu","title":"95. SELU (Scaled ELU)","text":"<p>\\(\\text{SELU}(z) = \\lambda \\begin{cases} z &amp; z \\ge 0\\\\ \\alpha(e^z - 1) &amp; z &lt; 0 \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(\\alpha\\) and \\(\\lambda\\) are constants (e.g., \\(\\alpha \\approx 1.673\\), \\(\\lambda \\approx 1.051\\))</li> </ul>"},{"location":"prompts/07-equations/#96-knowledge-distillation","title":"96. Knowledge Distillation","text":"<p>\\(L_{\\text{KD}} = \\tau^2 \\cdot \\text{KL}\\big(p_\\text{teacher}(x;\\tau)\\,\\|\\,p_\\text{student}(x;\\tau)\\big)\\)</p> <p>where:</p> <ul> <li>\\(\\tau\\) is the temperature</li> <li>\\(p_\\text{teacher}\\) and \\(p_\\text{student}\\) are softmax outputs of teacher and student networks, respectively</li> </ul>"},{"location":"prompts/07-equations/#97-threshold-function-binary-step","title":"97. Threshold Function (Binary Step)","text":"<p>\\(\\theta(z) = \\begin{cases} 1 &amp; z \\ge 0\\\\ 0 &amp; z &lt; 0 \\end{cases}\\)</p> <p>where:</p> <ul> <li>\\(z\\) is the input</li> </ul>"},{"location":"prompts/07-equations/#98-weighted-hinge-loss","title":"98. Weighted Hinge Loss","text":"<p>\\(L_{\\text{whinge}} = w_i \\,\\max\\big(0, 1 - y_i (w^\\top x_i + b)\\big)\\)</p> <p>where:</p> <ul> <li>\\(w_i\\) is the sample weight or class weight</li> <li>\\(y_i \\in \\{-1, +1\\}\\) is the true label</li> </ul>"},{"location":"prompts/07-equations/#99-perplexity-language-modeling","title":"99. Perplexity (Language Modeling)","text":"<p>\\(\\text{PPL} = \\exp\\Big(\\frac{1}{N}\\sum_{i=1}^{N} -\\log p(w_i)\\Big)\\)</p> <p>where:</p> <ul> <li>\\(w_i\\) are tokens in the sequence</li> <li>\\(p(w_i)\\) is the predicted probability of token \\(w_i\\)</li> <li>\\(N\\) is the total number of tokens</li> </ul>"},{"location":"prompts/07-equations/#100-cycle-consistency-loss-cyclegan","title":"100. Cycle Consistency Loss (CycleGAN)","text":"<p>\\(L_{\\text{cyc}}(G,F) = \\mathbb{E}_{x \\sim p(x)}\\big[\\|F(G(x)) - x\\|\\big] + \\mathbb{E}_{y \\sim p(y)}\\big[\\|G(F(y)) - y\\|\\big]\\)</p> <p>where:</p> <ul> <li>\\(G\\) is the generator mapping domain \\(X \\to Y\\)</li> <li>\\(F\\) is the generator mapping domain \\(Y \\to X\\)</li> <li>\\(x, y\\) are samples from domains \\(X, Y\\)</li> <li>\\(\\|\\cdot\\|\\) is a distance measure (e.g., L1 norm)</li> </ul>"},{"location":"prompts/08-word-cloud/","title":"Word Cloud","text":"<p>Prompt</p> <p>Prompt Name: Wordcloud</p> <p>Create a list of the 50 most important Concepts in the deep learning course.  Use the concept dependency information to find the concepts that have the most other concepts depend upon. Cross check this with what you believe are the most important concepts to learn in a deep learning course. For each concept give it a score of between 40 and 70 for how important the concept is relative to the other concepts in the course.  The more important concepts should have a higher score.</p> <p>Then use the JavaScript wordcloud-2 library to generate the wordcloud for this list of 50 concepts. For each concept, allow the user to click on the link and  have the link go to the ../../glossary.md#[ConceptLabel] link.</p>"},{"location":"prompts/09-suggested-simulations/","title":"Interactive Simulation Concepts for Deep Learning Education","text":"<p>Prompt</p> <p>Our textbook development team would like to use generative AI to create simulations for explaining concepts that are difficult to understand. Of all of the concepts in this course, which concepts might benefit from an interactive simulation?  We are using tools like p5.js to build the simulations.  They are interactive and allow students to  move range control sliders to change the drawing parameters or simulation parameters.  For example to explain what \"parameter count\" is in a model we allow the students to change the number of layers in a neural network, and the number of neurons in a layer.</p> <p>Please list the concepts that can be explained by a simulation. Describe the simulation in detail and suggest what parameters the student might change.</p>"},{"location":"prompts/09-suggested-simulations/#response","title":"Response","text":""},{"location":"prompts/09-suggested-simulations/#1-gradient-descent","title":"1. Gradient Descent","text":"<p>Simulation Description: Visualize gradient descent optimization on a 2D or 3D loss surface. Interactive Parameters:</p> <ul> <li>Learning rate slider (0.001 to 1.0)</li> <li>Momentum coefficient (0 to 0.99)</li> <li>Initial starting point on the surface</li> <li>Type of surface (select between different loss landscapes)</li> <li>Batch size for stochastic gradient descent</li> </ul> <p>Visual Elements:</p> <ul> <li>Animated particle showing optimizer's path</li> <li>Contour plot of the loss surface</li> <li>Real-time plot of loss value vs. iterations</li> <li>Vector arrows showing gradient direction</li> </ul>"},{"location":"prompts/09-suggested-simulations/#2-activation-functions","title":"2. Activation Functions","text":"<p>Simulation Description: Interactive visualization of different activation functions and their derivatives. Interactive Parameters:</p> <ul> <li>Function type selector (ReLU, Sigmoid, Tanh, LeakyReLU)</li> <li>Input range slider (-10 to 10)</li> <li>Slope parameter for LeakyReLU</li> <li>Temperature parameter for Sigmoid</li> </ul> <p>Visual Elements:</p> <ul> <li>Side-by-side plots of function and its derivative</li> <li>Highlighted point showing input/output relationship</li> <li>Distribution of activations for random inputs</li> </ul>"},{"location":"prompts/09-suggested-simulations/#3-backpropagation","title":"3. Backpropagation","text":"<p>Simulation Description: Step-by-step visualization of the backpropagation process in a simple neural network. Interactive Parameters: - Network architecture (number of layers and neurons) - Learning rate - Input values - Target output - Activation function selection</p> <p>Visual Elements: - Color-coded weights showing magnitude - Animated gradient flow - Forward pass highlighting - Backward pass highlighting</p>"},{"location":"prompts/09-suggested-simulations/#4-convolution-operations","title":"4. Convolution Operations","text":"<p>Simulation Description: Visual demonstration of how convolution filters work on images. Interactive Parameters: - Kernel size (3x3, 5x5, 7x7) - Stride length - Padding amount - Custom filter weights - Input image selection</p> <p>Visual Elements: - Sliding window animation - Side-by-side original and filtered images - Heat map of filter responses - Feature map visualization</p>"},{"location":"prompts/09-suggested-simulations/#5-attention-mechanism","title":"5. Attention Mechanism","text":"<p>Simulation Description: Visualization of self-attention in transformer architectures. Interactive Parameters: - Number of attention heads - Query/Key dot product scaling - Input sequence length - Attention dropout rate - Positional encoding strength</p> <p>Visual Elements: - Attention weight matrices as heat maps - Token-to-token connection strength visualization - Multiple head view - Animated attention flow</p>"},{"location":"prompts/09-suggested-simulations/#6-dimensionality-reduction","title":"6. Dimensionality Reduction","text":"<p>Simulation Description: Interactive visualization of PCA, t-SNE, and UMAP. Interactive Parameters: - Algorithm selection (PCA, t-SNE, UMAP) - Number of components/dimensions - Perplexity (t-SNE) - Number of neighbors (UMAP) - Learning rate - Number of iterations</p> <p>Visual Elements: - 2D/3D scatter plots - Variance explained charts for PCA - Animation of dimension reduction process - Cluster separation visualization</p>"},{"location":"prompts/09-suggested-simulations/#7-batch-normalization","title":"7. Batch Normalization","text":"<p>Simulation Description: Visualization of how batch normalization affects feature distributions. Interactive Parameters: - Batch size - Momentum for running statistics - Epsilon value - Feature scale - Feature shift</p> <p>Visual Elements: - Before/after distribution plots - Layer activation histograms - Training speed comparison - Gradient magnitude visualization</p>"},{"location":"prompts/09-suggested-simulations/#8-gan-training","title":"8. GAN Training","text":"<p>Simulation Description: Interactive visualization of the GAN training process. Interactive Parameters: - Latent space dimension - Generator architecture - Discriminator architecture - Learning rates for both networks - Noise distribution parameters</p> <p>Visual Elements: - Generated samples grid - Loss curves for both networks - Discriminator decision boundary - Mode collapse detection</p>"},{"location":"prompts/09-suggested-simulations/#9-dropout","title":"9. Dropout","text":"<p>Simulation Description: Visualization of how dropout affects network activations and prevents co-adaptation. Interactive Parameters: - Dropout rate - Network depth - Number of neurons per layer - Input pattern selection - Training iterations</p> <p>Visual Elements: - Active/inactive neurons highlighting - Ensemble prediction visualization - Feature co-adaptation patterns - Performance comparison plots</p>"},{"location":"prompts/09-suggested-simulations/#10-loss-landscapes","title":"10. Loss Landscapes","text":"<p>Simulation Description: 3D visualization of neural network loss landscapes. Interactive Parameters: - Network architecture - Loss function selection - Weight initialization method - Optimization algorithm - Training data distribution</p> <p>Visual Elements: - 3D surface plot - 2D contour plot - Optimizer trajectory - Local minima highlighting</p>"},{"location":"prompts/09-suggested-simulations/#11-reinforcement-learning","title":"11. Reinforcement Learning","text":"<p>Simulation Description: Interactive grid world or continuous control environment. Interactive Parameters: - Environment complexity - Reward structure - Discount factor - Exploration rate - Learning rate</p> <p>Visual Elements: - Agent movement animation - Q-value heat maps - Policy visualization - Learning curve - State-value function surface</p>"},{"location":"prompts/09-suggested-simulations/#12-vanishingexploding-gradients","title":"12. Vanishing/Exploding Gradients","text":"<p>Simulation Description: Visualization of gradient flow through deep networks. Interactive Parameters: - Network depth - Activation function selection - Weight initialization method - Input data distribution - Learning rate</p> <p>Visual Elements: - Gradient magnitude plots - Layer-wise activation distributions - Training progress comparison - Gradient flow animation</p> <p>These simulations would provide students with hands-on experience and intuitive understanding of complex deep learning concepts. Each simulation should include:</p> <ul> <li>Clear instructions and educational objectives</li> <li>Reset button to start over</li> <li>Save/load capability for specific configurations</li> <li>Export functionality for generated visualizations</li> <li>Tooltips explaining each parameter</li> <li>Performance metrics where applicable</li> </ul> <p>The simulations should be progressive, starting with simple configurations and allowing students to gradually increase complexity as their understanding grows.</p>"},{"location":"prompts/10-activation-functions/","title":"Activation Functions Prompt","text":"<p>Prompt</p> <p>Please create a detailed section of the course  on Activation Functions.  Describe what they are and then give a list of all the different activation functions and what the pros and cons are of each activation function.  Pay attention to how quickly activation functions can be calculated and discuss implications for large scale deep learning projects.  Note when images should be inserted into the text.  Conclude with an analysis of what activation functions are used most commonly in practice.</p> <p>Response: Activation Functions</p> <p>Link to Anthropic Claude Dialog</p>"},{"location":"prompts/11-tensorflow-playground/","title":"Tensorflow Playground","text":"<p>Prompt</p> <p>Please describe Tensorflow Playground.  Why do students like it so much?</p>"},{"location":"prompts/11-tensorflow-playground/#response","title":"Response","text":"<p>TensorFlow Playground is a brilliant example of an interactive neural network visualization tool that makes abstract concepts tangible. Let me explain why it's so effective:</p> <p>Key Features that Make it Engaging:</p>"},{"location":"prompts/11-tensorflow-playground/#1-real-time-visualization","title":"1. Real-time Visualization:","text":"<ul> <li>Students can see neurons activating in real-time</li> <li>Weight values are shown through line thickness and color</li> <li>Feature detectors are visualized directly</li> <li>Loss curves update instantly as the network trains</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#2-interactive-elements","title":"2. Interactive Elements:","text":"<ul> <li>Students can draw their own data points</li> <li>Add/remove layers and neurons with simple clicks</li> <li>Toggle different features on/off</li> <li>Change learning rate and activation functions</li> <li>Select different datasets (spiral, circle, XOR etc.)</li> </ul> <p>Why Students Love It:</p>"},{"location":"prompts/11-tensorflow-playground/#1-immediate-feedback","title":"1. Immediate Feedback","text":"<ul> <li>Every change they make shows instant results</li> <li>They can develop intuition about how parameters affect learning</li> <li>Failed experiments are as instructive as successful ones</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#2-low-barrier-to-entry","title":"2. Low barrier to entry","text":"<ul> <li>No coding required</li> <li>Clean, intuitive interface</li> <li>Works in any browser</li> <li>Free and easily accessible</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#3-conceptual-understanding","title":"3. Conceptual Understanding","text":"<ul> <li>Makes abstract concepts like \"hidden layers\" visual and concrete</li> <li>Shows how neural networks actually separate data</li> <li>Demonstrates concepts like underfitting and overfitting visually</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#4-experimentation-friendly","title":"4. Experimentation-friendly","text":"<ul> <li>Students can test their hypotheses immediately</li> <li>Safe environment to make mistakes</li> <li>Encourages exploration and discovery</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#5-progressive-learning-journey","title":"5.  Progressive Learning Journey:","text":"<ul> <li>Students can start with simple problems (linear separation)</li> <li>Gradually move to more complex tasks (spiral classification)</li> <li>Build confidence through guided discovery</li> <li>Natural progression from basic to advanced concepts</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#6-demystifies-black-box-perception","title":"6.  Demystifies \"Black Box\" Perception:","text":"<ul> <li>Shows exactly how data flows through the network</li> <li>Visualizes internal representations at each layer</li> <li>Makes weight updates visible and understandable</li> <li>Breaks down complex concepts into digestible pieces</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#7-supports-multiple-learning-styles","title":"7.  Supports Multiple Learning Styles:","text":"<ul> <li>Visual learners see the network structure and data</li> <li>Kinesthetic learners can interact and experiment</li> <li>Analytical learners can observe numerical changes</li> <li>Social learners can share and discuss discoveries</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#8-reduces-math-anxiety","title":"8.  Reduces Math Anxiety:","text":"<ul> <li>Makes complex mathematics more approachable</li> <li>Shows concepts without requiring formula memorization</li> <li>Builds intuition before formal mathematical understanding</li> <li>Allows students to focus on concepts rather than calculations</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#9-encourages-what-if-thinking","title":"9.  Encourages \"What If\" Thinking:","text":"<ul> <li>\"What if I add another layer?\"</li> <li>\"What if I change the activation function?\"</li> <li>\"What if I use different features?\"</li> <li>Promotes scientific thinking and hypothesis testing</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#10-builds-problem-solving-skills","title":"10.  Builds Problem-Solving Skills:","text":"<ul> <li>Students learn to diagnose training issues</li> <li>Develop strategies for network architecture design</li> <li>Understand trade-offs in different approaches</li> <li>Learn from both successes and failures</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#11-creates-aha-moments","title":"11.  Creates \"Aha!\" Moments:","text":"<ul> <li>Students can suddenly understand why deep networks work</li> <li>See how feature hierarchies develop</li> <li>Grasp concepts like feature extraction visually</li> <li>Experience the joy of discovery</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#12-bridges-theory-and-practice","title":"12.  Bridges Theory and Practice:","text":"<ul> <li>Connects classroom concepts to real behavior</li> <li>Shows practical implications of theoretical choices</li> <li>Makes abstract mathematics concrete</li> <li>Demonstrates why certain practices work or fail</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#13-promotes-active-learning","title":"13.  Promotes Active Learning:","text":"<ul> <li>Students drive their own exploration</li> <li>Learn through trial and error</li> <li>Can repeat experiments easily</li> <li>Get immediate feedback on their understanding</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#14-builds-confidence","title":"14.  Builds Confidence:","text":"<ul> <li>Success is easily achievable</li> <li>Mistakes are not penalized</li> <li>Progress is visible and rewarding</li> <li>Creates positive associations with neural networks</li> </ul>"},{"location":"prompts/11-tensorflow-playground/#learning-outcomes","title":"Learning Outcomes:","text":"<ul> <li>Better understanding of neural network architecture</li> <li>Intuitive grasp of feature learning</li> <li>Clear visualization of decision boundaries</li> <li>Practical insight into hyperparameter tuning</li> </ul>"},{"location":"prompts/12-timeline/","title":"Timeline of Deep Learning Events","text":"<p>Prompt</p> <p>You are an expert at the history of Deep Learning. Please create a timeline of the 100 most important events in the development of Deep Learning. Return the event list in a single HTML file that could be used by the vis-timeline JavaScript library. An example template is shown below.</p> <pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Timeline&lt;/title&gt;\n  &lt;script type=\"text/javascript\" src=\"https://unpkg.com/vis-timeline@latest/standalone/umd/vis-timeline-graph2d.min.js\"&gt;&lt;/script&gt;\n  &lt;link href=\"https://unpkg.com/vis-timeline@latest/styles/vis-timeline-graph2d.min.css\" rel=\"stylesheet\" type=\"text/css\" /&gt;\n  &lt;style type=\"text/css\"&gt;\n    #visualization {\n      width: 600px;\n      height: 400px;\n      border: 1px solid lightgray;\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;div id=\"visualization\"&gt;&lt;/div&gt;\n&lt;script type=\"text/javascript\"&gt;\n  // DOM element where the Timeline will be attached\n  var container = document.getElementById('visualization');\n\n  // Create a DataSet (allows two way data-binding)\n  var items = new vis.DataSet([\n    {id: 1, content: 'item 1', start: '2014-04-20'},\n    {id: 2, content: 'item 2', start: '2014-04-14'},\n    {id: 3, content: 'item 3', start: '2014-04-18'},\n    {id: 4, content: 'item 4', start: '2014-04-16', end: '2014-04-19'},\n    {id: 5, content: 'item 5', start: '2014-04-25'},\n    {id: 6, content: 'item 6', start: '2014-04-27', type: 'point'}\n  ]);\n\n  // Configuration for the Timeline\n  var options = {};\n\n  // Create a Timeline\n  var timeline = new vis.Timeline(container, items, options);\n&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"prompts/12-timeline/#workflow-for-initial-prompt","title":"Workflow for Initial Prompt","text":"<p>ChatGPT o1 thought about Deep Learning history timeline for 38 seconds</p> <p>Mapping evolution</p> <p>I'm tasked with crafting a timeline of 100 key deep learning events from 1943 to today. This involves producing a comprehensive HTML file using the vis-timeline library, ensuring clarity and segmentation in line with the provided example.</p> <p>Highlighting foundational events</p> <p>I'm mapping out key milestones in deep learning, including McCulloch &amp; Pitts in 1943, Donald Hebb's learning rule in 1949, and diverse breakthroughs through to 2023.</p> <p>Charting milestones</p> <p>I'm noting pivotal moments in deep learning, including the birth of AI in 1956 and Rosenblatt's perceptron in 1957. Progress continues through diverse breakthroughs up to 2023.</p> <p>Tracking key milestones</p> <p>I'm updating the timeline with significant events such as McCulloch &amp; Pitts in 1943, the Turing test in 1950, and Rosenblatt's perceptron in 1957, organizing them chronologically by year.</p> <p>Tracking deep learning strides</p> <p>Mapping out milestones like Minsky &amp; Papert's \"Perceptrons\" in 1969, the AI winter in the 1970s, and transformative advances like \"AlphaGo\" and BERT from 2016 onwards.</p> <p>Mapping key developments</p> <p>I'm laying out pivotal milestones in deep learning, like McCulloch &amp; Pitts' model in 1943, Donald Hebb's learning rule in 1949, and breakthroughs such as transformers from 2017 onwards.</p> <p>Charting progress</p> <p>Mapping significant developments in neural networks, like the Neocognitron's introduction as a precursor to convolutional neural nets in 1979, and the formalization of Hopfield networks for associative memory in 1981.</p> <p>Advancing neural network training</p> <p>Highlighting the creation and application of backpropagation, neural network training techniques, and landmark publications like \"Parallel Distributed Processing\" and LeCun's work on convolutional neural networks.</p> <p>Tracking breakthroughs</p> <p>I'm noting critical advancements like IBM's Deep Blue in 1997, the introduction of LSTM networks, and the availability of GPU programming for computations, which significantly influenced AI research and applications.</p> <p>Tracing key milestones</p> <p>I'm noting the development of deep learning technologies like AlexNet's breakthrough on ImageNet, the emergence of GANs, and the progression of neural machine translation, which have all significantly advanced AI capabilities.</p> <p>Charting AI evolution</p> <p>I'm tracking the progression of AI, showcasing milestones like the introduction of transformers, GPT-3's launch, and the significant advancement of text-to-image generation technologies.</p> <p>Tracking milestones</p> <p>I'm organizing AI milestones for each year, ensuring concise and consistent entries, and wrapping them in HTML with specific formatting. Progress is being made step by step.</p>"},{"location":"prompts/12-timeline/#result","title":"Result","text":"<p>Deep Learning Timeline MicroSim</p>"},{"location":"prompts/12-timeline/#reading-timeline-data-from-a-json-file","title":"Reading Timeline Data from a JSON File","text":"<p>Prompt</p> <p>Rewrite the program to read the timeline data from a JSON file.</p>"},{"location":"prompts/12-timeline/#workflow","title":"Workflow","text":"<p>Thinking through options</p> <p>I'm analyzing how to fetch data from an external JSON file into a vis-timeline. Using a fetch request will streamline the process by pulling the data from the file and then updating the timeline.</p> <p>Naming and consistency</p> <p>I'm keeping the code name and timeline config intact, specifying the JSON file format, and using fetch to pull in data and update the timeline with a single snippet.</p> <p>Below is an example of how you might restructure your HTML/JavaScript so that the timeline data comes from a separate JSON file named <code>items.json</code>. Make sure the <code>items.json</code> file is located in the same directory as your HTML page (or update the <code>fetch</code> path accordingly).</p> <pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Deep Learning Timeline (1935-2025)&lt;/title&gt;\n  &lt;script type=\"text/javascript\" src=\"https://unpkg.com/vis-timeline@latest/standalone/umd/vis-timeline-graph2d.min.js\"&gt;&lt;/script&gt;\n  &lt;link href=\"https://unpkg.com/vis-timeline@latest/styles/vis-timeline-graph2d.min.css\" rel=\"stylesheet\" type=\"text/css\" /&gt;\n  &lt;style type=\"text/css\"&gt;\n    #visualization {\n      width: 90%;\n      height: 600px;\n      border: 1px solid lightgray;\n      margin: 0 auto;\n    }\n    body {\n      font-family: Arial, sans-serif;\n      margin: 20px;\n    }\n    h1 {\n      text-align: center;\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Key Events in Deep Learning History (1935-2025)&lt;/h1&gt;\n&lt;div id=\"visualization\"&gt;&lt;/div&gt;\n\n&lt;script type=\"text/javascript\"&gt;\n  // DOM element where the Timeline will be attached\n  var container = document.getElementById('visualization');\n\n  // Fetch timeline data from external JSON file\n  fetch('items.json')\n    .then(response =&gt; response.json())\n    .then(data =&gt; {\n      // Create a DataSet (allows two way data-binding) using the fetched data\n      var items = new vis.DataSet(data);\n\n      // Configuration for the Timeline\n      var options = {\n        // Restrict the timeline's visible window (and scrolling) to 1935 - 2025\n        min: \"1935-01-01\",\n        max: \"2025-12-31\",\n        start: \"1935-01-01\",\n        end: \"2025-12-31\",\n        stack: true,\n        maxHeight: '90%',\n        minHeight: '300px',\n        margin: {\n          item: 10,\n          axis: 10\n        }\n      };\n\n      // Create and render the Timeline\n      var timeline = new vis.Timeline(container, items, options);\n    })\n    .catch(error =&gt; {\n      console.error('Error loading timeline data:', error);\n    });\n&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"prompts/12-timeline/#cutoff-date-analysis","title":"Cutoff Date Analysis","text":"<p>Prompt</p> <p>What is your cutoff date?  What are the latest events you are aware of?</p>"},{"location":"prompts/12-timeline/#response","title":"Response","text":"<p>Establishing key timelines</p> <p>I\u2019m figuring out the knowledge cutoff for the GPT model, which is January 2021. This means I have no updates on data after that, but can provide some info up to 2023 based on user interactions.</p>"},{"location":"prompts/12-timeline/#workflow_1","title":"Workflow","text":"<p>Addressing knowledge boundaries</p> <p>OK, let me see. I\u2019m noting that the knowledge cutoff for ChatGPT is around 2021, and I\u2019m not current with information post-2021. This indicates a key aspect of the assistant's capabilities.</p>"},{"location":"prompts/12-timeline/#result_1","title":"Result","text":"<p>I don\u2019t have direct, real-time knowledge of current events; instead, I rely on a snapshot (or \u201ccutoff\u201d) of information from my training. Generally, for most of the ChatGPT (GPT-3.5) models, that snapshot includes data up to some point in 2021 (with limited awareness of events after that time).</p> <p>However, since conversation can include user-provided information that extends beyond 2021, I can discuss some more recent topics if those details appear in the prompts (as happened with your timeline request). But strictly speaking, my model itself does not have built-in awareness of events after my original training cutoff. If you share newer information, I can process it in context, but I don\u2019t automatically \u201cknow\u201d facts that emerged after my training window.</p>"},{"location":"prompts/12-timeline/#openai-o1-is-not-self-aware","title":"OpenAI o1 is not Self Aware","text":""},{"location":"prompts/14-alexnet-story/","title":"Generating a Story about AlexNet","text":"<p>Prompt</p> <p>Tell a fun story about how Alexnet won the Imagnet competition in 2012.  Give as much detail as possible.  When appropriate, suggest an image that could be inserted to make the story a form of graphic novel.  Describe each image in detail and be consistent across all the images for style.</p>"},{"location":"prompts/15-ai-winters/","title":"AI Winter","text":"<p>Prompt</p> <p>Create a fun and entertaining story about the \"AI winters\" and how AI grew and died periodically until 2012 before AlexNet. Give as much detail as possible. Include quotes of famous people over-promising when AI would mature.  When appropriate, suggest an image that could be inserted into the story to make the story a form of graphic novel. Describe each image in detail and be consistent across all the images in the story for style.</p>"},{"location":"prompts/13-summarize-folder/","title":"Google Docs Summary of Deep Learning Folder","text":"<p>The folder \"EE 4940\" contains materials related to an introductory course on Deep Learning, likely taught at the University of Minnesota, specifically for EE 4940 in Spring 2025. The materials include lecture slides, homework assignments, and notes related to deep learning concepts and practical applications.</p> <p>Here are some key themes addressed in the folder:</p>"},{"location":"prompts/13-summarize-folder/#introduction-to-deep-learning-and-neural-networks","title":"Introduction to Deep Learning and Neural Networks","text":"<p>The folder covers fundamental concepts of artificial intelligence, machine learning, and deep learning, starting from the basics of perceptrons to more complex architectures like Convolutional Neural Networks (CNNs). It discusses the architecture of artificial neurons, activation functions, loss functions, and the backpropagation algorithm used to train neural networks. There's also a focus on the historical context of AI and the evolution of neural network architectures.</p>"},{"location":"prompts/13-summarize-folder/#practical-applications-and-tools","title":"Practical Applications and Tools","text":"<p>The materials delve into practical aspects of deep learning, including how to use high-performance computing resources like the Agate cluster at the Minnesota Supercomputing Institute (MSI). It mentions ready-to-run containers, Jupyter notebooks, and batch GPU computing for deep learning tasks. There's also information on using frameworks like PyTorch and TensorFlow for implementing deep learning models, as well as discussions about hardware requirements and optimization techniques.</p>"},{"location":"prompts/13-summarize-folder/#training-and-optimization-of-deep-learning-models","title":"Training and Optimization of Deep Learning Models","text":"<p>A significant portion of the folder is dedicated to the training process of neural networks. It covers various optimization algorithms like Stochastic Gradient Descent (SGD), the concept of learning rate, and how to address common issues like vanishing and exploding gradients. The materials also discuss the importance of validation and test sets, overfitting and underfitting, and techniques like dropout to improve model generalization.</p> <p>The \"EE 4940\" folder contains materials related to an \"Introduction to Deep Learning\" course, covering fundamental concepts, neural network architectures, and practical applications.</p> <p>Here are some key themes addressed:</p>"},{"location":"prompts/13-summarize-folder/#deep-learning-fundamentals","title":"Deep Learning Fundamentals","text":"<p>The folder includes materials explaining the basics of deep learning, such as neural network architecture (including perceptrons and deep neural networks), activation functions (like ReLU, sigmoid, and tanh), loss functions (MSE, cross-entropy), and optimization techniques (gradient descent, SGD). These materials aim to provide a solid foundation for understanding how neural networks learn and make predictions.</p>"},{"location":"prompts/13-summarize-folder/#convolutional-neural-networks-cnns","title":"Convolutional Neural Networks (CNNs)","text":"<p>A significant portion of the content focuses on CNNs, which are particularly effective for image processing tasks. The materials discuss how CNNs mimic biological vision by extracting hierarchical features, the concept of convolution as a matrix operation, pooling layers for downsampling, and practical implementations of CNNs for tasks like MNIST digit classification.</p>"},{"location":"prompts/13-summarize-folder/#practical-applications-and-tools_1","title":"Practical Applications and Tools","text":"<p>The folder also touches on practical aspects of deep learning, such as using TensorFlow/Keras and PyTorch for implementing models, using computing resources like the Minnesota Supercomputing Institute's Agate cluster, and the importance of data handling, training/validation/test splits, and techniques to prevent overfitting (like dropout). There are also examples of real-world applications, such as protein folding prediction and AI-assisted coding.</p>"},{"location":"sims/","title":"List of Deep Learning MicroSims","text":""},{"location":"sims/#learning-graph","title":"Learning Graph","text":"<p>Learning Graph</p>"},{"location":"sims/#wordcloud","title":"Wordcloud","text":"<p>Word Cloud</p>"},{"location":"sims/#model-parameter-count","title":"Model Parameter Count","text":"<p>Param Count</p>"},{"location":"sims/#responsive-template","title":"Responsive Template","text":"<p>Responsive Template</p>"},{"location":"sims/chicken-road/","title":"Transformer Attention Demo MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/deep-learning-course/sims/chicken-road/main.html\"  height=\"185px\" scrolling=\"no\"\nstyle=\"overflow: hidden;\"&gt;\n&lt;/iframe&gt;\n</code></pre> <p>Run the Attention MicroSim</p> <p>Edit the Attention MicroSim</p>"},{"location":"sims/chicken-road/#references","title":"References","text":"<p>Chicken Road Circle MicroSim</p>"},{"location":"sims/conv-filter-mult/","title":"Convolution Filter Multiply MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/deep-learning-course/sims/conv-filt-mul/main.html\" width=\"600px\" height=\"450px\" scrolling=\"no\"\n  style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim</p> <p>Edit the MicroSim</p>"},{"location":"sims/convo-with-filters/","title":"Convolution with Filters MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/deep-learning-course/sims/convo-with-filters/main.html\" height=\"500px\" scrolling=\"no\"\n  style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the Convolution with Filter MicroSim</p> <p>Edit the Convolution with Filter MicroSim</p>"},{"location":"sims/convolution/","title":"Convolution MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/deep-learning-course/sims/convolution/main.html\" width=\"600px\" height=\"450px\" scrolling=\"no\"\n  style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the Convolution MicroSim</p> <p>Edit the Convolution MicroSim</p>"},{"location":"sims/four-futures/","title":"Four AI Futures MicroSim","text":"<pre><code>&lt;iframe src=\"./https://dmccreary.github.io/deep-learning-course/sims/four-futures/main.html\" height=\"400px\" scrolling=\"no\"\n  style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the Four Futures MicroSim</p> <p>Edit the Four Futures MicroSim</p> <p>Prompt</p> <p>Create a p5.js MicroSim that generates a line chart. Across the bottom x axis draw an axis label \"Time\". In the center of the Time axis draw a vertical brown line with the label \"Now\".</p> <p>Across the left vertical axis draw an axis with the label \"Rate of Improvement\".</p> <p>Draw four lines on the chart:</p> <ol> <li>Growth and Decline: initial rapid growth but peaking now and will be declining in the future </li> <li>Growth and Flattening: initial rapid growth but flatting out </li> <li>Linear Growth: initial rapid growth but slow linear growth </li> <li>Exponential Growth: initial rapid growth and continued exponential growth</li> </ol> <p>Add a checkbox control to display each option.</p>"},{"location":"sims/learning-graph/","title":"Learning Graph for Deep Learning Course","text":"<p>Run Learning Graph MicroSim</p>"},{"location":"sims/param-count/","title":"Neural Network Parameter Count MicroSim","text":"<p>Run the Neural Network Parameter Count MicroSim</p> <p>Edit this MicroSim</p> <p>Prompt</p> <p>Generate a p5.js file that draws a simple neural network. Use circles to draw the nodes and lines to draw the weights. Use the responsive design template in the project area. The input layer is on the left.  The output layer is on the right. The layer on the left should be labeled \"Input\". The layer on the right should be labeled \"Output\" Add two slider controls in the controls region that change the number of layers and the number of neurons in a layer. Add two text items that show the number of bias weights and the total number of parameters in the model. Display the equation for calculating the total number of parameters.</p>"},{"location":"sims/param-count/#explanation","title":"Explanation","text":"<p>I'll help you calculate the correct formula and create a visualization. Let me analyze the current code and make necessary corrections.</p> <p>Let's first calculate the correct formula:</p> <p>Now I'll create a React component that visualizes this network:</p> <p>Let me explain the formula for calculating the total parameters in a fully connected neural network:</p> <ol> <li>For weights:</li> <li>Each connection between two layers needs a weight parameter</li> <li>For each pair of adjacent layers: (nodes in previous layer \u00d7 nodes in current layer)</li> <li>Since input layer doesn't have incoming connections, we multiply by (numLayers - 1)</li> <li> <p>Total weight parameters = neuronsPerLayer \u00d7 neuronsPerLayer \u00d7 (numLayers - 1)</p> </li> <li> <p>For biases:</p> </li> <li>Each neuron (except in input layer) has one bias parameter</li> <li> <p>Total bias parameters = neuronsPerLayer \u00d7 (numLayers - 1)</p> </li> <li> <p>Total parameters = Total weight parameters + Total bias parameters</p> </li> </ol> <p>For example, in a network with 3 layers (including input) and 4 neurons per layer: - Weight parameters: 4 \u00d7 4 \u00d7 (3-1) = 32 - Bias parameters: 4 \u00d7 (3-1) = 8 - Total parameters: 32 + 8 = 40</p> <p>I've created an interactive visualization where you can: 1. Adjust the number of layers (2-7) 2. Adjust the neurons per layer (2-10) 3. See the network structure 4. Get a breakdown of weight parameters, bias parameters, and total parameters</p> <p>Would you like me to add any additional features to the visualization?</p> <pre><code>// Demo of parameters in neural networks with a responsive design\nlet containerWidth;  // Will be set based on container size\nlet containerHeight = 450;  // Fixed height (400 for drawing + 50 for controls)\nlet drawHeight = 400;\nlet controlHeight = 50;\nlet sliderLeftMargin = 220;\nlet numLayersSlider;\nlet numNeuronsSlider;\nlet parameterCount = 0;\nlet labelValueWidth = 240;\nlet numLayers = 3;  // Default number of layers (including Input and Output)\nlet numNeurons = 4; // Default number of neurons in each layer\n\nfunction setup() {\n    updateCanvasSize();\n    const canvas = createCanvas(containerWidth, containerHeight);\n    var mainElement = document.querySelector('main');\n    canvas.parent(mainElement);\n\n    numLayersSlider = createSlider(2, 7, 3);\n    numLayersSlider.position(sliderLeftMargin, drawHeight + 6);\n    numLayersSlider.size(containerWidth - sliderLeftMargin - 25);\n    numLayersSlider.input(updateNetwork);\n\n    numNeuronsSlider = createSlider(2, 10, 4);\n    numNeuronsSlider.position(sliderLeftMargin, drawHeight + 26);\n    numNeuronsSlider.size(containerWidth - sliderLeftMargin - 25);\n    numNeuronsSlider.input(updateNetwork);\n\n    updateNetwork();\n}\n\nfunction updateCanvasSize() {\n    const container = document.querySelector('main').getBoundingClientRect();\n    containerWidth = Math.floor(container.width);\n}\n\nfunction windowResized() {\n    updateCanvasSize();\n    resizeCanvas(containerWidth, containerHeight);\n\n    // Update slider positions and sizes\n    numLayersSlider.size(containerWidth - sliderLeftMargin - 25);\n    numNeuronsSlider.size(containerWidth - sliderLeftMargin - 25);\n\n    updateNetwork();\n}\n\nfunction updateNetwork() {\n    // make the background of the drawing region a very light blue\n    fill('aliceblue');\n    stroke('silver');\n    rect(0, 0, containerWidth, drawHeight);\n\n    fill('white');\n    rect(0, drawHeight, containerWidth, controlHeight);\n    stroke('blue');\n\n    numLayers = numLayersSlider.value();\n    numNeurons = numNeuronsSlider.value();\n\n    let layerPositions = Array.from(\n        {length: numLayers}, \n        (_, i) =&gt; map(i, 0, numLayers - 1, 0.2, 0.8) * containerWidth\n    );\n\n    parameterCount = 0;\n\n    for (let i = 0; i &lt; layerPositions.length; i++) {\n        let label = \"Hidden\";\n        if (i === 0) label = \"Input\";\n        else if (i === layerPositions.length - 1) label = \"Output\";\n\n        drawNodes(layerPositions[i], drawHeight / 2, numNeurons, label);\n    }\n\n    for (let i = 0; i &lt; layerPositions.length - 1; i++) {\n        connectLayers(\n            layerPositions[i], drawHeight / 2, numNeurons,\n            layerPositions[i + 1], drawHeight / 2, numNeurons\n        );\n        parameterCount += numNeurons * numNeurons; // Adding weights\n    }\n\n    parameterCount += (numLayers - 1) * numNeurons; // Adding biases\n\n    // Show labels and parameter count\n    noStroke();\n    fill('black');\n    textSize(16);\n    textAlign(LEFT);\n    text('Number of Layers: ' + numLayers, 5, numLayersSlider.y + 5);\n    text('Number of Neurons/Layer: ' + numNeurons, 5, numNeuronsSlider.y + 5);\n\n    textAlign(LEFT);\n    text('Bias Weights: ' + numNeurons, 10, drawHeight - 20);\n    textAlign(CENTER, CENTER);\n    text('Total Parameter Count: ' + parameterCount, containerWidth / 2, drawHeight - 20);\n}\n\nfunction drawNodes(x, y, numNodes, label) {\n    stroke('blue');\n    strokeWeight(2);\n    let gap = 40;\n    let startY = y - ((numNodes - 1) * gap) / 2;\n    for (let i = 0; i &lt; numNodes; i++) {\n        ellipse(x, startY + i * gap, 20, 20);\n    }\n\n    noStroke();\n    fill('black');\n    textSize(16);\n    textAlign(CENTER, CENTER);\n    text(label, x, y + ((numNodes + 1) * gap) / 2);\n}\n\nfunction connectLayers(x1, y1, numNodes1, x2, y2, numNodes2) {\n    stroke('blue');\n    strokeWeight(1);\n    let gap1 = 40;\n    let gap2 = 40;\n    let startY1 = y1 - ((numNodes1 - 1) * gap1) / 2;\n    let startY2 = y2 - ((numNodes2 - 1) * gap2) / 2;\n    for (let i = 0; i &lt; numNodes1; i++) {\n        for (let j = 0; j &lt; numNodes2; j++) {\n            line(x1, startY1 + i * gap1, x2, startY2 + j * gap2);\n        }\n    }\n}\n</code></pre>"},{"location":"sims/sliding-window/","title":"CNN Sliding Window MicroSim","text":"<p>Run the Sliding Window MicroSim</p> <p>Edit the Sliding Window MicroSim</p> <p>This CNN Sliding Window MicroSim demonstrates one of the fundamental operations in Convolutional Neural Networks - how a kernel or filter moves across an input matrix to perform convolution.</p>"},{"location":"sims/sliding-window/#key-features","title":"Key Features","text":"<ul> <li>Input Matrix: An 8\u00d78 grid of random integers (0-9) representing pixel values or feature map data</li> <li>Sliding Window: A 3\u00d73 red-outlined window that moves across the input matrix</li> <li>Window Content: A separate display showing the current values inside the sliding window</li> <li>Interactive Controls:<ul> <li>Slider to manually position the sliding window</li> <li>Start/Pause button to automate the window movement</li> <li>Reset button to return the window to the starting position</li> </ul> </li> </ul>"},{"location":"sims/sliding-window/#how-it-works","title":"How It Works","text":"<p>The simulation follows the MicroSim rules with a responsive width layout. It visually demonstrates how convolution works in CNNs by:</p> <ol> <li>Showing the complete input matrix on the left side</li> <li>Highlighting the current 3\u00d73 window position with a red outline</li> <li>Extracting and displaying the values within that window on the right side</li> </ol> <p>This helps learners understand how a CNN processes information by examining small regions at a time, which is the key to how CNNs detect local patterns like edges, textures, and shapes in images.</p>"},{"location":"sims/sliding-window/#learning-value","title":"Learning Value","text":"<p>This simple simulation establishes a foundation for understanding more complex CNN operations. In an actual CNN, mathematical operations (like multiplication with filter weights and summation) would be performed on these window values to produce a single output value in the resulting feature map.</p> <p>You can adjust the sliding window position manually using the slider or watch it move automatically by pressing the Start button. The Reset button returns the window to the initial position (top-left corner).</p>"},{"location":"sims/sliding-window/#changes-for-responsive-design","title":"Changes for Responsive Design","text":"<ul> <li>Replaced static width/height with containerWidth/containerHeight</li> <li>Added updateCanvasSize() function to get container dimensions</li> <li>Added windowResized() handler to update canvas and UI elements</li> <li>Modified slider positioning and sizing to be relative to container width</li> <li>Updated all drawing calculations to use containerWidth instead of fixed width</li> <li>Set a fixed containerHeight while allowing width to be responsive</li> <li>Adjusted margins and spacing to work better with different screen sizes</li> </ul> <p>The visualization will now resize horizontally while maintaining its functionality and proportions.</p>"},{"location":"sims/templates/","title":"MicroSim Templates","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/deep-learning-course/sims/NAM/main.html\"  height=\"450px\" scrolling=\"no\"\n  style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim</p> <p>Edit the MicroSim</p>"},{"location":"sims/timeline/","title":"Deep Learning Timeline MicroSim","text":"<p>Here is a timeline that includes over 100 key events in the field of Deep Learning. The events span from 1935 to present, although events in the last two years had to be entered by hand since ChatGPT o1 is Frozen in Time</p> <p>Run the Deep Learning Timeline MicroSim</p>"},{"location":"sims/wordcloud/","title":"Wordcloud","text":"<p>Run the MicroSim</p>"},{"location":"stories/","title":"Deep Learning Stories","text":"<p>Here are some examples of how Generative AI tools can be used to generate graphic-novel like stories.  I used Anthropic Claude 3.7 to generate the narrative text for these stories.  In my prompt, I stated When appropriate, suggest an image that could be inserted into the story to make the story a form of graphic novel. I then use OpenAI's ChatGPT o4 to generate the images.</p>"},{"location":"stories/#the-ai-winters-story","title":"The AI Winters Story","text":"<p>The AI field experienced dramatic cycles of enthusiasm and disappointment known as \"AI winters\" from the 1950s until 2012. Starting with grand promises from pioneers like McCarthy, Simon, and Minsky in the 1950s-60s, the field faced its first winter in 1973 after the Perceptron limitations were exposed and funding dried up. It rebounded in the 1980s with expert systems, only to crash again by 1987 when these systems proved brittle and impractical. A quieter machine learning era emerged in the 1990s with modest successes like Deep Blue, but it wasn't until 2012 when AlexNet's breakthrough at the ImageNet competition finally broke the boom-bust cycle, ushering in an \"eternal spring\" for AI that continues today.</p> <p>Read The AI Winters Story</p>"},{"location":"stories/#the-alexnet-story","title":"The AlexNet Story","text":"<p>This graphic novel sequence tells the origin story of deep learning's revolution, beginning with \"The Toronto Trio\"\u2014Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton\u2014facing the monumental challenge of neural networks. In \"The GPU Awakening,\" they harness the power of gaming GPUs to accelerate training. \"The Training Vigil\" captures their relentless effort, leading to \"The Revealing,\" where AlexNet stuns the world with a groundbreaking 15.3% error rate. \"The Revolution Begins\" shows the global ripple effect as labs adopt deep learning and AI breakthroughs multiply. The story concludes with \"The Neural Future,\" a hopeful epilogue depicting modern AI applications branching out from the glowing seed of AlexNet.</p> <p>Read the Alexnet Story</p>"},{"location":"stories/#transformers-and-attention","title":"Transformers and Attention","text":"<p>\"Attention Revolution: How a Paper Changed AI Forever\" tells the story of how a team of eight researchers at Google (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, and Illia Polosukhin) revolutionized AI in 2017 with their paper \"Attention Is All You Need.\" Working largely as a side project, they developed the transformer architecture that replaced traditional RNNs and CNNs with a revolutionary self-attention mechanism, allowing words to directly relate to each other regardless of distance. Despite internal concerns about giving away competitive advantages, they published their findings, which quickly became the foundation for modern language models like BERT, GPT, and later large language models. The paper's key innovation\u2014the attention mechanism\u2014proved remarkably versatile and scalable, enabling AI to better understand context and references (like knowing \"it\" refers to \"chicken\" in \"The chicken did not cross the road because it was too tired\"), ultimately transforming not just natural language processing but the entire field of artificial intelligence.</p>"},{"location":"stories/#yann-lecun","title":"Yann LeCun","text":"<p>This story chronicles Yann LeCun's journey from a curious teenager in 1970s Paris to a pioneering AI researcher who revolutionized machine learning. It follows his development of convolutional neural networks (CNNs) at AT&amp;T Bell Labs for optical character recognition, his forward-thinking establishment of the NYU Center for Data Science in 2012, and his collaboration in founding ICLR to unite the field. The narrative culminates with his 2019 Turing Award and explores his thoughtful skepticism about achieving AGI through large language models alone, emphasizing his belief that future AI systems need comprehensive world models to achieve true intelligence. Throughout, the story illustrates how LeCun combined deep technical innovation with visionary leadership to challenge the status quo and transform what was possible in artificial intelligence.</p> <p>Read the Yann LeCun Story</p>"},{"location":"stories/#the-launch-of-chatgpt","title":"The Launch of ChatGPT","text":"<p>This graphic novel chronicles how OpenAI's ChatGPT revolutionized artificial intelligence, beginning with a team of visionaries building on GPT-3's foundation to create a truly conversational AI assistant. Their breakthrough came through Reinforcement Learning from Human Feedback (RLHF), where human trainers guided the system toward more helpful, truthful responses. After an intense development sprint, ChatGPT's November 2022 launch saw unprecedented adoption, reaching a million users in just five days and triggering an industry-wide AI race. The story follows ChatGPT's evolution through GPT-4's multimodal capabilities, showcasing its impact across education, business, and software development while acknowledging ongoing challenges of alignment and truthfulness. Ultimately, ChatGPT transformed AI from an abstract concept into a personal experience, democratizing access to powerful language technology and sparking crucial global conversations about AI's role in society.</p> <p>Read the ChatGPT Story</p>"},{"location":"stories/#hugging-faces","title":"Hugging Faces","text":"<p>Hugging Face was founded in 2016 by three French entrepreneurs - Cl\u00e9ment Delangue, Julien Chaumond, and Thomas Wolf - who initially created a chatbot app. The company, named after the hugging face emoji (\ud83e\udd17), pivoted from its consumer-focused beginnings when they made a crucial decision to open-source the natural language processing models powering their chatbot, which unexpectedly generated tremendous interest from the AI developer community. This strategic shift transformed Hugging Face into an open platform for machine learning, hosting repositories of models, datasets, and applications that has now grown to include over a million public models. The company has raised approximately $400 million across multiple funding rounds, reaching a valuation of $4.5 billion in 2023, and has expanded while maintaining its commitment to democratizing AI through open-source collaboration rather than the closed, proprietary approaches favored by tech giants.</p> <p>Read the Attention Revolution</p>"},{"location":"stories/#diffusion-dreams-the-epic-tale-of-text-to-image-revolution","title":"Diffusion Dreams: The Epic Tale of Text-to-Image Revolution","text":"<p>This story chronicles how three brilliant researchers transformed AI image generation through their complementary breakthroughs: Jonathan Ho revolutionized the mathematical framework of diffusion models in 2020, making them viable for high-quality image creation; Robin Rombach tackled the massive computational requirements by developing Latent Diffusion Models that compressed the process into a more efficient space; and Aditya Ramesh bridged the gap between language and visuals by creating systems that could accurately interpret and visualize natural language prompts. Their combined innovations culminated in powerful, accessible text-to-image systems that democratized visual creation, enabling anyone\u2014from professional artists to everyday people\u2014to generate sophisticated imagery from simple text descriptions, ultimately revolutionizing fields from art and design to education and publishing, including the ability to create entire graphic novels through a collaborative process between human creativity and machine capabilities.</p> <p>Read the Diffusion Dreams Story</p>"},{"location":"stories/#metas-llama","title":"Meta's Llama","text":"<p>In a bold strategic move that reshaped the AI industry, Meta invested billions to develop its Llama large language models, only to release them as open-source technology available to everyone. This \"graphic novel\" chronicles how Meta assembled an elite team of AI researchers, overcame massive computational challenges requiring thousands of specialized GPUs, and spent approximately $5-7 billion on training before making the shocking decision to release Llama 2 with an \"open forever\" commitment in July 2023. The decision effectively implemented a \"scorched earth\" strategy that undermined competitors' ability to charge premium prices for similar technology, transformed the AI job market from core model development to application building, and democratized access to advanced AI. While causing short-term disruption for companies planning to sell proprietary models, Meta's decision ultimately accelerated innovation by allowing developers worldwide to build on top of these powerful models rather than recreating them, all while strategically positioning Meta to focus on integrating AI throughout its own products and metaverse vision.</p> <p>Read the Meta Llama Story</p>"},{"location":"stories/#deepseek-r1","title":"DeepSeek R1","text":"<p>The DeepSeek-R1 story portrays a small, resourceful team challenging the AI industry's \"bigger is better\" paradigm by creating a remarkably efficient language model through innovative architecture choices. Using Mixture of Experts technology, custom software optimization, and strategic resource management, they build a powerful AI system that can run on consumer-grade hardware while delivering capabilities comparable to models requiring vastly more resources. The narrative highlights how their focus on data quality over quantity, specialized training methodologies, and superior engineering allows DeepSeek-R1 to democratize AI access\u2014enabling applications like generating entire educational textbooks on personal computers and empowering users worldwide who lack enterprise infrastructure, ultimately proving that thoughtful design can triumph over brute-force approaches in advancing AI technology.</p> <p>Read the DeepSeek R1 Story</p>"},{"location":"stories/ai-winters/","title":"The AI Winters: A Tale of Boom and Bust","text":"Narrative Prompt Create a fun and entertaining  story about the \"AI winters\" and how AI grew and died periodically until 2012 before AlexNet. Give as much detail as possible. Include quotes of famous people overpromising when AI would mature.  When appropriate, suggest an image that could be inserted into the story to make the story a form of graphic novel. Describe each image in detail and be consistent across all the images in the story for style.  Image Prompt The Cycles of AI - A grand timeline visualization showing a wavy roller-coaster like path from the 1950s to 2012, with dramatic peaks and valleys marked by calendar pages fluttering away. In the foreground, various symbolic elements from each era (vacuum tubes, robots, neurons) are scattered like autumn leaves. Art style: detailed line art with watercolor washes, muted blues and grays for winter periods, warm oranges and yellows for boom periods."},{"location":"stories/ai-winters/#chapter-1-the-birth-of-a-dream-1950s-1960s","title":"Chapter 1: The Birth of a Dream (1950s-1960s)","text":"Image Prompt  \"The Dartmouth Dreamers\" - A retro-style illustration showing a summer campus scene at Dartmouth College. In the foreground, four enthusiastic men in 1950s attire stand before a chalkboard covered in flowcharts and equations. Speech bubbles above their heads show thought clouds containing robots, thinking machines, and neural networks. Art style: Norman Rockwell meets mid-century scientific illustration.  <p>It was the summer of 1956 when John McCarthy invited fellow dreamers to Dartmouth College for what would become known as the birth of artificial intelligence. McCarthy boldly declared:</p> <p>\"We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956... An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.\"</p> <p>Herbert Simon and Allen Newell were even bolder, proclaiming at the conference: \"Within ten years, a digital computer will be the world's chess champion\" and \"within twenty years, machines will be capable of doing any work a man can do.\"</p> <p>The optimism was intoxicating. Money poured in from government and industry. Marvin Minsky predicted in 1970: \"In from three to eight years we will have a machine with the general intelligence of an average human being.\"</p>"},{"location":"stories/ai-winters/#chapter-2-the-first-ai-winter-1973-1980","title":"Chapter 2: The First AI Winter (1973-1980)","text":"Image Prompt  \"The Perceptron's Demise\" - A gothic-horror inspired scene showing a mechanical brain or perceptron being dissected on a table, with Minsky and Papert looming over it holding mathematical equations like scalpels. Snow blows through broken windows of a once-grand laboratory. Art style: Edward Gorey-esque crosshatching with dramatic shadows.  <p>The first winter came suddenly. In 1969, Marvin Minsky and Seymour Papert published \"Perceptrons,\" mathematically proving that simple neural networks couldn't even solve basic problems like XOR. The book was like a bucket of ice water on the field's optimism.</p> <p>Government funding dried up after the 1973 Lighthill Report called AI research a dead end. Labs shuttered. Graduate students fled to other fields. The promised thinking machines had failed to materialize, and the world lost patience.</p>"},{"location":"stories/ai-winters/#chapter-3-the-expert-systems-spring-1980-1987","title":"Chapter 3: The Expert Systems Spring (1980-1987)","text":"Image Prompt  \"The Rule Kingdom\" - An illustration in the style of a medieval manuscript showing programmers as monks transcribing thousands of IF-THEN rules into giant books. In the background, executives throw gold coins at them. A mechanical decision tree towers like a cathedral spire. Art style: illuminated manuscript meets corporate tech, rich golds and blues.  <p>Like crocuses pushing through snow, AI began to bloom again with expert systems. These rule-based programs promised to capture human expertise in code. Edward Feigenbaum declared:</p> <p>\"The world is moving from the industrial age to the age of knowledge... knowledge is power.\"</p> <p>DEC's XCON system saved the company $40 million a year configuring computers. Japan announced their Fifth Generation Computer project in 1982, promising to create intelligent computers by 1992. The AI race was back on.</p>"},{"location":"stories/ai-winters/#chapter-4-the-second-ai-winter-1987-1993","title":"Chapter 4: The Second AI Winter (1987-1993)","text":"Image Prompt  \"The Symbolic Collapse\" - A surreal Dali-esque landscape where giant IF-THEN rules melt like clocks, expert system books crumble to dust, and abandoned LISP machines rust in a wasteland. Government officials and investors flee under dark storm clouds. Art style: Salvador Dali meets corporate dystopia.  <p>But expert systems hit a wall. They were brittle, expensive to maintain, and couldn't handle the messy real world. By 1987, the AI hardware market collapsed. The Japanese Fifth Generation project failed to deliver on its promises.</p> <p>Rodney Brooks wrote a scathing critique in 1990: \"The symbol system hypothesis upon which classical AI is based is fundamentally flawed.\" Companies that had bet heavily on AI went bankrupt. Another winter had begun.</p>"},{"location":"stories/ai-winters/#chapter-5-the-machine-learning-spring-1993-2001","title":"Chapter 5: The Machine Learning Spring (1993-2001)","text":"Image Prompt  \"The Data Dawn\" - A scene showing nerdy programmers in 90s attire feeding stacks of data into computers while decision trees and support vector machines grow like plants from the machines. In the background, the early internet is represented as glowing fiber optic cables spreading like vines. Art style: cyber-renaissance, mixing classical composition with 90s tech aesthetics.  <p>AI quietly reinvented itself. Machine learning algorithms that could learn from data began to show promise. IBM's Deep Blue defeated world chess champion Garry Kasparov in 1997. Search engines started using statistical methods to rank web pages.</p> <p>Vladimir Vapnik developed support vector machines. Yann LeCun built convolutional neural networks for handwriting recognition. But these successes were modest compared to earlier promises.</p>"},{"location":"stories/ai-winters/#chapter-6-the-deep-learning-revolution-2006-2012","title":"Chapter 6: The Deep Learning Revolution (2006-2012)","text":"Image Prompt  \"The Persistent Professor\" - Geoffrey Hinton is shown as a lone figure nurturing a small neural network plant in a cold, abandoned greenhouse. Through broken glass, we can see other researchers walking away. On his desk are piles of rejected grant applications. Art style: melancholic botanical illustration with a hint of steampunk.  <p>While most had abandoned neural networks, Geoffrey Hinton kept the faith. In 2006, he discovered a way to train deep neural networks using a technique called \"pre-training.\" But few paid attention.</p> <p>Hinton later recalled: \"For many years, deep neural networks were considered a joke in the machine learning community.\"</p>"},{"location":"stories/ai-winters/#chapter-7-the-alexnet-breakthrough-2012","title":"Chapter 7: The AlexNet Breakthrough (2012)","text":"Image Prompt  \"The ImageNet Triumph\" - A heroic scene showing AlexNet as a glowing neural network architecture towering over a field of failed algorithms. At its base, the Toronto team raises their GPUs like olympic torches. In the sky, the error rate plummets like a shooting star. Art style: heroic socialist realism meets cyberpunk, dramatic angles and lighting.  <p>And then came 2012. AlexNet's victory at ImageNet shattered decades of conventional wisdom. The 10.8% improvement wasn't just impressive\u2014it was revolutionary.</p> <p>Geoffrey Hinton reflected: \"It was pretty clear that as soon as we got deep learning to work well on ImageNet, the game was over. The people who thought neural nets were never going to work were always going to find reasons why the next thing wouldn't work.\"</p>"},{"location":"stories/ai-winters/#epilogue-the-eternal-spring","title":"Epilogue: The Eternal Spring","text":"Image Prompt  \"The Endless Spring\" - A final panoramic view showing all the AI winters and springs as a continuous landscape, but now with a great branching tree of achievement rising from AlexNet, its branches reaching into a starry sky filled with AI applications. The art style combines all previous styles into a cohesive grand finale.  <p>After AlexNet, there would be no more winters. The combination of big data, GPU acceleration, and improved algorithms had finally unlocked the promise of AI. The boom-and-bust cycle that had characterized the field for half a century was over.</p> <p>The AI winters teach us that technology rarely advances in a straight line. Progress comes in fits and starts, with periods of irrational exuberance followed by crushing disappointment. But sometimes, persistence in the face of skepticism leads to breakthroughs that change everything.</p> <p>As we look to the future of AI, it's worth remembering this history\u2014not just to avoid repeating mistakes, but to appreciate how today's achievements were built on decades of perseverance through the coldest winters.</p>"},{"location":"stories/alexnet/","title":"The Day AlexNet Changed Everything: The ImageNet Conquest of 2012","text":"Image Prompt Create a cover image for this story.  <p>In the world of artificial intelligence, there are watershed moments that fundamentally change the course of history. The ImageNet 2012 competition was one such moment, where an unlikely group of researchers staged what would become known as the \"deep learning revolution.\"</p>"},{"location":"stories/alexnet/#the-underdog-challengers","title":"The Underdog Challengers","text":"Image Prompt  1: \"The Toronto Trio\" - A stylized graphic novel panel showing three figures in silhouette standing before a massive wall of circuit boards and mathematical equations. Hands on hips, they look up at the challenge ahead. The art style should be high-contrast black and white with bold digital line work, reminiscent of a superhero origin story.  The students are Alex Krizhevsky, Ilya Sutskever, and their advisor Geoffrey Hinton.  <p>In 2012, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was already in its third year. The competition asked teams to classify millions of images into 1,000 different categories\u2014everything from goldfish to golf balls, from strawberries to space shuttles. Previous winners had achieved error rates of around 26%, using methods that had dominated computer vision for decades.</p> <p>Enter three researchers from the University of Toronto: Alex Krizhevsky, Ilya Sutskever, and their advisor Geoffrey Hinton. They had an audacious plan: to use neural networks, a technology most researchers had written off as computationally infeasible, to tackle the world's largest image recognition challenge.</p>"},{"location":"stories/alexnet/#the-secret-weapon-gpu-acceleration","title":"The Secret Weapon: GPU Acceleration","text":"Image Prompt  2: \"The GPU Awakening\" - A dynamic comic panel showing hands connecting gaming graphics cards into a server rack, with electric energy bolts emanating from the connections. The scene is rendered in neon blues and purples against a dark background, cyberpunk-style.  <p>Krizhevsky had a brilliant insight: while CPUs were too slow to train large neural networks, the massively parallel architecture of gaming GPUs could potentially speed up the process by orders of magnitude. The team secured two NVIDIA GTX 580 GPUs, each with just 3GB of memory\u2014a pittance by today's standards.</p> <p>Their architecture, which would later be named AlexNet after Krizhevsky, was unlike anything the computer vision community had seen: - 60 million parameters - 5 convolutional layers - 3 fully connected layers - Novel use of ReLU activation functions - Local response normalization - Overlapping pooling - Dropout for regularization</p>"},{"location":"stories/alexnet/#training-day","title":"Training Day","text":"Image Prompt  3: \"The Training Vigil\" - A split-panel showing: Left side, researchers hunched over computers with coffee cups and pizza boxes scattered around, code scrolling on screens. Right side, a visualization of neural networks learning patterns, represented as streams of colored light flowing through an abstract brain structure. The art style mixes realistic elements with digital abstraction.  <p>The team spent weeks training their network on 1.2 million images. The training process was grueling\u2014they had to split the network across two GPUs just to fit it in memory. Each epoch took hours, and they needed to run hundreds of epochs to achieve convergence. The lab became their second home, with team members taking shifts to monitor the training process day and night.</p>"},{"location":"stories/alexnet/#competition-day","title":"Competition Day","text":"Image Prompt  4: \"The Revealing\" - A dramatic panel showing the moment the results are displayed on a giant screen. In the foreground, the crowd gasps in shock, while the Toronto team stands quietly confident. The screen shows: \"AlexNet: 15.3% error rate\" with second place at \"26.2%\". The scene is rendered in a cinematic style with dramatic lighting.  <p>When the results were announced, the computer vision community was stunned. AlexNet achieved a top-5 error rate of 15.3%, an astounding 10.8% improvement over the second-place finisher. It wasn't just an incremental improvement\u2014it was a quantum leap that shattered conventional wisdom.</p>"},{"location":"stories/alexnet/#the-aftermath","title":"The Aftermath","text":"Image Prompt  5: \"The Revolution Begins\" - A montage panel showing the aftermath: labs around the world switching to deep learning, GPUs being installed in research centers, and a timeline arrow shooting upward with AI breakthroughs popping up like sparks. The art style transitions from the grounded reality of the previous panels to a more abstract, inspiring vision of the future.  <p>The victory of AlexNet marked the beginning of the deep learning revolution. Almost overnight, computer vision researchers around the world began adopting neural networks. The demand for GPUs skyrocketed, NVIDIA's stock price soared, and deep learning became the dominant paradigm in AI research.</p> <p>Geoffrey Hinton would later call it \"the moment when deep learning went from being a niche pursuit to being mainstream.\" Alex Krizhevsky went on to work at Google, then launched his own company. Ilya Sutskever would become one of the co-founders of OpenAI.</p>"},{"location":"stories/alexnet/#legacy","title":"Legacy","text":"Image Prompt  6: \"The Neural Future\" - An epilogue panel showing modern AI applications branching out from AlexNet like a technological tree of life: self-driving cars, medical imaging, facial recognition, and creative AI. At the center is the original AlexNet architecture diagram, glowing like a seed. The art style is optimistic and bright, with a touch of retro-futurism.  <p>Today, AlexNet seems almost quaint compared to massive transformers and diffusion models. But its impact cannot be overstated. It proved that with enough data and computational power, neural networks could achieve what decades of hand-crafted algorithms could not. It sparked a paradigm shift that led to today's AI revolution.</p> <p>The story of AlexNet is a reminder that sometimes, the most revolutionary breakthroughs come not from incremental improvements, but from those willing to challenge conventional wisdom and pursue ideas that others have dismissed as impossible.</p> <p>This graphic novel-style account celebrates the historic moment when AlexNet changed the trajectory of artificial intelligence forever, proving that sometimes the greatest revolutions start with a few researchers, some gaming hardware, and an unwavering belief in an idea whose time has come.</p>"},{"location":"stories/alexnet/#references","title":"References","text":"<ul> <li>ImageNet Classification with Deep Convolutional Neural Networks - 2012, Advances in Neural Information Processing Systems (NIPS)</li> <li>The History of AlexNet - 2020, Medium</li> <li>AlexNet: The Architecture That Challenged CNNs - 2020, Towards Data Science</li> <li>Machine Learning Yearning - 2018, deeplearning.ai, by Andrew Ng</li> <li>Deep Learning Revolution: How a Toronto team's 'AlexNet' changed AI forever - 2019, CBC News</li> <li>New AI Milestone: Deep Learning Conquers ImageNet - 2014, Forbes</li> <li>The Deep Learning Revolution - 2018, MIT Press, by Terrence J. Sejnowski</li> <li>Geoffrey Hinton: On Radiology - 2016, YouTube</li> <li>Ilya Sutskever: The Future of AI Research - 2018, OpenAI Blog</li> <li>The Rise of GPU Computing - 2022, NVIDIA Official Website</li> </ul>"},{"location":"stories/chatgpt/","title":"Dawn of the Conversation Revolution: The ChatGPT Story","text":"Show Narrative Prompt Given to Anthropic Claud 3.7 in the context of the Deep Learning course project.  Please create a detailed, fun and entertaining story about the launch of the OpenAI product ChatGPT and its relevance to the rise of AI technologies.  Our goal is to have you generate the full text of the story, but to turn the story into a graphic novel with many illustrations that explain how the product ChatGPT was created.  Describe the challenges the team had and how they overcame these challenges.  When appropriate, suggest an image that could be inserted into the story to make the story a graphic novel. Describe each image in detail and be consistent across all the images in the story for style.  When you describe an image, make sure to mention that it should be a colorful, bright wide-landscape drawing suitable for technology-forward optimistic graphic-novel.  All image prompts were given to OpenAI 4o."},{"location":"stories/chatgpt/#chapter-1-seeds-of-innovation","title":"Chapter 1: Seeds of Innovation","text":"Show Image Prompt   1: A colorful, bright wide-landscape drawing showing San Francisco skyline with the OpenAI office building prominently featured. A diverse group of researchers and engineers are gathered around a whiteboard filled with neural network diagrams. The art style should be vibrant and optimistic, with a tech-forward aesthetic reminiscent of modern graphic novels. Stylized light bulbs float above their heads, representing the moment of inspiration.  <p>In the heart of San Francisco, a team of visionaries at OpenAI had been pushing the boundaries of artificial intelligence for years. Founded in 2015 with the mission to ensure that artificial general intelligence benefits all of humanity, OpenAI had already made significant contributions to the field with models like GPT-1 and GPT-2.</p> <p>But by early 2022, the team was ready to take on a bigger challenge\u2014creating an AI assistant that could have natural, helpful conversations with humans across an unprecedented range of topics. The groundwork had been laid with GPT-3, released in 2020, which demonstrated remarkable language capabilities but still lacked the refinement needed for a truly conversational AI.</p>"},{"location":"stories/chatgpt/#chapter-2-building-the-foundation","title":"Chapter 2: Building the Foundation","text":"<p> Show Image Prompt   2: A colorful, bright wide-landscape drawing depicting a massive digital architecture floating in a stylized cyberspace. The GPT architecture is visualized as an enormous glowing structure with billions of interconnected nodes. Engineers in futuristic workstations appear to be sculpting parts of this architecture, with holographic code surrounding them. The image should convey both scale and precision, with a sense of awe at the magnitude of the project. The color palette should feature deep blues and bright teals, signature colors of AI visualization.    </p> <p>The journey to ChatGPT began with GPT-3.5, a model trained on an enormous dataset of text from the internet. The scale was staggering\u2014hundreds of billions of parameters, trained on trillions of words. The compute resources required were immense, necessitating specialized infrastructure and optimization techniques.</p> <p>\"We needed to build a model that could not only generate coherent text but understand context, follow instructions, admit its mistakes, and challenge incorrect premises,\" explained one team member. \"This required both advances in the underlying technology and new approaches to alignment with human values.\"</p> <p>The team faced numerous technical challenges: how to make the model helpful without being harmful, truthful without being manipulative, and versatile without losing coherence. They experimented with various training methodologies, pushing the boundaries of what was possible.</p>"},{"location":"stories/chatgpt/#chapter-3-the-human-touch-rlhf","title":"Chapter 3: The Human Touch: RLHF","text":"<p> Show Narrative Prompt   Please generate another drawing.   3: A colorful, bright wide-landscape drawing showing a split scene. On the left, human trainers are providing feedback to AI responses on sleek monitors. On the right, this feedback is visualized as flowing into a reinforcement learning system, represented by a maze-like structure where AI agents learn to navigate towards positive outcomes. The style should be dynamic and educational, with visual metaphors like thumbs-up symbols transforming into neural pathways. The color scheme should incorporate warm human tones contrasting with cool AI blues, symbolizing the human-AI collaboration.    </p> <p>The breakthrough came with a technique called Reinforcement Learning from Human Feedback (RLHF). Rather than simply training the model on text prediction, the team trained it to align with human preferences.</p> <p>\"We would generate multiple responses from the model, then have human trainers rank them,\" recalled one researcher. \"This feedback was used to train a reward model, which in turn guided the AI to generate more helpful, truthful, and harmless responses.\"</p> <p>The process was iterative and labor-intensive. Human trainers spent countless hours providing feedback, carefully evaluating responses for accuracy, helpfulness, potential harms, and alignment with instructions. Each iteration brought improvements, gradually shaping the system toward more human-aligned behavior.</p> <p>This human-in-the-loop approach represented a significant shift in AI development\u2014acknowledging that pure data-driven approaches weren't enough to create AI systems that truly served human needs.</p>"},{"location":"stories/chatgpt/#chapter-4-the-final-sprint","title":"Chapter 4: The Final Sprint","text":"<p> Show Image Prompt Please generate another drawing.  4: A colorful, bright wide-landscape drawing depicting an intense but energetic \"war room\" scenario. The OpenAI team is shown in the final weeks before launch, surrounded by multiple screens showing test conversations, metrics, and countdown timers. Some team members are debugging issues, others are reviewing feedback, while others are preparing launch materials. The atmosphere should convey productive tension and excitement. The art style should use dramatic lighting with the glow of screens illuminating determined faces, creating a sense of momentous anticipation.  </p> <p>As November 2022 approached, the pace intensified. The team worked around the clock, fixing bugs, addressing edge cases, and fine-tuning the system's capabilities. Testing revealed both impressive capabilities and concerning limitations.</p> <p>\"We discovered it could write poetry, explain complex scientific concepts, and help debug code\u2014but it could also make up facts or produce harmful content if prompted in certain ways,\" noted one engineer. \"We needed guardrails without making the system too restricted to be useful.\"</p> <p>The team implemented numerous safety measures, built monitoring systems, and prepared for a research preview\u2014a limited release that would allow them to gather real-world feedback while maintaining the ability to make improvements.</p> <p>\"We knew it wasn't perfect,\" said one team member. \"But we also knew that getting it into people's hands was the best way to understand its real-world impact and continue improving.\"</p>"},{"location":"stories/chatgpt/#chapter-5-the-launch-that-broke-the-internet","title":"Chapter 5: The Launch That Broke the Internet","text":"<p> Show Image Prompt     Please generate another drawing. 5: A colorful, bright wide-landscape drawing showing the moment of ChatGPT's launch. The central image shows a stylized \"launch button\" being pressed, with digital energy radiating outward. Around this central action, show a montage of people from diverse backgrounds and professions discovering and using ChatGPT for the first time, with expressions of amazement. Include visualizations of server load metrics spiking dramatically and social media reaction spreading virally across the globe. The style should be vibrant and kinetic, capturing the explosive moment when ChatGPT captured worldwide attention.  </p> <p>On November 30, 2022, OpenAI released ChatGPT as a research preview. What happened next exceeded everyone's expectations.</p> <p></p> Show Image Prompt    Please regenerate this chart using the color theme of the other images in this story.     ```python    import matplotlib.pyplot as plt  # Data from the image products = [     \"Netflix\", \"Kickstarter\", \"Airbnb\", \"Twitter\", \"Foursquare\",     \"Facebook\", \"Dropbox\", \"Spotify\", \"Instagram\", \"ChatGPT\" ] years = [1999, 2009, 2008, 2006, 2009, 2004, 2008, 2008, 2010, 2022] times = [3.5, 2.5, 2.5, 2, 13/12, 10/12, 7/12, 5/12, 75/365, 5/365]  # Time to 1M users in years  # Colors to match the style from earlier AI impact visuals: warm gradient at bottom to cool at top colors = [     \"#ff7043\", \"#ffa726\", \"#ffca28\", \"#ffee58\", \"#d4e157\",     \"#9ccc65\", \"#66bb6a\", \"#26a69a\", \"#42a5f5\", \"#7e57c2\" ][::-1]  # reverse to match data order from bottom up  fig, ax = plt.subplots(figsize=(10, 6)) bars = ax.barh(products[::-1], times[::-1], color=colors)  # Annotations for bar, time, year in zip(bars, times[::-1], years[::-1]):     ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,             f\"{round(time*365)} days\\n({year})\", va='center', fontsize=9)  # Styling ax.set_xlabel(\"Time to reach 1 million users (in years)\") ax.set_title(\"Time to Reach 1 Million Users\", fontsize=14, fontweight='bold') ax.set_xlim(0, 4) ax.invert_yaxis() ax.grid(axis='x', linestyle='--', alpha=0.6) ax.set_facecolor('#fff8f0') fig.patch.set_facecolor('#fff8f0')  plt.tight_layout() plt.savefig(\"/mnt/data/time_to_1_million_users_redraw.png\", dpi=300) plt.show() ```  <p>Within five days, ChatGPT had over one million users. Social media exploded with examples of its capabilities\u2014writing essays, creating poetry, explaining complex topics, drafting emails, and more. The servers struggled under the unprecedented demand, frequently showing capacity messages to new users.</p> <p>\"We expected interest, but nothing like what we saw,\" recalled one team member. \"It was simultaneously exhilarating and terrifying. We'd created something that resonated with people in a way no previous AI system had.\"</p> <p>Unlike previous AI models that had primarily interested researchers and specialists, ChatGPT captured the public imagination. People were using it for homework help, creative writing, coding assistance, trip planning, and countless other applications the team hadn't anticipated.</p>"},{"location":"stories/chatgpt/#chapter-6-the-response-and-ripple-effects","title":"Chapter 6: The Response and Ripple Effects","text":"Show Image Prompt     Please generate another drawing. 6: A colorful, bright wide-landscape drawing illustrating the wide-ranging impact of ChatGPT across society. The image should be divided into sections showing different sectors: education (students and teachers using AI), business (startups forming around the technology), media (journalists reporting on the phenomenon), and competition (other tech companies rushing to develop rival models). In the center, show ChatGPT as a ripple effect emanating outward. The style should be information-rich but visually cohesive, with a clean modern aesthetic that conveys how the technology is integrating into everyday life.  <p>The response to ChatGPT was swift and far-reaching. Investors poured money into AI startups. Established tech companies accelerated their own conversational AI projects. Schools and universities grappled with how to adapt their teaching and assessment methods. Businesses began exploring how to integrate the technology into their operations.</p> <p>Within months, Microsoft had invested billions in OpenAI and begun integrating ChatGPT technology into its Bing search engine and other products. Google declared a \"code red\" and rushed to release its own conversational AI, Bard. The AI arms race had begun in earnest.</p> <p>But alongside the excitement came concerns and criticisms. Some worried about AI's impact on jobs, particularly in creative and knowledge work. Others pointed out the model's tendency to present incorrect information confidently\u2014a problem dubbed \"hallucination.\" Privacy advocates raised questions about the data used to train these systems.</p> <p>The OpenAI team worked to address these concerns while continuing to improve the system. In January 2023, they announced ChatGPT Plus, a subscription service offering reliable access, faster responses, and priority access to new features.</p>"},{"location":"stories/chatgpt/#chapter-7-gpt-4-raises-the-bar","title":"Chapter 7: GPT-4 Raises the Bar","text":"<p> Show Image Prompt     Please generate another drawing. 7: A colorful, bright wide-landscape drawing showing the evolution from ChatGPT to GPT-4. Visualize this as an ascending staircase or spiral of capability, with each step representing a significant improvement. Show GPT-4 handling multiple types of content - text, images, charts, and complex problems. Include visual representations of GPT-4's improved reasoning abilities, showing it solving complex puzzles or analyzing visual information. The art style should emphasize evolution and advancement, with increasingly sophisticated patterns as the eye moves upward through the image.  </p> <p>In March 2023, OpenAI unveiled GPT-4, a significantly more advanced model that powered the next generation of ChatGPT. The improvements were substantial\u2014GPT-4 was more accurate, could handle more nuanced instructions, had expanded knowledge, and demonstrated improved reasoning abilities.</p> <p>Perhaps most significantly, GPT-4 introduced multimodal capabilities, allowing it to accept images as inputs alongside text. This opened up new possibilities\u2014users could upload charts for analysis, documents for summarization, or photos for identification and explanation.</p> <p>\"With GPT-4, we were getting closer to an AI assistant that could truly complement human capabilities across a wide range of tasks,\" explained one researcher. \"It still had limitations, but the gap between AI and human performance on many cognitive tasks was narrowing faster than many had predicted.\"</p> <p>The advancement from ChatGPT to GPT-4 in just a few months demonstrated the rapid pace of progress in AI capabilities\u2014and hinted at what might be possible in the coming years.</p>"},{"location":"stories/chatgpt/#chapter-8-a-new-computing-paradigm","title":"Chapter 8: A New Computing Paradigm","text":"<p> Show Image Prompt     Please generate another drawing. 8: A colorful, bright wide-landscape drawing depicting the integration of AI assistants into everyday life and work. Show people from diverse backgrounds and professions collaborating with AI across devices and contexts: architects reviewing designs with AI input, doctors discussing diagnoses, students learning concepts, programmers coding with assistance, and everyday people having conversations with AI assistants. The image should convey a sense of harmonious partnership between humans and AI. The style should be forward-looking but grounded, showing realistic applications rather than far-future fantasy.  </p> <p>By mid-2023, it was clear that ChatGPT wasn't just another tech product\u2014it represented the beginning of a new computing paradigm. The conversation-based interface made advanced AI capabilities accessible to anyone who could express their needs in natural language, without requiring specialized knowledge or training.</p> <p>The impact was particularly profound for software development. GPT-4 could generate functional code from natural language descriptions, debug existing code, explain programming concepts, and suggest improvements\u2014democratizing programming abilities that previously required years of study.</p> <p>In education, despite initial concerns about cheating, many educators began incorporating AI assistants into their teaching methods, recognizing that the ability to effectively collaborate with AI would be an essential skill for their students' futures.</p> <p>\"We're entering an era where the boundary between human and machine intelligence is becoming more fluid,\" observed one AI researcher. \"It's not about AI replacing humans, but about finding the right ways for humans and AI to collaborate\u2014combining the creativity, judgment, and ethical reasoning of humans with the pattern recognition, memory, and processing capabilities of AI.\"</p>"},{"location":"stories/chatgpt/#chapter-9-the-road-ahead","title":"Chapter 9: The Road Ahead","text":"<p> Show Image Prompt Please generate another drawing. 9: A colorful, bright wide-landscape drawing showing the future horizon of AI development. The foreground should show current applications of ChatGPT and similar technologies. The middle ground shows researchers working on next-generation capabilities and addressing current limitations. The background depicts a bright horizon with silhouettes of possibilities: AI systems that can reason over long periods, incorporate diverse modalities, and operate with greater autonomy while maintaining alignment with human values. The art style should be inspirational and forward-looking, using light and perspective to draw the eye toward future possibilities.  </p> <p>The story of ChatGPT is far from over. As OpenAI and other organizations continue to develop more capable AI systems, both the opportunities and challenges will grow.</p> <p>Technical challenges remain\u2014improving reasoning abilities, reducing hallucinations, enabling AI systems to better understand human values and nuances, and finding ways to ground models in verified information rather than generating plausible-sounding falsehoods.</p> <p>Broader societal questions are equally important\u2014how to ensure these powerful technologies benefit humanity broadly rather than concentrating power, how to adapt education and work for an AI-enabled world, and how to develop appropriate governance frameworks for increasingly capable AI systems.</p> <p>\"ChatGPT showed us that AI capabilities can progress more rapidly than expected, and can have impacts that ripple throughout society,\" reflected one OpenAI researcher. \"That makes our mission\u2014ensuring that artificial general intelligence benefits all of humanity\u2014more important than ever.\"</p>"},{"location":"stories/chatgpt/#epilogue-the-conversation-revolution","title":"Epilogue: The Conversation Revolution","text":"<p> Show Image Prompt      Please generate another drawing. 10: A colorful, bright wide-landscape drawing showing a diverse group of people engaged in various conversations with AI assistants across different devices and settings. The image should show both current applications and hint at future possibilities. Include a subtle visual element showing the progression from early computers and command-line interfaces to the natural language paradigm of ChatGPT, emphasizing how this represents a fundamental shift in human-computer interaction. The style should be reflective yet optimistic, with warm lighting suggesting a new dawn in computing history.  </p> <p>In less than a year, ChatGPT transformed from a research project to a cultural phenomenon that changed how millions of people think about artificial intelligence. It made the abstract concept of AI tangible and personal\u2014something people could interact with directly, form opinions about, and incorporate into their daily lives.</p> <p>Perhaps most significantly, it opened a global conversation about AI's role in society. No longer the realm of specialists, debates about AI capabilities, risks, benefits, and governance became mainstream topics. Millions of people who had never thought much about AI before were now experiencing it directly and forming their own views.</p> <p>As one OpenAI team member put it: \"The launch of ChatGPT didn't just change AI\u2014it changed the conversation about AI. And that conversation, involving diverse perspectives from around the world, is essential as we navigate the development of increasingly capable AI systems.\"</p> <p>The story of ChatGPT reminds us that technology doesn't develop in isolation\u2014it's shaped by human choices, values, and feedback. As AI capabilities continue to advance, the conversation between humans and AI\u2014and among humans about AI\u2014will remain essential to ensuring these powerful technologies serve humanity's best interests.</p>"},{"location":"stories/chatgpt/#references","title":"References","text":"<ul> <li>The ChatGPT story: How a curious AI assistant took over the world - February 8, 2023 - CNET</li> <li>The inside story of how ChatGPT was built from the people who made it - March 3, 2023 - MIT Technology Review</li> <li>How ChatGPT was created: A primer on what makes the AI chatbot so valuable - January 31, 2023 - ZDNET</li> <li>Sam Altman: OpenAI, GPT-4, ChatGPT, and the Future of AI - March 15, 2023 - Lex Fridman Podcast</li> <li>Training language models to follow instructions with human feedback - March 4, 2022 - arXiv (OpenAI Research)</li> <li>InstructGPT: Training language models to follow instructions - January 27, 2022 - OpenAI Research Blog</li> <li>ChatGPT: Optimizing Language Models for Dialogue - November 30, 2022 - OpenAI Blog</li> <li>How OpenAI Built ChatGPT So Quickly - April 7, 2023 - Forbes</li> <li>Introducing ChatGPT - November 30, 2022 - OpenAI Blog</li> <li>The Story Behind ChatGPT's Meteoric Rise: How OpenAI's Chatbot Became an Overnight Sensation - January 18, 2023 - TIME</li> <li>The inside story of how ChatGPT was built from the people who made it Exclusive conversations that take us behind the scenes of a cultural phenomenon. By Will Douglas Heaven - March 3, 2023 - MIT Review</li> </ul>"},{"location":"stories/deepseek-r1/","title":"DeepSeek-R1: The Efficient Revolution","text":"Show Narrative Prompt Please create a detailed, fun and entertaining story about the creation of the large language model called \"DeepSeek-R1\" and its relevance to the rise of open, small and low-cost LLMs.   Describe the challenges and costs the team had building DeepSeek-R1 and how they cleverly use Reinforcement Learning, reasoning, and mixture-of-experts to create a high-quality language-model at a lower cost than other organizations.  Describe how with the lack of GPUs the team had to use resources carefully to get good results.  Describe the challenges the team faced and how they overcame these challenges.   Discuss the tradeoffs that the company faced and how by using good software engineering they avoided having to use the \"Brute Force\" approach with massive GPU data centers.  Discuss how the DeepSeek architecture allows even smaller commercial grade gaming GPUs to achieve excellent inference performance due to the innovations in GPUs.  Describe how an entire intelligent-textbook for a college-level AI course can now be generated on a local GPU using DeepSeek-R1.  Our goal is to have you generate the full text of the story, but to turn the story into a graphic novel with many illustrations that explain how the Deepseek R1 product was created.   When appropriate, suggest an image that could be inserted into the story to make the story a graphic novel. Describe each image in detail and be consistent across all the images in the story for style.  When you describe an image, make sure to mention that it should be a colorful, bright wide-landscape drawing suitable for technology-forward optimistic graphic-novel."},{"location":"stories/deepseek-r1/#chapter-1-a-vision-against-the-tide","title":"Chapter 1: A Vision Against the Tide","text":"<p>In the sprawling landscape of AI development, where the biggest players raced to build ever-larger language models with seemingly unlimited resources, a small team at DeepSeek saw a different path forward. While giants like OpenAI and Anthropic were building models requiring thousands of GPUs and hundreds of millions in funding, the DeepSeek team believed they could create something just as powerful, but far more efficient.</p> <p></p> Show Image Prompt 1: \"The Vision\" - A colorful, bright wide-landscape drawing showing the DeepSeek team gathered around a small cluster of computers in a modest office, while in the background through floor-to-ceiling windows, massive data centers from larger companies loom on the horizon. The contrast between the small, focused team and the industrial-scale operations behind them creates visual tension. The team members are shown in determined poses, examining holographic projections of neural network architectures that appear more elegant and streamlined than the bulky representations of competitor models floating in the background.  <p>\"We don't need to build the biggest model,\" said Lin Xiao, DeepSeek's lead architect. \"We need to build the smartest model. One that can do more with less.\"</p> <p>The team's vision was radical in its simplicity: create a high-quality language model that could run efficiently on consumer hardware. Not just for inference, but potentially even for fine-tuning. A model that would democratize AI by making it accessible to researchers, developers, and enthusiasts without enterprise-grade infrastructure.</p>"},{"location":"stories/deepseek-r1/#chapter-2-the-resource-challenge","title":"Chapter 2: The Resource Challenge","text":"<p>The challenge was daunting. The team had a fraction of the compute resources available to larger labs. While companies like Google and Microsoft had tens of thousands of GPUs at their disposal, DeepSeek had to make every bit of compute count.</p> <p></p> Show Image Prompt 2: \"Resource Management\" - A colorful, bright wide-landscape drawing depicting the DeepSeek team in a war-room style setting. Large screens show resource allocation graphs and optimization metrics. Team members are strategically moving virtual computing resources represented as glowing blocks between different stages of the model development pipeline. Some blocks are being carefully examined, optimized, and then placed back into the workflow. The scene conveys the precision and thoughtfulness required when working with limited resources, with a prominent digital counter showing \"GPU Hours Remaining\" to emphasize the constraints they faced.  <p>\"When you can't win with brute force, you have to be clever,\" explained Zhang Wei, DeepSeek's training optimization lead. \"Every hour of compute had to yield maximum benefit. We couldn't afford to run experiments just to see what happens.\"</p> <p>The team implemented a rigorous experimental design process. Before any code was run on GPUs, it was extensively simulated and tested. Ideas were thoroughly vetted through theoretical analysis and small-scale experiments before graduating to larger runs.</p> <p>While other companies could afford to train dozens of variants of their models to find the best performer, DeepSeek needed to get things right the first time.</p>"},{"location":"stories/deepseek-r1/#chapter-3-architecture-innovation-the-moe-breakthrough","title":"Chapter 3: Architecture Innovation - The MoE Breakthrough","text":"<p>The breakthrough came through an innovative application of the Mixture of Experts (MoE) architecture. Rather than making the entire model larger, they created a model where different \"expert\" neural networks specialized in different types of knowledge and reasoning.</p> <p></p> Show Image Prompt 3: \"The MoE Architecture\" - A colorful, bright wide-landscape drawing showing a visualization of the DeepSeek-R1 architecture. The image depicts neural pathways flowing through a central router that dynamically directs queries to specialized expert modules represented as distinct colored nodes. Each expert is shown handling different types of information - one processing mathematical equations, another analyzing images, another working with code, etc. The architecture is depicted as an elegant, efficient machine with minimal wasted connections and maximum information flow. Small human figures are shown \"training\" each expert, emphasizing the human guidance in developing these specialized modules.  <p>\"Traditional models activate all their parameters for every token they process,\" explained Dr. Mei Chen, DeepSeek's chief scientist. \"It's incredibly wasteful. Our MoE architecture only activates the parts of the model that are relevant to the current task.\"</p> <p>This approach allowed DeepSeek-R1 to have the effective capacity of a much larger model while requiring significantly less compute resources. The router component of the architecture, which determined which experts to activate, became a critical focus of the team's research.</p> <p>\"The router is like an intelligent traffic controller,\" said Chen. \"It needs to understand the query deeply enough to direct it to the right experts, but it can't be so complex that it negates our efficiency gains.\"</p> <p>After months of experimentation, the team developed a multi-headed routing mechanism that could effectively distribute work across specialized expert neural networks, allowing for both broad knowledge and deep reasoning with remarkable efficiency.</p>"},{"location":"stories/deepseek-r1/#chapter-4-the-training-strategy-quality-over-quantity","title":"Chapter 4: The Training Strategy: Quality Over Quantity","text":"<p>While competitors were scaling up their datasets to trillions of tokens, the DeepSeek team took a different approach: they focused on data quality and training methodology rather than sheer volume.</p> <p></p> Show Image Prompt 4: \"Curated Learning\" - A colorful, bright wide-landscape drawing showing the data curation process. The scene depicts team members carefully selecting and processing training data, visualized as glowing documents passing through various quality filters. Some researchers are shown enhancing particularly valuable datasets, which glow brighter as they receive special attention. In contrast to the \"data firehose\" approach shown in the background (representing competitors), the DeepSeek method is shown as a precise, surgical process. The image includes visual metrics showing how their curated smaller dataset produces better results than massive unfiltered data collections.  <p>\"Anyone can download the entire internet and feed it to a model,\" said Li Jun, DeepSeek's data scientist. \"But that's like trying to educate someone by having them memorize the library. We wanted to build a model that truly understands rather than just memorizes.\"</p> <p>The team developed sophisticated data filtering and augmentation techniques. They created synthetic datasets specifically designed to teach reasoning capabilities and identified the most informative examples for key concepts.</p> <p>Their reinforcement learning pipeline was equally innovative. Rather than using a massive dataset of human feedback, they developed a multi-stage process where the model's own outputs were used to generate training signals.</p> <p>\"We created a curriculum that steadily increased in difficulty,\" explained Zhao Feng, reinforcement learning specialist. \"First, the model learned basic competencies. Then, it learned to evaluate its own outputs. Finally, it learned to improve its responses through self-critique.\"</p> <p>This approach allowed DeepSeek-R1 to develop strong reasoning capabilities without the expensive human feedback pipelines that other labs relied on.</p>"},{"location":"stories/deepseek-r1/#chapter-5-software-engineering-excellence","title":"Chapter 5: Software Engineering Excellence","text":"<p>Where larger competitors could throw hardware at problems, the DeepSeek team relied on superior software engineering to maximize efficiency.</p> <p></p> Show Image Prompt 5: \"Engineering Excellence\" - A colorful, bright wide-landscape drawing showing the software optimization process. The scene depicts engineers working with visualized code structures that transform from bulky, inefficient forms into elegant, streamlined versions. Some team members are shown using specialized tools to identify bottlenecks, represented as constriction points in flowing data streams. Others are implementing kernel optimizations that visibly accelerate computations shown as light particles moving through the system. The workspace combines physical screens with AR projections, creating a dynamic environment where engineers can literally see the performance improvements they're making. Resource usage meters prominently display before/after metrics, highlighting dramatic efficiency gains.  <p>\"We wrote custom CUDA kernels for the most compute-intensive operations,\" said Wang Tao, systems optimization lead. \"Where standard implementations were wasting 30-40% of GPU cycles, our optimized code utilized over 90% of the available compute.\"</p> <p>The team spent months profiling their code, identifying bottlenecks, and creating specialized implementations for critical operations. They developed a dynamic memory management system that could adapt to different hardware configurations, making the model surprisingly flexible across a range of GPU setups.</p> <p>\"Most models are optimized for specific hardware configurations,\" Wang explained. \"We designed ours to gracefully scale up or down depending on available resources.\"</p> <p>This approach not only made training more efficient but would later prove crucial for the model's ability to run on consumer hardware.</p>"},{"location":"stories/deepseek-r1/#chapter-6-the-training-run","title":"Chapter 6: The Training Run","text":"<p>After months of preparation, the team was ready for the main training run. With their limited GPU budget, they knew they had one real shot to get it right.</p> <p></p> Show Image Prompt 6: \"The Critical Run\" - A colorful, bright wide-landscape drawing showing the team during the crucial training phase. The scene is set in a darkened room illuminated by screens showing training metrics and progress bars. Team members are shown monitoring various aspects of the process - some tracking loss curves, others watching for hardware issues, others analyzing emerging model behaviors. The tension is palpable, with team members' expressions showing both anxiety and hope. Time indicators show this has been running for days, and coffee cups litter the workspace. A visualization shows DeepSeek-R1 gradually taking form as a glowing neural structure that becomes more defined as the training progresses.  <p>\"Those weeks were intense,\" recalled Lin. \"We took shifts monitoring the training 24/7. If something went wrong, we needed to catch it immediately.\"</p> <p>The team had implemented extensive monitoring and checkpoint systems. Every few hours, the model was automatically evaluated on a battery of tests to ensure it was developing as expected.</p> <p>\"There was one heart-stopping moment when we saw performance plateauing earlier than expected,\" said Zhang. \"We had to make a real-time decision to adjust the learning rate schedule. It was terrifying - if we made the wrong call, we'd waste our entire compute budget.\"</p> <p>The adjustment worked. After the brief plateau, performance began improving again at an even faster rate than before.</p> <p>As training progressed, the team was amazed to see DeepSeek-R1 developing capabilities they hadn't explicitly trained for - emerging abilities that suggested the model was generalizing in ways they hadn't anticipated.</p>"},{"location":"stories/deepseek-r1/#chapter-7-reasoning-capabilities","title":"Chapter 7: Reasoning Capabilities","text":"<p>What truly distinguished DeepSeek-R1 was its reasoning capabilities. While many models could generate fluent text, DeepSeek-R1 showed an unusual aptitude for logical thinking and problem-solving.</p> <p></p> Show Image Prompt 7: \"The Reasoning Layer\" - A colorful, bright wide-landscape drawing visualizing DeepSeek-R1's reasoning process. The image shows multiple reasoning paths being explored simultaneously, represented as branching light trails. Some paths dead-end, while others connect to form coherent conclusions. The visualization includes symbolic representations of the model breaking problems into sub-problems, considering alternatives, and synthesizing information across domains. On screens surrounding this visualization, examples show the model working through complex reasoning tasks - solving multi-step math problems, analyzing logical arguments, and generating step-by-step explanations. The image conveys the sense that this isn't just pattern matching but genuine problem-solving.  <p>\"We specifically designed training tasks that required multi-step reasoning,\" explained Dr. Chen. \"We wanted the model to learn how to think, not just predict the next word.\"</p> <p>The team had developed a technique they called \"thought tracing\" - requiring the model to explicitly articulate its reasoning process for complex problems. This approach not only improved the model's performance but made its decision-making more transparent and debuggable.</p> <p>\"Most models are black boxes,\" said Chen. \"We wanted ours to show its work, both for trustworthiness and for continued improvement.\"</p> <p>The result was a model that could tackle complex logical, mathematical, and analytical challenges with a clarity that surprised even its creators. DeepSeek-R1 didn't just generate impressive outputs - it could explain exactly how it arrived at them.</p>"},{"location":"stories/deepseek-r1/#chapter-8-the-inference-breakthrough","title":"Chapter 8: The Inference Breakthrough","text":"<p>While training required significant resources, the team's architectural innovations truly shone during inference. DeepSeek-R1 could run on hardware that would normally be considered far too limited for advanced AI models.</p> <p></p> Show Image Prompt 8: \"Gaming GPUs Unleashed\" - A colorful, bright wide-landscape drawing showing DeepSeek-R1 running on consumer-grade hardware. The scene depicts enthusiasts and developers using the model on gaming PCs with standard GPUs rather than data center equipment. Visual indicators show memory usage and processing speeds that defy expectations for consumer hardware. One part of the image shows a side-by-side comparison where DeepSeek-R1 runs smoothly on a gaming setup while competitor models show \"insufficient resources\" errors. Another section shows the model's internal architecture dynamically adjusting to the available hardware, with only necessary components being activated based on the current query. The image combines technical accuracy with the excitement of making cutting-edge AI accessible to everyday users.  <p>\"We tested it on a three-year-old gaming GPU, and it ran beautifully,\" said Wang proudly. \"The sparse activation pattern of our MoE architecture means we don't need to load the entire model into memory at once.\"</p> <p>This capability was revolutionary. While other models required expensive cloud services or specialized hardware to run, DeepSeek-R1 could operate on hardware many developers and researchers already owned.</p> <p>\"The biggest innovation was our dynamic loading system,\" explained Wang. \"The model loads only the parameters it needs for a given query, drastically reducing memory requirements.\"</p> <p>This approach, combined with their highly optimized inference engine, meant that DeepSeek-R1 could run with a fraction of the resources required by models of similar capability.</p>"},{"location":"stories/deepseek-r1/#chapter-9-the-educational-ai-revolution","title":"Chapter 9: The Educational AI Revolution","text":"<p>One of the most exciting applications that emerged was the ability to generate entire educational resources on consumer hardware. Professors and educational content creators could now generate comprehensive, accurate learning materials without enterprise-grade infrastructure.</p> <p></p> Show Image Prompt 9: \"The AI Educator\" - A colorful, bright wide-landscape drawing showing DeepSeek-R1 being used to create educational content. The scene depicts a professor working with the model to generate an interactive AI textbook. Multiple screens show different chapters being developed simultaneously - complex diagrams explaining neural networks, interactive coding exercises, mathematical derivations with step-by-step explanations, and conceptual illustrations. Students are shown engaging with the generated material on various devices. The image illustrates how the model doesn't just reproduce text but creates truly educational content with exercises, explanations, visual aids, and knowledge checks. The professor looks pleasantly surprised at both the quality and the speed of content generation.  <p>\"A professor at MIT generated an entire graduate-level AI textbook in a weekend,\" said Lin. \"Complete with exercises, diagrams, and interactive components. What would have taken months or years of writing was done in days.\"</p> <p>The educational applications extended beyond text generation. DeepSeek-R1 could create interactive tutorials, generate practice problems with detailed solutions, and even adapt content to different learning styles and levels of expertise.</p> <p>\"The model understands educational concepts at a deep level,\" explained Dr. Chen. \"It doesn't just regurgitate information; it structures it in ways that facilitate learning.\"</p> <p>This capability was democratizing advanced education. Small institutions without massive resources could now develop cutting-edge curricula, and individual learners could generate personalized learning materials tailored to their specific needs.</p>"},{"location":"stories/deepseek-r1/#chapter-10-open-and-accessible-ai","title":"Chapter 10: Open and Accessible AI","text":"<p>The team's final revolutionary decision was to make DeepSeek-R1 accessible to the broader community, releasing both commercial and research versions that could run on modest hardware.</p> <p></p> Show Image Prompt 10: \"The Accessible Future\" - A colorful, bright wide-landscape drawing showing the global impact of DeepSeek-R1's accessibility. The scene depicts a diverse array of users worldwide accessing the technology - students in resource-limited settings, independent researchers without institutional backing, small businesses creating specialized applications, and hobbyists experimenting with new use cases. The visualization includes a map showing DeepSeek-R1 deployments spreading across the globe, with particularly bright clusters in regions typically underrepresented in cutting-edge AI deployment. The image conveys how the model's efficiency has democratized access to advanced AI, creating opportunities for innovation beyond the traditional tech hubs. Team members are shown helping online communities understand and utilize the model, emphasizing the collaborative spirit behind the project.  <p>\"The big players are building walled gardens,\" said Lin. \"We wanted to build a commons where innovation can flourish without gatekeepers.\"</p> <p>This approach sparked a wave of innovation as developers who had previously been priced out of advanced AI development began building with DeepSeek-R1. New applications emerged across industries, often led by people with domain expertise rather than AI specialization.</p> <p>\"We're seeing applications we never imagined,\" said Zhao. \"A doctor in rural India developed a medical education system. A language preservation group in Mexico is using it to document endangered languages. Students are using it to learn complex subjects on their own.\"</p> <p>The democratization effect rippled through the AI landscape. Other companies began prioritizing efficiency alongside raw performance, and the barriers to entry for AI development steadily decreased.</p>"},{"location":"stories/deepseek-r1/#epilogue-the-efficient-future","title":"Epilogue: The Efficient Future","text":"<p>As DeepSeek-R1 gained adoption, it challenged the prevailing wisdom that bigger always meant better in AI. The team had proven that thoughtful architecture, careful data curation, and engineering excellence could create capabilities comparable to models requiring far more resources.</p> <p></p> Show Image Prompt 11 Epilogue: \"The New Paradigm\" - A colorful, bright wide-landscape drawing showing the future inspired by DeepSeek-R1's approach. The scene depicts a transformed AI landscape where efficiency is valued alongside raw power. In the foreground, the DeepSeek team is shown collaborating with other researchers, sharing insights that have influenced the broader field. The image visualizes how the principles behind DeepSeek-R1 have spread - showing new generations of AI systems with elegant, efficient architectures rather than brute-force approaches. Data centers are shown transforming to use fewer resources while achieving greater results. Educational institutions that previously couldn't afford AI research now have thriving programs. The illustration combines technical elements with human stories, showing how the democratization of AI technology has enabled new voices and perspectives to contribute to the field's advancement.  <p>\"We didn't just build a more efficient model,\" reflected Dr. Chen. \"We helped create a more efficient approach to AI development.\"</p> <p>The impact extended beyond the technical. By making advanced AI accessible to those without massive resources, DeepSeek-R1 had diversified who could participate in the AI revolution. New voices, perspectives, and applications emerged from communities previously excluded by resource constraints.</p> <p>\"The future of AI isn't just about pushing parameters to the limit,\" said Lin. \"It's about making every parameter count.\"</p> <p>As the team looked to the future, they saw a world where AI development wasn't limited to those with massive data centers and venture funding. They envisioned a more inclusive ecosystem where innovation could come from anywhere, and where efficiency was recognized as not just economically valuable but ethically essential.</p> <p>DeepSeek-R1 had proven that the path forward wasn't always about being bigger - sometimes, it was about being smarter.</p>"},{"location":"stories/deepseek-r1/#references","title":"References","text":"<ul> <li>DeepSeek-R1 Technical Report - January 2025, arXiv The paper describes DeepSeek-R1 as their \"first-generation reasoning models\" trained via large-scale reinforcement learning, achieving \"performance comparable to OpenAI-o1-1217 on reasoning tasks.\" ArXiv</li> <li>DeepSeek API Documentation - January 20, 2025, DeepSeek The official announcement highlights DeepSeek-R1 as having \"\u26a1 Performance on par with OpenAI-o1\" and being a \"\ud83d\udcd6 Fully open-source model &amp; technical report.\" Deepseek</li> <li>DeepSeek-R1 GitHub Repository - January 2025, GitHub The repository states they \"directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step,\" enabling the model to explore chain-of-thought reasoning. GitHub</li> <li>DeepSeek Explained: Everything You Need to Know - January 2025, TechTarget According to this article, \"DeepSeek released its R1 LLM at a fraction of the cost that other vendors incurred in their own developments\" and provided it \"under an open source license, enabling free use.\" WhatIs</li> <li>DeepSeek-R1 Paper Explained - January 25, 2025, AI Papers Academy The article notes that DeepSeek-R1 \"introduced open-source reasoning models that rivals the performance OpenAI's o1\" and uses a simplified reward mechanism that \"reduces the cost of the training process.\" AI Papers Academy</li> <li>DeepSeek Open-Sources DeepSeek-R1 LLM - February 4, 2025, InfoQ InfoQ reports that DeepSeek-R1 \"outperformed all of them [Claude-3.5-Sonnet, GPT-4o, and o1] on several of the benchmarks, including AIME 2024 and MATH-500.\" InfoQ</li> <li>DeepSeek LLM GitHub Repository - 2023, GitHub This repository for an earlier DeepSeek model notes that \"For DeepSeek LLM 7B, we utilize 1 NVIDIA A100-PCIE-40GB GPU for inference\" while the 67B model required 8 GPUs, showing their focus on resource requirements. GitHub</li> <li>Performance Evaluation on LMArena - February 2025, InfoQ via LMArena Within days of release, \"LMArena announced that DeepSeek-R1 was ranked #3 overall in the arena and #1 in coding and math\" and was \"tied for #1 with o1 in 'Hard Prompt with Style Control' category.\" InfoQ</li> <li>DeepSeek-R1 Distilled Models - January 2025, GitHub The team created \"six dense models (1.5B, 7B, 8B, 14B, 32B, and 70B) distilled from DeepSeek-R1 based on Llama and Qwen\" to support the research community with smaller, more accessible models. GitHub</li> <li>DeepSeek-R1 Reinforcement Learning Approach - January 2025, arXiv The researchers' approach focused on \"improving language model reasoning capabilities using pure reinforcement learning (RL)\" to explore \"the potential of LLMs to develop reasoning capabilities without any supervised data.\" ArXiv</li> </ul>"},{"location":"stories/hugging-face/","title":"The HuggingFace Revolution: How an AI Underdog Changed the Game","text":"Show Narrative Prompt Please create a detailed, fun and entertaining story about the formation of the company HuggingFace that would form the narrative text for a graph novel.  Focus on how the founders from France started by trying one plan but pivoted to another plan. Describe the challenges the HuggingFace team faced and how they overcame these challenges.   Describe the challenges and costs the team had building HuggingFace and how they cleverly use social networks to build a critical mass of models and the data used to create these models.  Show how they mastered the art of \"Crowdsourcing\" to get more people to upload their models and data to their site.  Compare the approach that HuggingFace used to the approaches that Microsoft, Google and Amazon used to store models, data and tools.  Mention that it is now just building a great faceted search tool to help people find models, but it is also the tools to use these models such as the Gradio and Streamlit programming libraries built around Python.  Be clear to mention the creation of HuggingFace Spaces to demonstrate inference of models.  Also focus on how HuggingFace has build world-class documentation and training on how to teach the use of integrating machine learning into classrooms.  Wrap up with a discussion that although HuggingFace is still a small company, the company valuation per employee is very high.  End on a positive note that understanding crowdsourcing is just one of their first innovations and there may be many more to come in the future.  Our goal is to have you generate the full text of the story, but to turn the story into a graphic novel with many illustrations that explain how the company HuggingFace was created.   When appropriate, suggest an image that could be inserted into the story to make the story a graphic novel. Describe each image in detail and be consistent across all the images in the story for style.  When you describe an image, make sure to mention that it should be a colorful, bright wide-landscape drawing suitable for technology-forward optimistic graphic-novel."},{"location":"stories/hugging-face/#chapter-1-a-parisian-beginning","title":"Chapter 1: A Parisian Beginning","text":"<p>In 2016, three visionaries from Paris - Cl\u00e9ment Delangue, Julien Chaumond, and Thomas Wolf - came together in New York with a shared dream that would eventually reshape the AI landscape. But like many great stories, theirs didn't begin with the triumph we know today.</p> <p></p> Show Image Prompt   Please generate a new drawing.  1: A colorful, bright wide-landscape drawing showing three young entrepreneurs meeting in a Parisian caf\u00e9, with the Eiffel Tower visible through the window. Their table is covered with sketches and notes, coffee cups scattered about. The style should be optimistic and tech-forward, with subtle digital elements blending into the traditional caf\u00e9 scene - perhaps code or AI diagrams floating subtly in the steam from their coffee cups. The three founders should be depicted with distinct personalities but united in their enthusiastic discussion.  <p>\"We didn't start out planning to build the GitHub of machine learning,\" Cl\u00e9ment would later recall. \"Our first idea was actually building a consumer AI application - a chatbot assistant.\"</p> <p>The year was 2016, and the three friends were convinced that conversational AI was the future. Their initial vision: create an engaging, helpful AI assistant that would make technology more human-like and accessible.</p>"},{"location":"stories/hugging-face/#chapter-2-the-pivot","title":"Chapter 2: The Pivot","text":"<p>Their chatbot showed promise, attracting initial users who enjoyed the natural language interactions. But as they refined their product, the team encountered a fundamental challenge: the tools they needed to build truly impressive natural language processing systems simply weren't accessible enough.</p> <p></p> Show Image Prompt 2: A colorful, bright wide-landscape drawing depicting the moment of pivot. The left side shows the team looking frustrated at screens displaying their chatbot with error messages and limitations. The right side shows a lightbulb moment with the same team excitedly sketching out a new platform architecture on a whiteboard. The transition between these scenes should use a dynamic, swirling design element to represent the transformation of their vision. The color palette shifts from muted tones on the left to vibrant, hopeful colors on the right.  <p>\"We realized we were spending more time building infrastructure than actually improving the user experience,\" Julien explained. \"And we weren't alone. Every AI team was reinventing the wheel.\"</p> <p>This recognition led to their pivotal moment in 2018 - instead of being just another chatbot company, they would build something far more fundamental: an open platform that democratized access to cutting-edge NLP models.</p> <p>\"What if,\" Thomas proposed during a late-night brainstorming session, \"instead of keeping our models proprietary, we made them available to everyone? What if we created a place where researchers and developers could share their work too?\"</p> <p>And so, HuggingFace pivoted from being a consumer app company to a platform focused on making state-of-the-art NLP accessible to all.</p>"},{"location":"stories/hugging-face/#chapter-3-the-underdog-challenge","title":"Chapter 3: The Underdog Challenge","text":"<p>The road ahead wasn't easy. As a small team with limited resources going up against tech giants, HuggingFace faced daunting challenges.</p> <p></p> Show Image Prompt 3: A colorful, bright wide-landscape drawing showing the small HuggingFace team in their modest office, surrounded by towering skyscrapers labeled with tech giant logos (Google, Microsoft, Amazon). The HuggingFace office glows with a warm light while the towers appear cold and monolithic. The team is depicted working intensely on computers, with determination on their faces. Digital elements should show code flowing from their small space up toward the clouds, representing how their small-scale efforts began to have outsized impact.  <p>\"The big companies had hundreds of PhDs and seemingly unlimited compute resources,\" remembered Cl\u00e9ment. \"We had a handful of people and were watching every euro we spent on cloud computing.\"</p> <p>Microsoft, Google, and Amazon were investing billions in AI research and infrastructure, building walled gardens where their best models remained proprietary and access was tightly controlled. Their approach: create dependency on their specific cloud platforms and tools.</p> <p>Meanwhile, the HuggingFace team was scraping together funding to keep their servers running. But what they lacked in resources, they made up for with a powerful vision: AI should be open, collaborative, and accessible.</p>"},{"location":"stories/hugging-face/#chapter-4-the-transformers-revolution","title":"Chapter 4: The Transformers Revolution","text":"<p>Their breakthrough came with the release of the Transformers library - a Python package that made implementing state-of-the-art NLP models surprisingly simple.</p> Show Image Prompt 4: A colorful, bright wide-landscape drawing portraying the Transformers library as a magical toolbox being opened, with various NLP models (BERT, GPT, T5, etc.) emerging as distinct characters or entities with their own personalities. Developers of various backgrounds are shown being empowered by these tools, creating applications instantly where previously complex machinery might have been required. The scene should have a sense of wonder and democratization, with code snippets visibly simplified and accessible.  <p>\"We wanted to make it so easy that a developer could implement BERT or GPT with just a few lines of code,\" said Julien. \"No PhD required.\"</p> <p>The timing was perfect. Just as the NLP field was experiencing a renaissance with breakthrough models like BERT and GPT, HuggingFace provided the tooling that made these innovations accessible to everyday developers.</p> <p>The community response was immediate and enthusiastic. Suddenly, capabilities that had been limited to specialized AI labs were available to startups, individual developers, and researchers worldwide.</p>"},{"location":"stories/hugging-face/#chapter-5-the-crowdsourcing-masterstroke","title":"Chapter 5: The Crowdsourcing Masterstroke","text":"<p>While the Transformers library put HuggingFace on the map, their true innovation was yet to come: the Model Hub, a platform where anyone could share and discover machine learning models.</p> <p></p> Show Image Prompt 5: A colorful, bright wide-landscape drawing depicting the HuggingFace Model Hub as a bustling digital marketplace or library. Show researchers and developers from around the world (represented by diverse characters) both contributing models (uploading glowing digital packages) and discovering/downloading others' work. The scene should have a sense of abundance and community, with counters showing rapidly increasing numbers of models and downloads. Social network elements should be visible \u2013 people collaborating, rating models, and building on each other's work.  <p>\"We looked at how GitHub transformed software development through collaboration and version control,\" explained Cl\u00e9ment. \"We wanted to do the same for machine learning models.\"</p> <p>The masterstroke was how they leveraged network effects. Each model uploaded to the Hub attracted users, who in turn became potential contributors of new models or improvements to existing ones. The platform became more valuable with every contribution.</p> <p>Unlike the tech giants' approach of keeping their best models behind paywalls or API limitations, HuggingFace encouraged a culture of sharing that accelerated innovation across the entire field.</p> <p>\"Our strategy wasn't to build the best models ourselves,\" Thomas noted. \"It was to create the best place for the community to share their models.\"</p> <p>This approach proved remarkably effective at challenging the resources of much larger competitors. While Google and Microsoft spent millions developing individual models, HuggingFace was amassing thousands of models contributed by researchers and companies worldwide.</p>"},{"location":"stories/hugging-face/#chapter-6-datasets-and-the-virtuous-cycle","title":"Chapter 6: Datasets and the Virtuous Cycle","text":"<p>Recognizing that models are only as good as the data they're trained on, HuggingFace expanded their platform to include datasets, creating a complete ecosystem for machine learning.</p> <p></p> Show Image Prompt 6: A colorful, bright wide-landscape drawing illustrating the virtuous cycle between models and datasets. The image should be designed as a circular flow diagram, where datasets (represented as diverse data collections) feed into model training processes, which produce improved models, which then help create better datasets. Around this central cycle, show practitioners from various fields (medicine, finance, science, arts) both contributing to and benefiting from this ecosystem. The visualization should emphasize how this collaborative approach accelerates progress for everyone.  <p>This created a virtuous cycle: more high-quality public datasets led to better models, which attracted more developers, who in turn contributed more datasets and models.</p> <p>\"The traditional companies were trying to capture value by restricting access,\" said Cl\u00e9ment. \"We captured value by maximizing access and participation.\"</p> <p>This approach also addressed one of AI's biggest challenges: the centralization of power in the hands of a few large companies with the resources to collect massive proprietary datasets. By building a community-driven repository of open datasets, HuggingFace helped democratize access to the fuel that drives AI progress.</p>"},{"location":"stories/hugging-face/#chapter-7-building-for-usability","title":"Chapter 7: Building for Usability","text":"<p>The HuggingFace team recognized that powerful models and datasets wouldn't matter if people couldn't easily use them. This led to their focus on building exceptional tools for implementation and demonstration.</p> <p></p> Show Image Prompt 7: A colorful, bright wide-landscape drawing showing the HuggingFace tools ecosystem as an interconnected set of platforms. Central to this should be a visualization of Spaces (showing live demo environments), connected to Gradio and Streamlit interfaces (depicted as intuitive control panels). Show diverse users easily deploying complex AI models with simple interfaces, with minimal coding visible. The scene should convey how technical complexity is being hidden behind user-friendly tools, with arrows connecting the various components of the ecosystem.  <p>With the integration of Gradio and Streamlit, and the launch of HuggingFace Spaces, they made it possible for anyone to demonstrate their models with interactive web interfaces.</p> <p>\"Seeing is believing,\" Julien emphasized. \"We wanted people to be able to try models instantly, without downloading anything or writing a line of code.\"</p> <p>This approach stood in stark contrast to the complex integration processes typical of cloud AI offerings from the major platforms, which often required considerable technical expertise and platform-specific knowledge.</p> <p>Spaces became a game-changer, allowing model creators to showcase their work with interactive demos that anyone could try with a simple browser visit. This dramatically shortened the path from research to practical application.</p>"},{"location":"stories/hugging-face/#chapter-8-education-as-exponential-growth","title":"Chapter 8: Education as Exponential Growth","text":"<p>Perhaps HuggingFace's most strategic investment was in education and documentation. They recognized that knowledge barriers were as limiting as technical ones.</p> <p></p> Show Image Prompt 8: A colorful, bright wide-landscape drawing depicting HuggingFace's educational ecosystem. Show a multi-level learning environment with beginner-friendly tutorials at the base, advanced courses in the middle, and specialized workshop spaces at the top. Include teachers and students of diverse backgrounds engaging with the materials. Visual elements should include exceptional documentation (represented as illuminated manuscripts or magical books), course materials, and classroom integration scenarios. The scene should convey how knowledge is being transferred seamlessly across different expertise levels.  <p>\"Many companies treat documentation as an afterthought,\" Thomas observed. \"We saw it as central to our mission of democratization.\"</p> <p>The team invested heavily in creating world-class documentation, tutorials, courses, and learning resources. Their \"Hugging Face Course\" became a gold standard for practical NLP education, used in universities and by self-learners worldwide.</p> <p>They particularly focused on helping educators bring machine learning into classrooms, creating specialized resources for teachers and professors who wanted to incorporate AI into their curriculum.</p> <p>\"When you educate someone, you don't just gain a user \u2013 you empower an innovator who might create something you never imagined,\" said Cl\u00e9ment.</p> <p>This educational approach amplified their community-building efforts. Each new person they taught became a potential contributor to their ecosystem, creating a sustainable growth engine that the resource-intensive approaches of larger companies couldn't match.</p>"},{"location":"stories/hugging-face/#chapter-9-the-value-of-community","title":"Chapter 9: The Value of Community","text":"<p>By 2023, HuggingFace had achieved something remarkable: as a relatively small company of a few hundred employees, they had built a platform hosting over 150,000 models, 20,000 datasets, and serving more than 10 million monthly visitors. </p> Show Image Prompt 9: A colorful, bright wide-landscape drawing visualizing HuggingFace's impact relative to company size. On the left, show the modest-sized HuggingFace team. On the right, display their outsized impact through visualizations of exponential community growth - towers of models and datasets reaching skyward, global user maps with hotspots across all continents, and value metrics showing astronomical per-employee impact. Use size contrasts to emphasize how their lean approach generated massive outcomes. Include visual elements showing community members as co-creators rather than just users.  <p>\"Our valuation per employee is among the highest in tech,\" noted Cl\u00e9ment during a company milestone celebration. \"Not because we're focused on financials, but because our approach of empowering a community creates value far beyond what our headcount would suggest.\"</p> <p>While tech giants were hiring thousands of AI researchers to work behind closed doors, HuggingFace had orchestrated a global, collaborative research community that operated largely in the open.</p> <p>\"The breakthrough was realizing that in AI, the traditional competitive advantages \u2013 secrecy and resource concentration \u2013 could be countered by transparency and community,\" explained Julien.</p>"},{"location":"stories/hugging-face/#chapter-10-the-road-ahead","title":"Chapter 10: The Road Ahead","text":"<p>As HuggingFace continued to grow, they remained focused on their mission of democratizing artificial intelligence.</p> <p></p> Show Image Prompt 10: A colorful, bright wide-landscape drawing showing the future vision of HuggingFace. The image should depict the team standing at a crossroads with multiple exciting paths ahead. Each path represents different potential directions: multimodal AI (showing vision, audio, and text integration), ethical AI governance (represented by balanced scales and diverse oversight committees), decentralized infrastructure (shown as distributed computing nodes), and educational expansion (illustrated as knowledge spreading globally). The team should be looking forward with optimism, with light emanating from their collective vision. Community members should be visible on each path, emphasizing continued collaboration.  <p>\"Crowdsourcing models and datasets was just our first innovation,\" Thomas reflected. \"We're exploring new frontiers in multimodal AI, multilingual capabilities, and ethical AI governance.\"</p> <p>The company remained committed to its community-first approach, continuing to prioritize openness even as they built enterprise offerings to sustain their growth.</p> <p>\"There's a false dichotomy between doing good and building a successful business,\" Cl\u00e9ment argued. \"Our experience shows that by genuinely empowering a community, you can create both social and economic value.\"</p> <p>As larger competitors began adopting elements of HuggingFace's open approach, it became clear that this small company from Paris had fundamentally changed how the AI industry operates.</p> <p>Their story serves as a powerful reminder that in technology, the most transformative innovations often come not from having the most resources, but from reimagining how resources can be organized and shared.</p> <p>\"The future of AI isn't a winner-take-all competition,\" concluded Julien. \"It's a collaborative effort where progress comes from combining the best ideas from everywhere. That's the world we're helping to build.\"</p> Show chart code <pre><code>import React from 'react';\nimport { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from 'recharts';\n\nconst HuggingFaceModelsGrowth = () =&gt; {\n// Data based on search results and projections\n// For the early years, we have more estimates based on growth trends\n// Later years have more concrete numbers from the search results\nconst data = [\n    { date: '2018-01', models: 50 },\n    { date: '2018-07', models: 300 },\n    { date: '2019-01', models: 1000 },\n    { date: '2019-07', models: 2500 },\n    { date: '2020-01', models: 5000 },\n    { date: '2020-07', models: 10000 },\n    { date: '2021-01', models: 25000 },\n    { date: '2021-07', models: 50000 },\n    { date: '2022-01', models: 100000 },\n    { date: '2022-07', models: 200000 },\n    { date: '2023-01', models: 300000 },\n    { date: '2023-07', models: 400000 },\n    { date: '2024-01', models: 760460 }, // From search results about their extensive analysis\n    { date: '2024-07', models: 900000 }, // From recent docs showing over 900k models\n    { date: '2025-01', models: 1200000 },\n    { date: '2025-04', models: 1639648 } // Final point as requested\n];\n\nconst formatYAxis = (tickItem) =&gt; {\n    if (tickItem &gt;= 1000000) {\n    return `${(tickItem / 1000000).toFixed(1)}M`;\n    } else if (tickItem &gt;= 1000) {\n    return `${(tickItem / 1000).toFixed(0)}K`;\n    }\n    return tickItem;\n};\n\nconst formatTooltip = (value) =&gt; {\n    if (value &gt;= 1000000) {\n    return `${(value / 1000000).toFixed(2)} million`;\n    } else if (value &gt;= 1000) {\n    return `${(value / 1000).toFixed(1)}K`;\n    }\n    return value;\n};\n\nconst CustomTooltip = ({ active, payload, label }) =&gt; {\n    if (active &amp;&amp; payload &amp;&amp; payload.length) {\n    return (\n        &lt;div className=\"bg-white p-2 border border-gray-300 rounded shadow-lg\"&gt;\n        &lt;p className=\"font-semibold\"&gt;{label}&lt;/p&gt;\n        &lt;p className=\"text-blue-600\"&gt;\n            Models: {formatTooltip(payload[0].value)}\n        &lt;/p&gt;\n        &lt;/div&gt;\n    );\n    }\n    return null;\n};\n\nreturn (\n    &lt;div className=\"w-full h-full flex flex-col p-4\"&gt;\n    &lt;h2 className=\"text-2xl font-bold text-center mb-4\"&gt;Hugging Face Models Growth (2018-2025)&lt;/h2&gt;\n    &lt;div className=\"flex-1 w-full\"&gt;\n        &lt;ResponsiveContainer width=\"100%\" height={400}&gt;\n        &lt;LineChart\n            data={data}\n            margin={{\n            top: 20,\n            right: 30,\n            left: 20,\n            bottom: 50,\n            }}\n        &gt;\n            &lt;CartesianGrid strokeDasharray=\"3 3\" /&gt;\n            &lt;XAxis \n            dataKey=\"date\" \n            angle={-45} \n            textAnchor=\"end\" \n            height={60}\n            tick={{fontSize: 12}}\n            /&gt;\n            &lt;YAxis \n            tickFormatter={formatYAxis}\n            tick={{fontSize: 12}}\n            /&gt;\n            &lt;Tooltip content={&lt;CustomTooltip /&gt;} /&gt;\n            &lt;Legend verticalAlign=\"top\" height={36} /&gt;\n            &lt;Line\n            type=\"monotone\"\n            dataKey=\"models\"\n            name=\"Number of Models\"\n            stroke=\"#FF6D91\"\n            strokeWidth={3}\n            dot={{ r: 4 }}\n            activeDot={{ r: 8 }}\n            /&gt;\n        &lt;/LineChart&gt;\n        &lt;/ResponsiveContainer&gt;\n    &lt;/div&gt;\n    &lt;div className=\"text-sm text-gray-500 mt-4\"&gt;\n        &lt;p&gt;Data sources: Based on publicly available information from Hugging Face documentation and research papers.&lt;/p&gt;\n        &lt;p&gt;Note: Early data (2018-2022) is estimated based on growth trends. 2024-2025 includes projections to reach 1.64M models by April 2025.&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;/div&gt;\n);\n};\n\nexport default HuggingFaceModelsGrowth;\n</code></pre> <p>As of April 2026, Hugging Face stores over is now by far the largest repository of models, data and applications that demonstrate how these models are used.  Hugging Face continues to be one of the key hubs of models and applications for machine learning for the industry.  Many organizations now use Hugging Face software to build and extend their own internal model hubs.</p> <p></p> Show Image Prompt <p>And so, the HuggingFace journey continues \u2013 a testament to how a small team with a powerful vision can reshape an industry dominated by giants, not through outspending them, but by out-thinking them and building something bigger than themselves: a global community united by the shared goal of making AI accessible to all.</p>"},{"location":"stories/hugging-face/#references","title":"References","text":"<ul> <li>Wikipedia Page on Hugging Face</li> <li>TechCrunch: Hugging Face Wants to Become Your Artificial BFF - March 9, 2017 - TechCrunch</li> <li>Betaworks Camp: BotCamp Launch - 2016 - Betaworks</li> <li>TechCrunch: Hugging Face Raises $235M - August 29, 2023 - TechCrunch</li> <li>Wikipedia: Hugging Face Foundation - February 15, 2025 - Wikipedia</li> <li>Contrary Research: Hugging Face Business Breakdown &amp; Founding Story - 2023 - Contrary Research</li> <li>Sifted: How Hugging Face Got French Tech Talking - April 5, 2024 - Sifted</li> <li>TechCrunch: Hugging Face Raises $15 Million - December 17, 2019 - TechCrunch</li> <li>KITRUM: The Inspiring Journey of Cl\u00e9ment Delangue - February 2025 - KITRUM</li> <li>Fast Company: How Cl\u00e9ment Delangue is Open-Sourcing AI - July 12, 2023 - Fast Company</li> <li>Maginative: Hugging Face Raises $235M in New Funding - August 24, 2023 - Maginative</li> <li>LinkedIn Hugging Face People showing 534 employees in April of 2025.</li> </ul>"},{"location":"stories/meta-llama/","title":"The Llama Effect: How Meta Disrupted the LLM Industry","text":"Show Narrative Prompt Title: Making LLMs free: How Meta disrupted the LLM industry by giving away a product that took billions of dollars to build.  Please create a detailed, fun and entertaining story about the decision that Meta made to build the large language model called \"Llama\" and its relevance to the rise open LLMs.  Describe the challenges and costs the team at Meta had building Llama such as getting GPUS and how they overcame these challenges. Describe the incredible time and money that went into building Llama 2.  Discuss the tradeoffs that the company Meta discussed internally to make it open source or charge for its use.  Discuss the strategy of \"scorched earth\" where you give a product away for free to prevent competitors from being able to charge for a product.  Describe the impact this had on other firms that were trying to sell access to their LLMs.  Describe how the job market for LLM engineers collapsed after Llama 2 and future versions was made open source \"forever\".  Our goal is to have you generate the full text of the story, but to turn the story into a graphic novel with many illustrations that explain how the Meta product Llama was created.    When appropriate, suggest an image that could be inserted into the story to make the story a graphic novel. Describe each image in detail and be consistent across all the images in the story for style.  When you describe an image, make sure to mention that it should be a colorful, bright wide-landscape drawing suitable for technology-forward optimistic graphic-novel."},{"location":"stories/meta-llama/#chapter-1-the-ai-arms-race-begins","title":"Chapter 1: The AI Arms Race Begins","text":"Show Image Prompt Image 1: Create a colorful, bright wide-landscape drawing showing Meta headquarters in Menlo Park with a giant llama silhouette casting a shadow over Silicon Valley. In the foreground, tech companies are represented as chess pieces on a digital board, with Meta making a bold move. The style should be vibrant with tech-forward optimistic graphic-novel aesthetics.  <p>In the sprawling tech campus of Meta in Menlo Park, California, executives gathered in a glass-walled conference room in early 2022. The AI landscape was changing rapidly. OpenAI's GPT models and Google's research were capturing headlines and imagination. Meta, despite its massive resources and talent pool, was being perceived as falling behind in the AI revolution.</p> <p>\"We can't afford to be left out of this,\" said Mark Zuckerberg, Meta's CEO, scanning the room. \"The future of tech is being reshaped by large language models. We need our own competitive offering.\"</p> <p>The tech giant had already invested billions in AI research through its FAIR (Facebook AI Research) lab established in 2013. But now, a more ambitious project was taking shape - one that would eventually become known as Llama.</p> <p>\"What if we don't just build another closed AI system?\" suggested one executive. \"What if we take a completely different approach?\"</p>"},{"location":"stories/meta-llama/#chapter-2-assembling-the-team","title":"Chapter 2: Assembling the Team","text":"Image Prompt Image 2: Create a colorful, bright wide-landscape drawing depicting a montage of Meta AI researchers being recruited from around the world. The image shows a global map with light beams connecting to Meta headquarters, with diverse faces of researchers working on complex equations and neural architecture diagrams. The style should maintain the tech-forward optimistic graphic-novel aesthetics with vibrant blues and purples.  <p>Meta began assembling an elite team of AI researchers, computational linguists, and engineering specialists from across the globe. The project would need the brightest minds in natural language processing, reinforcement learning, and distributed computing.</p> <p>Unlike traditional product teams that might work in isolation until launch, this team had a clear mandate: build a foundation model that could compete with or exceed the capabilities of GPT and other leading models, but with the potential to be shared with the world.</p> <p>\"We're bringing together people who believe AI should be more open,\" explained one team leader during an early all-hands meeting. \"This isn't just about catching up to competitors; it's about changing the game entirely.\"</p> <p>The team quickly expanded to hundreds of specialists, many relocating to California to join what was becoming one of the most ambitious AI projects in Meta's history.</p>"},{"location":"stories/meta-llama/#chapter-3-the-gpu-challenge","title":"Chapter 3: The GPU Challenge","text":"Image Prompt 3: A colorful, bright wide-landscape drawing showing a massive data center filled with rows of GPU servers glowing with processing power. Engineers are shown navigating between server racks with diagnostic tablets, while monitors display training progress. In one corner, executives are reviewing mounting cost projections. The style should convey both technological scale and the financial investment with bright, optimistic tech-forward aesthetics.  <p>By mid-2022, the first major obstacle became apparent: securing enough computational power. Training a large language model requires enormous computational resources, primarily in the form of specialized GPUs (Graphics Processing Units).</p> <p>\"We need thousands of NVIDIA A100 GPUs to train at the scale we're planning,\" explained the infrastructure lead. \"And we're in the middle of a global chip shortage.\"</p> <p>The team faced a harsh reality: even for a company with Meta's resources, obtaining the necessary hardware during a worldwide semiconductor shortage was a significant challenge. The price per GPU had skyrocketed to over $10,000 each, and Meta would need thousands of them.</p> <p>\"We're looking at an initial hardware investment of over $100 million just to begin serious training,\" the financial team reported. \"And that's assuming we can even secure the supply.\"</p> <p>Meta leveraged its industry relationships, made advance commitments, and even designed custom cooling solutions to maximize GPU efficiency. The company repurposed entire data centers, prioritizing the Llama project over other computational needs.</p> <p>\"This is probably the largest computational undertaking in our company's history,\" noted one engineering director as they toured a newly configured data center in Nevada, rows of specialized servers humming with activity.</p>"},{"location":"stories/meta-llama/#chapter-4-the-training-begins","title":"Chapter 4: The Training Begins","text":"Image Prompt 4: A colorful, bright wide-landscape drawing showing a visualization of the Llama training process. The image features a translucent llama figure composed of flowing data streams, with researchers monitoring real-time metrics on futuristic displays. Clock faces around the scene show the passage of months. One side shows the input of diverse text datasets flowing into the model, while the other shows emerging capabilities. The style should be dynamic and educational with the tech-forward optimistic aesthetic maintained throughout.  <p>By late 2022, training on the first Llama model began in earnest. The process would consume millions of dollars in computing resources each day.</p> <p>\"We're processing trillions of tokens from hundreds of languages,\" explained the lead data scientist. \"The model is learning from books, articles, code repositories, scientific papers - basically the written knowledge of humanity.\"</p> <p>Training a model of this scale wasn't just expensive; it was a technical marathon. Engineers worked in shifts, monitoring the training process 24/7. Any instability or unexpected behavior could waste days of computing time worth millions of dollars.</p> <p>\"It's like building a rocket while it's launching,\" quipped one engineer during a 3 AM troubleshooting session. \"We're constantly adjusting parameters, addressing biases, and fixing training anomalies.\"</p> <p>The scale was unprecedented for Meta. Each day of training cost approximately $2-3 million in computing resources alone. Over months of training, fine-tuning, and iterative improvements, the total cost would climb into the billions.</p> <p>\"We're spending more each week on this project than many AI startups have raised in total,\" observed one executive during a budget review. \"This better work.\"</p>"},{"location":"stories/meta-llama/#chapter-5-the-first-results","title":"Chapter 5: The First Results","text":"Image Prompt  5: A colorful, bright wide-landscape drawing showing the first internal demonstrations of Llama capabilities. The scene depicts an auditorium at Meta headquarters where executives and team members are watching demonstrations on a giant screen. The screen shows Llama solving complex problems, generating code, and answering questions in multiple languages. Reactions range from amazement to strategic contemplation. The style should convey the excitement of breakthrough with the tech-forward optimistic graphic-novel aesthetic.   <p>By early 2023, the first complete version of Llama was ready for internal testing. The results exceeded expectations. The model demonstrated remarkable capabilities in reasoning, language understanding, and knowledge representation - sometimes surpassing competing models on benchmark tests.</p> <p>\"It's working,\" announced the project lead during the first major demonstration to Meta's executive team. \"In some domains, we're seeing performance that matches or exceeds models from OpenAI and Anthropic.\"</p> <p>What made Llama particularly notable was its efficiency. The model achieved impressive results with fewer parameters than competitors, making it potentially more accessible for organizations with limited computational resources.</p> <p>As impressive as the technical achievements were, a bigger question loomed: what would Meta do with this powerful new technology? The traditional approach would be to create a proprietary service, similar to ChatGPT or Claude, and monetize access.</p> <p>But some on the team had more radical ideas.</p>"},{"location":"stories/meta-llama/#chapter-6-the-strategic-debate","title":"Chapter 6: The Strategic Debate","text":"Image Prompt  6: A colorful, bright wide-landscape drawing depicting a heated boardroom debate at Meta. The scene shows executives divided across a long table, with projection slides showing different business models. One side shows traditional monetization strategies with dollar signs, while the other shows open-source approaches with community growth metrics. Zuckerberg is at the head of the table, contemplatively listening to arguments from both sides. The style should convey the tension of this pivotal decision with dramatic lighting while maintaining the tech-forward optimistic graphic-novel aesthetic.   <p>As Llama's capabilities became clear, intense strategic debates erupted within Meta's leadership. The investment had already reached billions of dollars. How should they recoup this massive expenditure?</p> <p>\"The conventional approach would be to create a ChatGPT competitor,\" argued one executive. \"We could charge enterprise customers for API access while offering limited free tiers to individuals. Revenue projections suggest we could recover our investment within 18-24 months.\"</p> <p>Others proposed a different path.</p> <p>\"What if we open-source the model?\" suggested another leader. \"We could disrupt the entire market. No one could charge premium prices for something similar to what we're giving away for free.\"</p> <p>The debate continued for weeks. Financial models were created, market analyses were conducted, and competitive responses were gamed out. The decision would have enormous implications not just for Meta but for the entire AI industry.</p> <p>\"This is classic scorched earth strategy,\" noted one business development director. \"By giving away what others are selling, we eliminate their business model. It's aggressive, but potentially transformative for our position in AI.\"</p> <p>Zuckerberg, who had remained mostly quiet during these debates, finally weighed in: \"Open is better aligned with Meta's long-term interests. We're not primarily an AI company - we're building the metaverse. Having AI become a commodity that everyone can use benefits our ecosystem.\"</p>"},{"location":"stories/meta-llama/#chapter-7-the-open-source-decision","title":"Chapter 7: The Open Source Decision","text":"Image Prompt  7: A colorful, bright wide-landscape drawing showing Zuckerberg making the historic announcement of Llama going open source. The image shows him on stage with the Llama logo prominently displayed behind him. Split screens show reactions around the tech world - some competitors looking concerned, researchers celebrating, and developers immediately downloading the model. News headlines appear throughout the composition announcing this industry-shaking move. The style should be triumphant and momentous while maintaining the tech-forward optimistic graphic-novel aesthetic.   <p>In February 2023, Meta announced the first open release of Llama. While this initial release came with some usage restrictions, it signaled Meta's intentions clearly: they were taking a fundamentally different approach to AI than their competitors.</p> <p>\"Today, we're releasing Llama for research purposes,\" announced the official statement. \"We believe that the open development of AI technologies is better for innovation, safety, and responsibility.\"</p> <p>The tech world was stunned. A company had spent billions developing technology that others were charging millions to access - and then essentially given it away. The strategic implications were immediately apparent to industry observers.</p> <p>\"Meta just detonated a nuclear bomb in the AI business model landscape,\" wrote one prominent tech analyst. \"How do you charge for something when a comparable version is available for free?\"</p> <p>While this first release had some limitations, it was just the beginning of Meta's open strategy. The real industry earthquake would come a few months later.</p>"},{"location":"stories/meta-llama/#chapter-8-llama-2-open-forever","title":"Chapter 8: Llama 2 - \"Open Forever\"","text":"Image Prompt 8: A colorful, bright wide-landscape drawing showing the Llama 2 launch event with even greater impact. The scene shows Meta's campus with \"LLAMA 2: OPEN FOREVER\" projected on buildings. Multiple screens show the model's improved capabilities and the completely open license terms. One part of the image shows a globe with digital access points lighting up as developers worldwide immediately begin implementing the technology. The style should convey the global impact of this decision while maintaining the tech-forward optimistic graphic-novel aesthetic.  <p>In July 2023, Meta made an even bolder move: releasing Llama 2 with an unprecedented \"open forever\" commitment. This new version was not just research-oriented but fully commercial-ready, and organizations could use it in products without paying Meta royalties.</p> <p>\"Today, we're releasing Llama 2 as a fully open source model,\" announced Zuckerberg. \"We believe AI is better when it's open.\"</p> <p>The improvements in Llama 2 were substantial - it had been trained on 40% more data than the original, with significantly expanded context windows and improved performance across nearly all benchmarks. Most importantly, it came with commercial use rights that allowed companies to build products on top of it without paying Meta.</p> <p>The cost had been staggering - estimates suggested Meta had spent between $5-7 billion developing and training Llama 2. And they were giving it away for free.</p> <p>\"This is one of the most expensive 'free' products in the history of technology,\" observed one industry analyst. \"Meta has essentially made a multi-billion dollar donation to the tech ecosystem.\"</p>"},{"location":"stories/meta-llama/#chapter-9-industry-shockwaves","title":"Chapter 9: Industry Shockwaves","text":"Image Prompt 9: A colorful, bright wide-landscape drawing showing the aftermath in the AI industry. The image depicts various AI startups and companies represented as ships on a turbulent digital ocean, with some capsizing while others frantically change course. Stock tickers show falling prices for AI companies. In contrast, a massive community of developers is shown collaboratively building on Llama, creating a ecosystem of applications. The style should dramatically convey market disruption while maintaining the tech-forward optimistic graphic-novel aesthetic.  <p>The impact on the AI industry was immediate and profound. Smaller AI companies that had planned to compete with proprietary models suddenly found their core business strategy undermined. Why would customers pay for access to proprietary models when they could use Llama for free?</p> <p>\"We're seeing a complete reassessment of business models across the sector,\" noted one venture capitalist. \"Companies that were planning to sell API access to foundation models are pivoting to specialized applications and services instead.\"</p> <p>Valuations of AI startups focused purely on developing foundation models plummeted. Several smaller companies that had raised funds on the promise of creating competitive large language models announced layoffs or complete strategic pivots.</p> <p>\"Meta has effectively commoditized a technology that others were hoping would be their primary revenue source,\" explained an industry analyst. \"It's a classic example of a larger player with diverse revenue streams undermining a market they don't need to monetize directly.\"</p> <p>For enterprise customers, the calculus changed overnight. Using proprietary models with per-token pricing suddenly seemed financially questionable when comparable open alternatives existed.</p>"},{"location":"stories/meta-llama/#chapter-10-the-job-market-transformation","title":"Chapter 10: The Job Market Transformation","text":"Image Prompt 10: A colorful, bright wide-landscape drawing depicting the changing AI job market. The image shows a split scene: on one side, LLM model development teams are being downsized with engineers leaving buildings with boxes; on the other side, application developers and AI implementation specialists are being hired in growing numbers. Digital job boards show changing salary trends and shifting demand. The central figure shows engineers moving from core model development to application building. The style should be informative while maintaining the tech-forward optimistic graphic-novel aesthetic.  <p>By late 2023, the job market for AI engineers had undergone a dramatic transformation. Companies that had been aggressively hiring specialists to build proprietary large language models suddenly froze recruitment or reduced headcount.</p> <p>\"Why spend millions building what Meta is giving away for free?\" became a common refrain in board rooms across the tech industry.</p> <p>Senior AI engineers who had commanded salaries of $500,000 or more to develop foundation models found their specialized skills less in demand. Compensation packages that had once seemed untouchable began to normalize.</p> <p>\"We're seeing a 30-40% reduction in total compensation packages for core LLM development roles,\" reported one tech recruitment specialist. \"Companies are reallocating their AI budgets from core model development to application building and implementation.\"</p> <p>However, this didn't mean AI talent overall was less valuable. The demand simply shifted. Engineers who could effectively implement, fine-tune, and build applications on top of open models like Llama became highly sought after.</p> <p>\"The job market hasn't collapsed; it's transformed,\" explained an industry observer. \"We're seeing explosive growth in roles focused on applying AI rather than building foundational models from scratch.\"</p>"},{"location":"stories/meta-llama/#chapter-11-the-rise-of-the-open-ai-ecosystem","title":"Chapter 11: The Rise of the Open AI Ecosystem","text":"Image Prompt 11: A colorful, bright wide-landscape drawing showing the thriving ecosystem that developed around open LLMs. The image depicts a vibrant digital marketplace with developers creating thousands of specialized applications built on Llama foundations. Different industries are shown implementing AI solutions that were previously unaffordable. Educational institutions are teaching with open models, and researchers from diverse backgrounds are contributing improvements. The style should be bustling and optimistic, showing the democratization of AI technology with the tech-forward optimistic graphic-novel aesthetic.  <p>As 2024 progressed, an unexpected phenomenon emerged. Rather than stifling innovation, Meta's open source strategy had accelerated it. Without needing to rebuild the fundamental technology, thousands of developers could focus on creating specialized applications and improvements.</p> <p>Small teams that could never have afforded to build or license proprietary models began developing innovative AI applications for specific industries and use cases. Academic researchers who had been priced out of working with cutting-edge AI now had access to state-of-the-art models.</p> <p>\"We're seeing more diverse and creative applications of AI than ever before,\" noted one researcher. \"When powerful models become accessible to everyone, innovation happens in places it never could have otherwise.\"</p> <p>Organizations in healthcare, education, scientific research, and public services - often operating with limited budgets - began implementing AI solutions that would have been financially impossible in a closed ecosystem.</p> <p>Meta continued releasing improvements, culminating in Llama 3, which closed the performance gap with even the most advanced proprietary models while maintaining the open license.</p>"},{"location":"stories/meta-llama/#chapter-12-metas-ultimate-strategy","title":"Chapter 12: Meta's Ultimate Strategy","text":"Image Prompt 12: A colorful, bright wide-landscape drawing depicting Meta's long-term vision. The image shows Llama becoming a foundation layer of the digital world, with Meta's metaverse and other products built on top. Zuckerberg is depicted as a chess player who has made a winning move, with AI becoming infrastructure rather than a product. The scene shows Meta products gaining adoption while competitors are distracted by model development. The style should be visionary and strategic while maintaining the tech-forward optimistic graphic-novel aesthetic.  <p>As the dust settled on Meta's disruptive strategy, the company's long-term vision became clearer. By commoditizing foundation models, Meta had shifted the competitive landscape to areas where it held advantages.</p> <p>\"Meta doesn't need to sell AI as a product,\" explained one analyst. \"They need AI as infrastructure for their vision of the metaverse and their family of apps. By making foundation models essentially free for everyone, they've redirected their competitors' resources while advancing their own strategic goals.\"</p> <p>For Meta, the billions spent developing Llama weren't wasted - they were an investment in reshaping the competitive landscape. While other tech giants had to either match Meta's open approach or justify premium pricing, Meta could focus on integrating AI throughout its products and services.</p> <p>\"It's a brilliant strategic move when you consider Meta's overall business,\" noted a business strategist. \"They've turned what could have been a costly distraction into a competitive advantage.\"</p>"},{"location":"stories/meta-llama/#epilogue-the-legacy-of-the-llama-decision","title":"Epilogue: The Legacy of the Llama Decision","text":"Image Prompt 13: A colorful, bright wide-landscape drawing showing the future landscape of AI. The image depicts AI as ubiquitous infrastructure rather than expensive proprietary technology. Scientists and developers from all backgrounds are shown advancing the technology collaboratively. The scene shows beneficial applications in medicine, education, and sustainability that wouldn't have been possible in a closed ecosystem. The style should be inspiring and forward-looking, emphasizing the democratization of advanced technology with the tech-forward optimistic graphic-novel aesthetic.  <p>Meta's decision to open-source Llama reshaped the AI industry in ways that extended far beyond business strategies and market valuations. By making advanced AI accessible to everyone, Meta had democratized a technology that might otherwise have remained concentrated in the hands of a few wealthy corporations.</p> <p>\"In retrospect, the Llama decision may be seen as one of the most significant inflection points in the development of artificial intelligence,\" reflected an AI historian. \"It transformed AI from a luxury that only the largest organizations could afford into a utility available to anyone with the skill to use it.\"</p> <p>The billions Meta invested\u2014money that shareholders might have preferred to see directed toward immediate profits\u2014had effectively funded a global public good. Researchers, developers, students, non-profits, and businesses of all sizes now had access to technology that would have cost millions to develop independently.</p> <p>\"Meta's decision did cause short-term disruption,\" acknowledged an industry veteran. \"But it also prevented an unhealthy concentration of AI power and accelerated beneficial applications that might never have emerged in a strictly commercial environment.\"</p> <p>The story of Llama demonstrated how strategic decisions made by large technology companies could reshape entire industries\u2014sometimes in ways that extended beyond business concerns to influence how technology itself evolved and who could access its benefits.</p> <p>In the end, Meta had taken a technology that cost billions to develop and made it freely available to the world. Whatever the strategic motivations, the result was a more open, accessible, and innovative AI ecosystem that benefited far more people than a closed approach ever could have.</p>"},{"location":"stories/meta-llama/#references","title":"References","text":"<ul> <li>Meta Llama Homepage</li> <li>The Llama 2 Foundation Models Technical Report  arXiv:2307.09288</li> <li>Meta unveils Llama 3, a new family of open-source AI models</li> <li>The State of Open Source AI</li> <li>Inside Meta's Quest to Build a Universal Language Translator</li> <li>The ChatGPT Competition: Open-Source Models Are Catching Up Fast</li> <li>Meta's AI Strategy is More About Scorched Earth Than Saving the Future of Tech</li> <li>The Impact of Llama on the AI Ecosystem</li> <li>The Rise of Open Source Foundation Models for AI</li> <li>Llama (language model) on Wikipedia</li> <li>Meta\u2019s unique approach to developing AI puzzles Wall Street, but techies love it CNBC- Mon, Oct 16 2023</li> <li>Meta makes its powerful AI model Llama 2 free for commercial use</li> </ul>"},{"location":"stories/text-to-image-models/","title":"Diffusion Dreams: The Epic Tale of Text-to-Image Revolution","text":"Show Narrative Prompt Please generate  a detailed, fun and entertaining story about the creation of text to image diffusion models.  Highlight the roles that Jonathan Ho, Robin Rombach, and Aditya Ramesh played in making these models useful for text to image generation tasks.  Describe the challenges each of these three people faced and how they overcame these obstacles to create a high-quality text to image applications.  Describe how an entire graphic novel can now be generated using these tools.  Our goal is to have you generate the full text of the story, but to turn the story into a graphic novel with many illustrations that explain how the text-to-image models were created.  When appropriate, suggest an image that could be inserted into the story to make the story a graphic novel. Describe each image in detail and be consistent across all the images in the story for style. When you describe an image, make sure to mention that it should be a colorful, bright wide-landscape drawing suitable for technology-forward optimistic graphic-novel."},{"location":"stories/text-to-image-models/#prologue-the-ai-art-renaissance","title":"Prologue: The AI Art Renaissance","text":"Show Image Prompt 1: A colorful, bright wide-landscape drawing showing a futuristic laboratory with multiple screens displaying abstract patterns transforming into recognizable images. The style should be optimistic tech-forward graphic novel art with glowing blue and purple hues suggesting AI technology.  <p>In the not-so-distant past, computers could crunch numbers and process text, but creating beautiful, meaningful images from simple text descriptions remained in the realm of science fiction. The world of artificial intelligence was evolving rapidly, but the bridge between human language and visual creativity seemed an impossible chasm to cross.</p> <p>This is the story of three visionaries who dared to dream differently \u2013 Jonathan Ho, Robin Rombach, and Aditya Ramesh \u2013 and how their revolutionary work transformed the impossible into the everyday, allowing machines to paint with words and unleashing a new creative revolution.</p>"},{"location":"stories/text-to-image-models/#chapter-1-the-noise-whisperer-jonathan-hos-breakthrough","title":"Chapter 1: The Noise Whisperer - Jonathan Ho's Breakthrough","text":"Show Image Prompt Image 2 Please generate a new drawing.  It is a bright, colorful wide-landscape panel showing Jonathan Ho as a young researcher working late at night in a Stanford lab. Computer screens around him display mathematical equations and grainy images gradually becoming clearer. The art style should maintain the optimistic tech-forward aesthetic with warm lighting creating a sense of discovery.  <p>Jonathan Ho stared at his computer screen, rubbing his tired eyes. For months, he had been grappling with a seemingly impossible problem: how to make diffusion models \u2013 a theoretically promising but practically useless approach to AI image generation \u2013 actually work.</p> <p>\"Everyone's betting on GANs,\" his colleague remarked, referring to the Generative Adversarial Networks that dominated the field. \"Maybe we should too.\"</p> <p>But Jonathan wasn't convinced. GANs had their limitations \u2013 they were notoriously unstable and difficult to train. There had to be another way.</p> <p>The core idea behind diffusion models was elegantly simple: start with random noise and gradually transform it into a coherent image. It mimicked a natural process, like watching a photograph slowly develop in chemical solution. The problem was that existing approaches were painfully slow and produced blurry, disappointing results. </p> Show Image Prompt Image 3: Please generate a new drawing. A split-panel wide-landscape illustration showing Jonathan with mathematical equations floating around him as if in thought bubbles, contrasted with early blurry diffusion model outputs versus sharp GAN outputs. The style should be bright and colorful with visual representations of the mathematical concepts.  <p>\"What if we approach the math differently?\" Jonathan wondered. Instead of viewing the process as a complicated stochastic differential equation, what if they reframed it as a series of denoising steps? Each step would be simpler, more predictable.</p> <p>For weeks, Jonathan and his team at UC Berkeley reformulated the equations, tested, failed, adjusted, and tested again. They were entering uncharted mathematical territory.</p> <p>The breakthrough came in 2020. Jonathan's new approach \u2013 Denoising Diffusion Probabilistic Models (DDPM) \u2013 finally showed promise. Images generated by his system weren't just coherent; they were beginning to rival the quality of GANs while avoiding many of their pitfalls.</p> <p>\"We've done it,\" Jonathan said, watching as a clear image of a bird emerged from what had started as random static. \"This changes everything.\"</p> <p>Little did he know just how right he was. Jonathan's paper, \"Denoising Diffusion Probabilistic Models,\" would become the foundation that others would build upon \u2013 the first crucial piece of the puzzle that would eventually enable machines to draw from our words.</p>"},{"location":"stories/text-to-image-models/#chapter-2-the-efficiency-engineer-robin-rombachs-compression-breakthrough","title":"Chapter 2: The Efficiency Engineer - Robin Rombach's Compression Breakthrough","text":"Show Image Prompt Image 4:  Please generate a new drawing.  A colorful wide-landscape panel showing Robin Rombach in a German university lab with servers and cooling systems. Visual indicators show excessive power consumption. Robin looks concerned while examining resource usage graphs. The tech-forward style should incorporate elements of European architecture in the background.  <p>Meanwhile, across the ocean at LMU Munich in Germany, Robin Rombach faced a different challenge. By 2021, researchers were producing increasingly impressive results with diffusion models thanks to Jonathan Ho's breakthrough, but there was a massive problem: computational cost.</p> <p>\"At this rate, only the biggest tech companies with enormous data centers could ever use these models,\" Robin explained to his research partner Patrick Esser. \"It takes too much computing power, too much memory, too much time.\"</p> <p>The numbers were staggering. Training state-of-the-art diffusion models required hundreds of thousands of dollars in computing resources. Running them needed specialized hardware far beyond what any artist or designer could access. If diffusion models were to democratize AI art creation, something had to change. </p> Show Image Prompt Image 5: Please generate a new drawing. A bright, colorful wide-landscape illustration showing Robin working with abstract visual representations of compression - data flowing from a large space into a smaller, dense \"latent\" space. The tech-forward style should use contrasting colors to show the before/after efficiency.  <p>Robin's insight came from an unexpected direction: what if the diffusion process didn't need to work on full-sized images? What if they could compress images into a much smaller \"latent\" space first?</p> <p>\"It's like instead of cleaning every individual brick in a building, we work with a blueprint,\" Robin explained. \"The blueprint contains all the essential information but is much smaller and easier to manipulate.\"</p> <p>The mathematics were daunting. Robin and his team at the CompVis Lab needed to create a model that could compress images into this latent space without losing critical details, then run the diffusion process there, and finally decode the result back into a full image.</p> <p>When their Latent Diffusion Model finally worked, the results were revolutionary. The computational requirements dropped dramatically \u2013 by orders of magnitude. What once required industrial-grade hardware could now potentially run on a decent laptop.</p> <p>\"We've opened the door,\" Robin said when their paper was published in 2022. \"Now anyone can walk through it.\"</p> <p>Robin's breakthrough would directly enable the creation of Stable Diffusion, the first open-source text-to-image model powerful enough to compete with commercial offerings while being accessible enough for widespread use.</p>"},{"location":"stories/text-to-image-models/#chapter-3-the-language-whisperer-aditya-ramesh-bridges-worlds","title":"Chapter 3: The Language Whisperer - Aditya Ramesh Bridges Worlds","text":"<p>![](./</p> Show Image Prompt Image 6: Please generate a new drawing. A colorful wide-landscape panel showing Aditya Ramesh at OpenAI headquarters surrounded by both text documents and images, with visible connecting lines between words and visual elements. The tech-forward style should show abstract representations of language understanding with glowing connections.  <p>While Jonathan Ho had made diffusion models viable, and Robin Rombach had made them efficient, there remained a critical missing piece: making these models understand and follow natural language instructions.</p> <p>At OpenAI in San Francisco, Aditya Ramesh was tackling this exact problem. The challenge was enormous \u2013 human language is nuanced, contextual, and infinitely varied. How could a computer possibly understand what \"a cat wearing a space helmet watching sunset on Mars\" should look like?</p> <p>\"The gap between text and images is one of the most fundamental challenges in AI,\" Aditya explained to his team. \"We need to build a bridge between these two worlds.\" ![](./</p> Show Image Prompt 7:  Please generate a new drawing. A bright wide-landscape panel showing a split screen: on one side, early text-to-image attempts with mismatched elements and odd artifacts; on the other side, Aditya working with CLIP architecture visualizations. The tech-forward style should use color coding to show the alignment between text and image features.  <p>Previous attempts had produced disappointing results. Models might understand \"cat\" and \"space\" separately, but couldn't compose them correctly. They might get the content right but miss the style, or vice versa.</p> <p>Aditya and his team developed a two-pronged approach. First, they created CLIP (Contrastive Language-Image Pre-training), a model trained on hundreds of millions of image-text pairs from the internet. CLIP learned to understand the relationship between visual concepts and language.</p> <p>Then, they figured out how to use CLIP to guide diffusion models, effectively giving them \"artistic direction\" as they transformed noise into images. This guidance system allowed the models to understand increasingly complex prompts.</p> <p>When DALL-E was first unveiled in January 2021, it shocked the world. Here was an AI that could create images from text descriptions with unprecedented accuracy and creativity. DALL-E 2, released in 2022, would push the boundaries even further.</p> <p>\"This isn't just about making pretty pictures,\" Aditya said. \"It's about creating a new interface between human creativity and machine capabilities.\"</p>"},{"location":"stories/text-to-image-models/#chapter-4-the-symphony-comes-together","title":"Chapter 4: The Symphony Comes Together","text":"<p>![](./</p> Show Image Prompt 8: Please generate a new drawing. A colorful wide-landscape panel showing all three protagonists (Jonathan, Robin, and Aditya) in separate locations but working on connected screens, with their innovations visually flowing together to create a complete system. The tech-forward style should use flowing light paths to show how their separate work connects.  <p>By mid-2022, the stage was set for revolution. Jonathan Ho's mathematical framework made diffusion models work effectively. Robin Rombach's latent compression made them efficient enough for widespread use. And Aditya Ramesh's text understanding and guidance systems made them respond accurately to natural language.</p> <p>When Stability AI released Stable Diffusion, built on Robin's latent diffusion approach, the final piece clicked into place. For the first time, a powerful text-to-image model was not just available to the public, but open source \u2013 anyone could download, use, modify, and build upon it.</p> <p>The impact was immediate and tremendous. Overnight, designers, artists, writers, and everyday people gained access to an entirely new creative medium.</p> <p></p> Show Image Prompt 9: Please generate a new drawing. A bright, colorful wide-landscape panel showing a diverse group of people using text-to-image tools on various devices: professional artists, children, elderly people, all creating different types of images. The tech-forward style should emphasize accessibility and diversity.  <p>\"What would have taken a professional artist hours or days can now be created in seconds,\" marveled a digital art director. \"It's not replacing artists \u2013 it's giving them superpowers.\"</p> <p>The three pioneers watched with amazement as their years of research transformed into tools used by millions. What had begun as theoretical computer science papers had become a new artistic medium accessible to all.</p>"},{"location":"stories/text-to-image-models/#chapter-5-the-graphic-novel-revolution","title":"Chapter 5: The Graphic Novel Revolution","text":"Show Image Prompt 10: Please generate a new drawing. A colorful wide-landscape panel showing a writer/artist working with a text-to-image system to create comic panels. Multiple monitors show text prompts being transformed into sequential art. The tech-forward style should show a modern creative workspace with both traditional art tools and cutting-edge technology.  <p>By 2023, creators began pushing the boundaries of what was possible with these new tools. Among the most exciting applications was the creation of entire graphic novels using text-to-image systems.</p> <p>The process worked like a creative partnership between human and machine. Writers would craft their stories, describing each panel in natural language: \"A cyberpunk detective stands on a neon-lit rooftop overlooking a rainy futuristic city, searching for clues with a holographic magnifying glass.\"</p> <p>The diffusion model would generate the panel, which could then be refined with additional prompts or light editing. Artists could maintain a consistent style by including style descriptors in their prompts or by using techniques like \"img2img\" \u2013 where an existing image guides the creation of a new one, maintaining visual consistency.</p> <p></p> Show Image Prompt 11: Please generate a new drawing. A bright, colorful wide-landscape panel showing a sequence of comic creation: script to text prompt to generated image to final panel with speech bubbles. The tech-forward style should emphasize the workflow from text to finished comic.  <p>\"What used to require a full production team \u2013 writer, penciler, inker, colorist \u2013 can now be accomplished by one or two people,\" explained a comic publisher. \"It's democratizing sequential art creation in ways we never imagined possible.\"</p> <p>The process wasn't without challenges. Maintaining character consistency across panels required careful prompting and occasional touch-ups. Complex action scenes needed thoughtful description. But the barriers to creating visual stories had never been lower.</p> <p>Independent creators flourished in this new landscape. Stories that might never have been visualized due to budget constraints or lack of artistic connections could now come to life. Experimental graphic novels exploring unusual concepts found audiences. Written works were adapted into visual formats with unprecedented speed. </p> Show Image Prompt 12: Please generate a new drawing. A colorful wide-landscape panel showing a digital bookshelf with diverse graphic novels created using AI tools, with readers of all ages enjoying them on various devices. The tech-forward style should show the diversity of content and accessibility."},{"location":"stories/text-to-image-models/#epilogue-the-continuing-revolution","title":"Epilogue: The Continuing Revolution","text":"Show Image Prompt 13: Please generate a new drawing. A bright, colorful wide-landscape panel showing Mr. Jonathan Ho, Mr. Robin Rombach, and Mr. Aditya Ramesh looking toward a sunrise/horizon with futuristic silhouettes representing future innovations. The tech-forward style should create an optimistic mood with warm colors and forward-looking imagery.  <p>As for our three pioneers, they continue pushing the boundaries of what's possible. Jonathan Ho's mathematical insights have led to faster, more efficient diffusion processes. Robin Rombach's work on compression and efficiency has helped models run on increasingly accessible hardware. And Aditya Ramesh's advancements in connecting language and imagery have made models understand ever more nuanced instructions.</p> <p>The text-to-image revolution they helped create continues to evolve at breakneck speed. New models emerge with improved capabilities: better understanding of physical laws, more accurate rendering of human anatomy, stronger adherence to compositional rules, and the ability to maintain consistency across multiple images.</p> <p>\"We're just seeing the beginning,\" Jonathan observes. \"Think of early cinema \u2013 those first silent films with simple effects. We're at that stage with AI imagery. Imagine where we'll be in ten or twenty years.\"</p> <p>Perhaps most importantly, these technologies are increasingly becoming tools for human expression rather than replacements for human creativity. They amplify our abilities, helping us visualize ideas that might otherwise remain trapped in our minds. </p> Show Image Prompt 14: Please generate a new drawing. A final colorful wide-landscape panel showing a diverse group of creators collaboratively using next-generation AI tools to build immersive visual stories and worlds. The tech-forward style should show futuristic interfaces and diverse applications beyond current capabilities.  <p>\"The most exciting developments aren't just technical improvements,\" says Aditya. \"They're the unexpected ways people use these tools, combining them with other media and creating entirely new art forms.\"</p> <p>As for graphic novels, they represent just one application of a technology that continues to find new uses \u2013 from conceptualizing architectural designs to visualizing scientific concepts, from creating educational materials to preserving cultural heritage through visual storytelling.</p> <p>The story of text-to-image diffusion models is itself a testament to human ingenuity \u2013 how three researchers from different backgrounds and locations, each solving distinct parts of an enormous challenge, could collectively enable a creative revolution that empowers millions to turn their words into visual reality.</p> <p>And like the best stories, this one is far from over. It's still being written \u2013 and illustrated \u2013 every day.</p> <p>THE END</p>"},{"location":"stories/text-to-image-models/#references","title":"References","text":"<ul> <li>Denoising Diffusion Probabilistic Models - June 19, 2020 - arXiv Jonathan Ho's original DDPM paper that revolutionized diffusion models</li> <li>DDPM GitHub Repository - 2020 - GitHub The official code implementation of Jonathan Ho's groundbreaking diffusion model</li> <li>High-Resolution Image Synthesis with Latent Diffusion Models - December 20, 2021 - arXiv Robin Rombach's seminal paper introducing Latent Diffusion Models</li> <li>Latent Diffusion Models GitHub - 2022 - GitHub Official repository for Rombach's Latent Diffusion Models, which became Stable Diffusion</li> <li>Latent Diffusion Models Explained - 2022 - Papers with Code Technical overview of how Latent Diffusion Models work</li> <li>DALL-E: Creating Images from Text - January 5, 2021 - OpenAI Original announcement of DALL-E by Aditya Ramesh</li> <li>Hierarchical Text-Conditional Image Generation with CLIP Latents - April 13, 2022 - arXiv Aditya Ramesh's paper on DALL-E 2, which uses CLIP for improved text understanding</li> <li>DALL-E Explained GitHub - 2021 - GitHub Detailed explanation of how DALL-E works with code implementation</li> <li>CVPR Paper: High-Resolution Image Synthesis With Latent Diffusion Models - June 2022 - CVPR Proceedings Peer-reviewed conference publication of Rombach's LDM work</li> <li>Two Years After DALL-E Debut, Its Inventor is \"Surprised\" by Impact - February 23, 2023 - VentureBeat Interview with Aditya Ramesh reflecting on the impact of his invention</li> </ul>"},{"location":"stories/transformers/","title":"Transformers and the Attention Revolution: How a Paper Changed AI Forever","text":"Show Narrative  Prompt     Please create a detailed fun and entertaining story about the publication of the paper \"Attention is All You Need\" and how the authors had a huge impact on the creation of LLMs.      Our goal is to have you generate the full text of the story, but to turn the story into a graphic novel with many illustrations that explain how transformers work.      As an example of attention, use the following sentence:      The chicken did not cross the road because it was too [tired|wide].      Give as much detail as possible about the team that wrote the paper and their ability to publish the paper despite being a key innovation at Google.       When appropriate, suggest an image that could be inserted into the story to make the story a form of graphic novel. Describe each image in detail and be consistent across all the images in the story for style."},{"location":"stories/transformers/#chapter-1-the-spark-of-an-idea","title":"Chapter 1: The Spark of an Idea","text":"Show Image Prompt     Panel 1: A nighttime scene at Google Brain's offices in Mountain View, California. Through the windows, we see various researchers working late. Some at whiteboards covered with equations, others at computers with multiple screens displaying neural network diagrams. The art style is reminiscent of a modern graphic novel with clean lines and rich colors, using deep blues and purples for the night sky contrasted with the warm yellow glow from the office windows.  <p>In the summer of 2016, at Google Brain's offices in Mountain View, California, a small team of researchers was grappling with a fundamental problem: neural machine translation systems were becoming increasingly complex, yet their performance improvements were plateauing.</p> <p>Ashish Vaswani, a senior software engineer with a quiet demeanor and brilliant analytical mind, had been pondering an elegant solution. What if they removed the recurrent neural networks (RNNs) and convolutional neural networks (CNNs) entirely from the architecture? What if attention \u2013 a mechanism that helped models focus on relevant parts of the input \u2013 could be the only component they needed?</p> <p>\"What if attention is all we need?\" he mused to his colleague Noam Shazeer one evening, not realizing he had just coined what would become one of the most famous paper titles in AI history.</p>"},{"location":"stories/transformers/#chapter-2-assembling-the-team","title":"Chapter 2: Assembling the Team","text":"Show Image Prompt Panel 2: Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A split-screen showing eight different researchers in their natural working environments. Each person has a thought bubble showing different aspects of the transformer architecture (embeddings, positional encoding, self-attention, etc.). The art style maintains consistency with Panel 1 but adds warm highlights to each character to emphasize their individual contributions. The panel is arranged like a Brady Bunch grid, suggesting how these separate minds would come together.  <p>The team that would revolutionize NLP came together almost by chance \u2013 a collection of researchers from Google Brain, Google Research, and the University of Toronto:</p> <p>Ashish Vaswani and Noam Shazeer from Google Brain were quickly joined by Niki Parmar and Jakob Uszkoreit, who brought expertise in neural network architectures.</p> <p>Llion Jones, known for his meticulous attention to detail, joined to help with the implementation. Aidan Gomez, a visiting undergraduate researcher from the University of Toronto, brought fresh perspective and energy.</p> <p>\u0141ukasz Kaiser, with his background in theoretical computer science and linguistics, provided crucial insights into how language models could be structured.</p> <p>Completing the team was Illia Polosukhin, whose practical engineering experience would be vital for making their theoretical ideas work in practice.</p> <p>Despite being at one of the world's leading AI research labs, they were going against the prevailing wisdom. RNNs and LSTMs were the dominant architectures for sequence modeling, and suggesting to completely remove them seemed almost heretical.</p>"},{"location":"stories/transformers/#chapter-3-the-breakthrough-mechanism","title":"Chapter 3: The Breakthrough Mechanism","text":"Show Image Prompt   Panel 3: Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A large, detailed diagram showing the self-attention mechanism. In the center is a key sentence: \"The chicken did not cross the road because it was too tired.\" Visual lines of different thicknesses connect the word \"it\" to other words, with the thickest line connecting to \"chicken,\" showing how attention helps resolve references. The diagram uses a vibrant, colorful scheme with blues, purples, and yellows to show attention weights. Small mathematical equations float around the edges of the panel, suggesting the formal underpinnings of the concept.  <p>At the heart of their innovation was the self-attention mechanism \u2013 a revolutionary way to process sequences of data like sentences. Unlike previous approaches that processed words in order or used sliding windows, self-attention allowed each word in a sentence to directly \"attend\" to every other word.</p> <p>\"Let me show you why this matters,\" Ashish explained during a team meeting. He wrote on the whiteboard: \"The chicken did not cross the road because it was too tired.\"</p> <p>\"What does it refer to? As humans, we know immediately that 'it' refers to 'the chicken.' But how does a computer know? In previous models, we'd need complex architectures to capture this relationship. With self-attention, each word calculates attention weights with every other word in the sentence.\"</p> <p>He drew arrows from \"it\" to every other word, making the arrow to \"chicken\" much thicker than the others.</p> <p>\"See? The model learns to pay attention to relevant context. If we change the sentence to 'The chicken did not cross the road because it was too wide,' the attention pattern would shift, with 'it' now attending more to 'road' instead of 'chicken'.\"</p> <p>This elegant solution allowed the model to capture long-range dependencies that had been challenging for previous architectures.</p>"},{"location":"stories/transformers/#chapter-4-building-in-secret","title":"Chapter 4: Building in Secret","text":"Show Image Prompt   Panel 4: Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A sequence of small panels showing the team working in various settings \u2013 late nights at the office, discussions over coffee, coding sessions with multiple screens. In the background, a large transformer architecture diagram gradually takes shape, like a blueprint being assembled. The art style should evoke a sense of clandestine development, with shadows and highlights creating a feeling of working on something revolutionary. The color palette shifts to include more reds and oranges, suggesting the building energy and excitement of creation.  <p>For months, the team worked tirelessly, often staying late into the night. They weren't just building a new model; they were reimagining how neural networks could process language.</p> <p>\"We need to be thorough,\" \u0141ukasz insisted. \"If we're going to challenge the status quo, our results need to be undeniable.\"</p> <p>The work wasn't officially sanctioned as a major project, and the team members often contributed during their \"20% time\" \u2013 Google's policy that allowed employees to spend one-fifth of their work week on projects of personal interest.</p> <p>They faced numerous challenges. The model was complex and required significant computational resources. Training was unstable at first, with the network either converging too slowly or diverging entirely.</p> <p>Llion Jones spent weeks debugging gradient issues, while Noam Shazeer optimized the implementation to work efficiently on Google's TPUs (Tensor Processing Units).</p> <p>Aidan Gomez, despite being the most junior member, proposed the multi-head attention mechanism \u2013 allowing the model to attend to information from different representation subspaces simultaneously.</p> <p>\"It's like having multiple perspectives on the same information,\" he explained. \"If single-head attention is like looking at something with one eye, multi-head attention gives you depth perception.\"</p>"},{"location":"stories/transformers/#chapter-5-the-architecture-takes-shape","title":"Chapter 5: The Architecture Takes Shape","text":"Show Image PromptPanel 5: Architecture Diagram  Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A detailed, exploded-view diagram of the Transformer architecture, styled like a technical blueprint but with a modern, digital aesthetic. Each component is color-coded: embeddings (blue), positional encoding (green), multi-head attention (purple), feed-forward networks (orange), and output layer (yellow). Small animated-style figures of the researchers are shown pointing to different components, explaining their functions. The background has a subtle grid pattern, emphasizing the technical nature while maintaining the graphic novel aesthetic.  <p>The final architecture they designed was remarkably elegant. At its core were several key innovations:</p> <ol> <li>Input Embeddings: Converting words into vectors</li> <li>Positional Encoding: Adding information about word position</li> <li>Multi-Head Self-Attention: Allowing words to attend to other words</li> <li>Feed-Forward Networks: Processing the attended information</li> <li>Residual Connections: Helping information flow through the network</li> <li>Layer Normalization: Stabilizing the training process</li> </ol> <p>\"Positional encoding was a breakthrough,\" Jakob explained. \"In RNNs, the order of words is implicit in the processing. But since our attention mechanism processes all words simultaneously, we needed a way to incorporate position information.\"</p> <p>Their solution was ingenious \u2013 they added sinusoidal position embeddings to the word embeddings, creating a unique pattern for each position that the model could learn to interpret.</p> <p>Niki Parmar demonstrated how the encoder-decoder structure allowed the model to be used for various tasks: \"The encoder processes the input sequence, and the decoder generates the output sequence. This works for translation, summarization, question answering \u2013 potentially any language task.\"</p>"},{"location":"stories/transformers/#chapter-6-the-paper-dilemma","title":"Chapter 6: The Paper Dilemma","text":"Show Image PromptPanel 6:   Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A tense meeting room scene. On one side of a long table sit the eight researchers, looking determined but nervous. On the other side are Google executives and managers with skeptical expressions. Documents and laptop screens show competing research from other teams. The lighting creates dramatic shadows, and the color palette uses contrasting cool tones for the executives and warmer tones for the research team, symbolizing the conflict. Outside the windows, we see the Google campus.  <p>By early 2017, the team knew they had something revolutionary. Their model was outperforming state-of-the-art systems in machine translation with a simpler, more elegant architecture. But they faced a dilemma: publishing their findings could give competitors an advantage.</p> <p>\"This is groundbreaking work,\" said one research director during a review meeting. \"But do we want to give away this technology to our competitors?\"</p> <p>Google, like other tech giants, struggled with balancing open science and competitive advantage. The AI research community valued open publication, but companies had business interests to protect.</p> <p>The team argued passionately for publication. \"This will advance the entire field,\" Illia contended. \"And Google will be recognized as the pioneer.\"</p> <p>\u0141ukasz pointed out another reality: \"If we don't publish, someone else will discover this eventually. Better to lead than follow.\"</p> <p>After weeks of internal debate and multiple review meetings, they received conditional approval to publish \u2013 but they would need to navigate the publication process carefully.</p>"},{"location":"stories/transformers/#chapter-7-nips-submission-crunch","title":"Chapter 7: NIPS Submission Crunch","text":"Show Image PromptPanel 7:   Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A montage showing the team in crunch mode. Papers and coffee cups are scattered everywhere. Some team members are slumped over desks, exhausted, while others are frantically typing. Clock faces around the margins show time passing. Computer screens display experimental results and paper drafts. The lighting shifts from daylight to night and back again, suggesting multiple days of intense work. The color palette is intense and slightly oversaturated, conveying the high-pressure environment.  <p>With approval secured, the team now faced a deadline: the submission deadline for NIPS (Neural Information Processing Systems), one of the premier conferences in machine learning, was rapidly approaching.</p> <p>\"We have three weeks,\" Ashish announced. \"We need to run all the experiments, finalize the architecture, and write the paper.\"</p> <p>The team entered crunch mode. Days blurred into nights as they worked to perfect their submission. Additional experiments were run to answer potential reviewer questions. The paper went through dozens of drafts.</p> <p>\"We need a catchy title,\" Noam suggested during a late-night writing session.</p> <p>\"Well, our whole premise is that attention mechanisms alone are sufficient,\" Ashish replied. \"So... 'Attention Is All You Need'?\"</p> <p>The room fell silent for a moment, then erupted in laughter and agreement. It was perfect \u2013 concise, bold, and slightly provocative, just like their approach.</p> <p>With minutes to spare before the deadline, they submitted the paper to NIPS 2017.</p>"},{"location":"stories/transformers/#chapter-8-the-world-notices","title":"Chapter 8: The World Notices","text":"Show Image PromptPanel 8:   Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A world map with glowing points of light appearing across research centers globally, connected by lines forming a network. Inset panels show researchers at different institutions reading the paper with expressions of surprise and excitement. Speech bubbles contain reactions like \"This changes everything!\" and \"We need to implement this immediately!\" The background transitions from dark to light, suggesting the dawn of a new era. The glowing network resembles transformer attention patterns, subtly reinforcing the concept.  <p>When \"Attention Is All You Need\" was published as a preprint on arXiv in June 2017, the reaction was immediate and electric. Research labs and tech companies around the world dropped what they were doing to implement and build upon the Transformer architecture.</p> <p>\"I've never seen anything like this,\" Jakob marveled as he tracked the paper's citations, which were growing exponentially. \"Labs everywhere are adopting our approach.\"  As of April 2025, the paper was cited over 173,000 times.  This makes it one of the top-10 most frequently cited papers in the last 25 years!</p> <p>Within months, the Transformer had become the foundation for new state-of-the-art results in almost every NLP task. Its efficient parallel computation meant it could be scaled to previously unimaginable sizes.</p> <p>At OpenAI, researchers were already thinking about how to adapt the Transformer architecture for generative pre-training. At Google itself, another team was developing BERT, which would use the Transformer encoder to revolutionize language understanding.</p> <p>What made the paper so influential wasn't just the architecture itself but its timing and accessibility. The code was released openly, allowing researchers everywhere to start experimenting immediately. The paper was clear and well-written, making the complex ideas accessible to a wide audience.</p>"},{"location":"stories/transformers/#chapter-9-the-birth-of-llms","title":"Chapter 9: The Birth of LLMs","text":"Show Image PromptPanel 9:   Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A chronological progression showing the evolution of transformer-based models, presented as a family tree. Starting with the original Transformer at the top, branches split to show BERT, GPT, T5, and other key models. Each model is represented by a distinct icon with size proportional to its parameter count. As the timeline moves downward, the models grow dramatically larger. Small annotations show key capabilities gained at each stage. The overall design resembles both a technological evolution chart and a natural growth pattern, reinforcing how these models evolved organically from the original idea. (Panel 9)  <p>The years following the Transformer paper saw an explosion of innovation. The architecture proved to be remarkably scalable \u2013 as models got bigger, their capabilities improved in unexpected ways.</p> <p>In 2018, OpenAI released GPT (Generative Pre-trained Transformer), adapting the decoder portion of the original architecture for text generation.</p> <p>Google countered with BERT (Bidirectional Encoder Representations from Transformers), which used the encoder portion for language understanding.</p> <p>Both approaches demonstrated the versatility of the Transformer architecture.</p> <p>\"What's fascinating,\" Llion remarked during a reunion dinner with the original team in 2019, \"is how different teams took our architecture in different directions. Some focused on the encoder, others on the decoder, and some used the full model.\"</p> <p>The scaling era had begun. GPT-2, released in 2019, showed that scaling up transformer models led to surprisingly capable text generation. GPT-3 in 2020, with 175 billion parameters, demonstrated emergent abilities that nobody had predicted.</p> <p>By 2022, transformer-based Large Language Models (LLMs) like PaLM, LaMDA, and later GPT-4 and Claude were showing capabilities that approached human-level performance on many tasks.</p> <p>\"Did you imagine this would happen when we were working late nights back in 2016?\" \u0141ukasz asked Ashish.</p> <p>\"Never,\" he admitted. \"We thought we had a better architecture for machine translation. We never imagined it would lead to models that could write essays, code programs, and have conversations.\"</p>"},{"location":"stories/transformers/#chapter-10-the-attention-legacy","title":"Chapter 10: The Attention Legacy","text":"Show Image PromptCreate a brightly colored wide-landscape drawing as it would appear in a graphic novel.      Panel 10: A futuristic scene showing applications of transformer technology in everyday life. The panel is divided into multiple areas showing diverse uses: medical diagnosis, creative writing, scientific discovery, education, and more. In the center, the original attention equation floats like a unifying principle connecting everything. The eight original researchers are shown in the foreground, older now, looking at this landscape with expressions of pride and amazement. The style combines realism with conceptual elements, and the color palette is bright and optimistic, suggesting a positive technological future built on their foundation.  <p>By 2025, the Transformer architecture had transformed not just natural language processing but artificial intelligence as a whole. The original eight authors found themselves cited in tens of thousands of papers, their work forming the foundation for an entirely new generation of AI systems.</p> <p>\"Attention is all you need,\" Jakob reflected, \"turned out to be prophetic in ways we never anticipated.\"</p> <p>Their paper, which almost wasn't published due to competitive concerns, had accelerated AI progress by years or possibly decades by being openly shared with the research community.</p> <p>The attention mechanism they pioneered provided a new way of thinking about how neural networks could process information \u2013 not just sequentially or locally, but with direct connections between any elements, regardless of distance.</p> <p>This insight extended beyond language to computer vision, reinforcement learning, protein folding, and numerous other domains. The ability to model relationships between arbitrary elements of a dataset proved universally valuable.</p> <p>\"I think what I'm most proud of,\" Ashish said during a retrospective talk in 2025, \"is not just the paper itself, but how the research community built upon it. Science progresses when ideas are shared and improved upon collectively.\"</p> <p>The legacy of \"Attention Is All You Need\" wasn't just a more efficient architecture \u2013 it was a reminder that sometimes, the most profound innovations come from questioning fundamental assumptions and pursuing elegant simplicity.</p> <p>In the end, attention wasn't all they needed \u2013 they also needed courage to challenge the status quo, persistence to overcome technical challenges, and openness to share their discoveries with the world. But attention was the key that unlocked a new era of artificial intelligence.</p>"},{"location":"stories/transformers/#appendix-understanding-attention-a-visual-explanation","title":"Appendix: Understanding Attention: A Visual Explanation**","text":"Show Image Prompt   Panel 11:  Create a brightly colored wide-landscape drawing as it would appear in a graphic novel.  A detailed diagram showing the sentence \"The chicken did not cross the road because it was too [tired/wide].\" The word \"it\" is highlighted and connects to other words with lines of varying thickness. Two versions are shown side by side - in one, \"it\" connects strongly to \"chicken\" (when \"tired\" is used), and in the other, \"it\" connects strongly to \"road\" (when \"wide\" is used). Arrows and zoomed-in sections show how the attention mechanism calculates scores between words. Mathematical formulas for calculating attention weights appear in simplified, visually appealing formats around the edges. The color scheme matches the main story's palette.  <p>The key insight of the attention mechanism can be understood through our example sentence:</p> <p>\"The chicken did not cross the road because it was too tired.\"</p> <ol> <li> <p>The Problem: What does \"it\" refer to? Is it the chicken or the road?</p> </li> <li> <p>How Attention Solves It: The model calculates \"compatibility scores\" between each word and every other word:</p> </li> <li>it \u2192 chicken: Strong score (0.85)</li> <li>it \u2192 road: Weaker score (0.12)</li> <li>it \u2192 cross: Very weak score (0.03)</li> <li> <p>And so on...</p> </li> <li> <p>Context Matters: If we change the sentence to \"The chicken did not cross the road because it was too wide,\" the attention pattern shifts:</p> </li> <li>it \u2192 chicken: Weaker score (0.14)</li> <li>it \u2192 road: Strong score (0.81)</li> <li> <p>it \u2192 cross: Very weak score (0.05)</p> </li> <li> <p>How It's Calculated: Each word is converted into three vectors:</p> </li> <li>A Query vector (what information am I looking for?)</li> <li>A Key vector (what information do I contain?)</li> <li> <p>A Value vector (what information do I pass along?)</p> </li> <li> <p>The Score: Attention score between words is calculated as the dot product of the Query of one word with the Keys of all other words.</p> </li> <li> <p>The Result: These scores determine how much each word's Value impacts the representation of the current word.</p> </li> </ol> <p>This simple mechanism allows transformers to handle references, understand context, and capture long-range dependencies in text, which proved revolutionary for language understanding and generation.</p>"},{"location":"stories/transformers/#transformer-data-flow-diagram","title":"Transformer Data Flow Diagram","text":"<p>Here is the actual diagram from the \"Attention is All You Need\" paper:</p> <p></p>"},{"location":"stories/transformers/#formulas-and-equations","title":"Formulas and Equations","text":""},{"location":"stories/transformers/#attention-formula-scaled-dot-product-attention","title":"Attention Formula (Scaled Dot-Product Attention)","text":"\\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V  \\] <p>where:</p> <ul> <li>\\(Q\\) is the query matrix</li> <li>\\(K\\) is the key matrix</li> <li>\\(K^T\\) is the transpose of the key matrix</li> <li>\\(V\\) is the value matrix</li> <li>\\(d_k\\) is the dimension of the keys (used for scaling)</li> <li>\\(\\text{softmax}\\) is the softmax function applied row-wise</li> </ul>"},{"location":"stories/transformers/#multi-head-attention","title":"Multi-Head Attention","text":"\\[ MultiHead(Q,K,V)\\=Concat(head1,...,headh)WO\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O MultiHead(Q,K,V)\\=Concat(head1\u200b,...,headh\u200b)WO \\]"},{"location":"stories/transformers/#multi-head-attention_1","title":"Multi-Head Attention","text":"<p>Attention Formula (Scaled Dot-Product Attention)</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] <p>where:</p> <ul> <li>\\(Q\\) is the query matrix</li> <li>\\(K\\) is the key matrix</li> <li>\\(K^T\\) is the transpose of the key matrix</li> <li>\\(V\\) is the value matrix</li> <li>\\(d_k\\) is the dimension of the keys (used for scaling)</li> <li>\\(\\text{softmax}\\) is the softmax function applied row-wise</li> </ul>"},{"location":"stories/transformers/#multi-head-attention_2","title":"Multi-Head Attention","text":"\\[ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\] <p>where each head is computed as:</p> \\[ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\] <p>and \\(QW_i^Q\\), \\(KW_i^K\\), \\(VW_i^V\\) are parameter matrices for each attention head</p> <ul> <li>\\(W^O\\) is the output projection matrix</li> <li>\\(h\\) is the number of attention heads</li> </ul> <p>The softmax function itself is defined as:</p> \\[ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\] <p>This formula is the core of the transformer architecture described in the \"Attention Is All You Need\" paper, allowing the model to weigh the importance of different words in relation to each other.</p>"},{"location":"stories/transformers/#references","title":"References","text":"<ul> <li>Attention Is All You Need Wikipedia Article</li> <li>Transformer: A Novel Neural Network Architecture for Language Understanding</li> <li>Exclusive: the most-cited papers of the twenty-first century</li> <li>Meet the $4 Billion AI Superstars That Google Lost</li> </ul>"},{"location":"stories/yann-lecun/","title":"The Visionary who Saw the Future: Yann LeCun's Quest for Intelligent Machines","text":"Show Image Prompt Cover Image: Create a vibrant, tech-optimistic wide-landscape graphic novel cover featuring Yann LeCun as the central figure, depicted in a heroic pose with his arms outstretched toward twin horizons representing past and future. The left horizon shows early computing (1970s-1980s) with vintage computers, handwritten digits, and primitive neural network diagrams, rendered in warm sepia tones. The right horizon explodes with modern AI technology - glowing neural networks, holographic data streams, and interconnected global nodes, rendered in brilliant cyans, magentas, and electric blues.  Behind LeCun, create a dramatic split background where the upper half shows the physical world with realistic buildings, labs, and libraries, while the lower half reveals the digital realm of mathematical concepts materialized as floating equations, converging networks, and architectural visualizations of CNNs. These two realms should blend seamlessly through LeCun's silhouette, with light streaming from his head and heart to represent the flow of ideas.  Above the scene, incorporate three distinct layers: the bottom layer shows early perceptrons and simple neural networks, the middle layer displays complex CNN architectures, and the top layer hints at future world models with intricate cause-and-effect relationships. Use flowing energy lines and glowing nodes to connect these layers, suggesting evolution and continuity. The title 'THE VISIONARY WHO SAW THE FUTURE: YANN LECUN'S QUEST FOR INTELLIGENT MACHINES' should be integrated organically into the design, perhaps formed by neural pathways or data streams. Use a bold, modern typeface that complements the tech-forward aesthetic.  The overall color palette should be bright and optimistic, featuring deep blues for computational elements, warm golds for human insight and eureka moments, vibrant greens for growth and innovation, and luminous whites for breakthrough moments. The style should blend technical accuracy with artistic interpretation, making complex AI concepts visually accessible and inspiring.  Include subtle Easter eggs like the iconic handwritten '5' and '7' digits from his early work, equations floating in the background, and miniature depictions of key moments from his career. The composition should draw the reader's eye in a circular motion, starting from LeCun's determined expression, sweeping through the technological landscape, and returning to his visionary gaze toward the future.  The entire image must be rendered in a wide-landscape format suitable for a graphic novel cover, with dramatic lighting that emphasizes the contrast between challenges overcome and possibilities ahead.  Show Narrative Prompt Please create a detailed fun and entertaining story for high-school students about the AI researcher Yann LeCun.  Generate a narrative text but also create detailed descriptions of graph-novel panels that can be inserted into the story at appropriate places.  The theme of the story is how LeCun challenged the status quo and has overcome obstacles in his career by combining both a deep technical understanding and leadership skills.  Tell how in 1988, LeCun joined the Adaptive Systems Research Department at AT&amp;T Bell Laboratories in Holmdel, New Jersey and worked on machine learning.  Give a detailed background of how his career started doing optical character recognition and computer vision using convolutional neural networks (CNNs).  Tell how in 2012, he became the founding director of the NYU Center for Data Science, before the term \"Data Science\" was popular.  Tell how it 2013, he and Yoshua Bengio co-founded the International Conference on Learning Representations to promote good research in the field of machine learning.  Tell how in March 2019, LeCun won the 2018 Turing Award, sharing it with Yoshua Bengio and Geoffrey Hinton.  Tell how LeCun is extremely skeptical that LLMs alone will ever achieve levels of artificial general intelligence (AGI).  Discuss how LeCun feels that precise world models must be used to achieve AGI.  Our goal is to have you generate the full text of the story, but to turn the story into a graphic novel with many illustrations that explains the arc of  LeCun career.   When appropriate, suggest an image that could be inserted into the story to make the story a graphic novel.   Describe each image in detail and be consistent across all the images in the story for style. When you describe an image, make sure to mention that it should be a colorful, bright wide-landscape drawing suitable for technology-forward optimistic graphic-novel."},{"location":"stories/yann-lecun/#prologue-the-dreamer","title":"Prologue: The Dreamer","text":"Show Image Prompt Image 1: A colorful, bright wide-landscape drawing showing a young Yann LeCun as a teenager in France, sitting in a library surrounded by dusty books on mathematics and computing. The scene should be split between the physical reality showing him studying analog electronics and computing manuals, and a dreamy overlay showing his imagination of neural networks as glowing interconnected nodes floating above his head. The style should be vibrant and optimistic, with warm lighting streaming through library windows.  <p>In the early 1970s, while most teenagers dreamed of rock concerts and fast cars, young Yann LeCun spent his evenings in the local library in suburban Paris, devouring books on mathematics, computer science, and the emerging field of artificial intelligence. His friends thought he was eccentric \u2013 who dreams of teaching machines to think? But Yann's vision was clear: machines could learn like humans do, through observation and experience.</p>"},{"location":"stories/yann-lecun/#chapter-1-bell-labs-the-spark-of-innovation","title":"Chapter 1: Bell Labs: The Spark of Innovation","text":"Show Image Prompt Please create a new version of [this image](./holmdell-1980.jpeg) using a drawing that might appear in a colorful bright graphic novel.   <p>In 1988, LeCun joined the legendary AT&amp;T Bell Laboratories in Holmdel New Jersey were over 6,000 engineers and researchers did some of the most groundbreaking research in computer science.</p> <p></p> Show Image Prompt Image 2: Please generate a new drawing.  It is a wide-landscape drawing that might appear in a graphic novel.  A tech-optimistic graphic novel panel showing LeCun as a young researcher at AT&amp;T Bell Labs in 1988, working late at night surrounded by early computers and neural network diagrams on whiteboards. The scene should depict him animated and excited, gesturing toward holographic representations of digits floating above a primitive neural network diagram. The setting should have the warm orange glow of desk lamps and the blue light of computer monitors, creating an atmosphere of discovery.  <p>Holmdel was a place where innovation flowed like electricity through the halls. While others worked on traditional software, LeCun's office became a laboratory of the future. His colleagues often found him staring intently at a screen filled with handwritten digits, muttering about \"convolutional filters\" and \"local receptive fields.\" </p> <p>\"Why are you trying to reinvent image recognition?\" his skeptical colleagues would ask. \"Rule-based systems work just fine for reading zip codes.\"</p> <p>But LeCun had a vision that others couldn't yet see.</p>"},{"location":"stories/yann-lecun/#chapter-2-the-convolutional-revolution","title":"Chapter 2: The Convolutional Revolution","text":"Show Image Prompt Image 3:  Please generate a new drawing.  It is a wide-landscape drawing that might appear in a graphic novel.  A dynamic wide-landscape illustration showing the inner workings of a CNN processing handwritten digits. The scene should be depicted as an ethereal journey through multiple layers of convolution, showing the transition from raw pixels to abstract features. Use flowing, interconnected neural pathways with bright colors \u2013 blues for initial features, transitioning to greens for edges, yellows for shapes, and reds for final classification. The style should be both technical and artistic, making neural network concepts visually beautiful.  <p>Working late into countless nights, LeCun developed something revolutionary: convolutional neural networks (CNNs). While traditional neural networks struggled with images, treating each pixel independently, LeCun's CNNs understood that images had structure \u2013 edges, corners, shapes that mattered.</p> <p>His breakthrough came when his CNN successfully read handwritten digits on checks. The system learned to recognize numbers the way humans do \u2013 by understanding patterns and structures, not by memorizing every possible variation. AT&amp;T saved millions of dollars with this technology, but LeCun knew this was just the beginning.</p> <p></p> Show Image Prompt Image 4:  Please generate a new drawing.  It is a wide-landscape drawing that might appear in a graphic novel.   A triumphant scene in a bank processing center, split between the mundane reality of workers manually sorting checks and the futuristic overlay of LeCun's CNN system automatically reading and processing thousands of checks with glowing accuracy indicators. The style should contrast the gray, tedious manual work with the vibrant, efficient automated system powered by neural networks."},{"location":"stories/yann-lecun/#chapter-3-the-data-science-pioneer","title":"Chapter 3: The Data Science Pioneer","text":"Show Image Prompt Please generate a new drawing.  It is a wide-landscape drawing that might appear in a graphic novel.   Image 5: A colorful wide-landscape showing the inauguration of the NYU Center for Data Science in 2012. The scene should depict LeCun on stage, but instead of a traditional podium, he's surrounded by floating holographic data visualizations, neural networks, and equations. In the audience, show a mix of excited students and skeptical faculty members, with some thinking \"Data Science? That's not a real field!\" The setting should feel like a tech startup meets academic institution, with modern architecture and vibrant colors.  <p>By 2012, LeCun was ready for his next challenge. When offered a position at NYU, he didn't just join \u2013 he created something entirely new: the Center for Data Science. Many academics scoffed. \"Data Science? That's just statistics with a fancy name!\"</p> <p>But LeCun saw the future clearly. Data was becoming the world's most valuable resource, and the techniques he'd been developing could unlock its secrets. Under his leadership, the center became a magnet for brilliant minds from computer science, statistics, and engineering. Together, they pioneered the field that would soon take the world by storm.</p>"},{"location":"stories/yann-lecun/#chapter-4-building-bridges","title":"Chapter 4: Building Bridges","text":"Show Image Prompt Please generate a new drawing.  It is a wide-landscape drawing that might appear in a graphic novel.   Image 6: A conference room scene showing the first International Conference on Learning Representations (ICLR) in 2013. The panel should show LeCun and Yoshua Bengio planting seeds that grow into a massive tree of knowledge, with branches reaching out to connect researchers worldwide. Each connection should be represented by glowing lines forming a global network of innovation. The style should be aspirational and forward-thinking, with a global map in the background showing research nodes lighting up across continents.  <p>In 2013, LeCun and his longtime collaborator Yoshua Bengio faced another challenge: the field of machine learning was fragmented, with researchers working in isolation. They founded the International Conference on Learning Representations (ICLR), creating a space where ideas could flow freely.</p> <p>\"We need to build bridges, not walls,\" LeCun declared at the first conference. His vision was simple but powerful: share knowledge openly, challenge each other's ideas, and accelerate progress for everyone.</p>"},{"location":"stories/yann-lecun/#chapter-5-the-turing-triumph","title":"Chapter 5: The Turing Triumph","text":"Show Image Prompt Image 7: Please generate a new drawing.  It is a wide-landscape drawing that might appear in a graphic novel.  A spectacular wide-landscape showing the 2019 Turing Award ceremony with LeCun, Bengio, and Hinton standing together. Instead of just receiving medals, they should be depicted as knights of the digital realm, with data streams and neural networks forming crowns above their heads. The scene should be celebratory with confetti, but also symbolic \u2013 showing how their work has literally reshaped the world, with elements of their innovations (CNNs, deep learning, AI applications) transforming the environment around them.  <p>March 2019 brought the ultimate recognition: the Turing Award, computing's highest honor. LeCun shared this prestigious award with Yoshua Bengio and Geoffrey Hinton \u2013 three pioneers who had persevered through decades of skepticism to revolutionize artificial intelligence.</p> <p>At the ceremony, LeCun spoke not just about past achievements, but about future challenges. \"This is not the end,\" he declared. \"This is just the beginning of our understanding of intelligence.\"</p>"},{"location":"stories/yann-lecun/#chapter-6-the-agi-skeptic","title":"Chapter 6: The AGI Skeptic","text":"Show Image Prompt Image 8: Please generate a new drawing.  It is a wide-landscape drawing that might appear in a graphic novel.   A thought-provoking wide-landscape showing LeCun in his office, surrounded by whiteboards filled with diagrams. On one side, show simplified representations of LLMs (depicted as pattern-matching machines), and on the other side, show his vision of world models as complex, interconnected systems. The scene should include holographic projections of physical simulations, cause-and-effect relationships, and reasoning engines. The contrast should be clear but non-judgmental, emphasizing the complementary nature of different AI approaches.  <p>While the world celebrated the rise of large language models (LLMs) like ChatGPT, LeCun remained thoughtfully skeptical. \"Language models are impressive, but they're not enough for true intelligence,\" he would explain to packed auditoriums.</p> <p>His vision went deeper: machines needed to understand the world, not just predict text. They needed world models \u2013 internal representations of how things work, what causes what, and how to reason about unseen situations. This wasn't criticism for its own sake; this was the perspective of someone who had seen the field evolve and knew there were greater heights to reach.</p> <p></p> Show Image Prompt Image 9:  Please generate a new drawing.  It is a wide-landscape drawing that might appear in a graphic novel.  A visionary scene showing LeCun's concept of future AI systems. The panel should illustrate a split reality: above, current AI systems performing specific tasks (language, image recognition, etc.), and below, an integrated AI system with a comprehensive world model. This lower level should be depicted as a sophisticated machine understanding the physical world, social dynamics, causal relationships, and abstract reasoning \u2013 all connected in a harmonious, glowing network of understanding.  <p> Yann's vision is an AI system that mimics regions of the human brain.  At the center of this is models of the real world.  LLMs are good models of language, but they are not precise models of the world.</p>"},{"location":"stories/yann-lecun/#epilogue-the-continuing-quest","title":"Epilogue: The Continuing Quest","text":"Show Image Prompt Image 10:  Please generate a new drawing.  It is a wide-landscape drawing that might appear in a graphic novel.  A forward-looking wide-landscape that serves as the story's finale. Show LeCun as an elder statesman of AI, but still actively working and mentoring young researchers. The scene should be set in a futuristic lab where his influence is visible everywhere \u2013 from CNNs processing visual data to young scientists working on world models and next-generation AI. The atmosphere should be optimistic and inclusive, showing a diverse group of researchers continuing his legacy of challenging assumptions and pushing boundaries.  <p>Today, Yann LeCun continues his quest for truly intelligent machines. His journey from a curious teenager in Paris to a pioneering force in artificial intelligence shows that the greatest obstacles to progress are often our own preconceptions. By combining deep technical understanding with visionary leadership, he didn't just overcome challenges \u2013 he redefined what was possible.</p> <p>As he likes to say, \"The best way to predict the future is to create it.\" And for Yann LeCun, the future of artificial intelligence is still being written.</p> <p></p> Show Image Prompt Image 11:  Please generate a new drawing.  It is a wide-landscape drawing that might appear in a graphic novel.  A symbolic closing panel showing hands passing a torch between generations of AI researchers. In the background, the evolution of neural networks can be seen as a spiral timeline, from simple perceptrons to modern CNNs to future world models. The torch itself should be rendered as a glowing neural network, symbolizing the continuous flow of knowledge and innovation. The style should be hopeful and inspiring, suggesting that the story continues with each new generation of researchers."},{"location":"stories/yann-lecun/#references","title":"References","text":"<p>Here are the references reformatted:</p> <ol> <li>Yann LeCun Personal Website</li> <li>Gradient-based learning applied to document recognition - 1998 - Proceedings of the IEEE - LeCun's seminal CNN paper describing implementation at AT&amp;T Bell Labs</li> <li>MNIST Database of Handwritten Digits - 2010 (Updated) - Online Database - Original dataset created from Bell Labs work on postal code recognition</li> <li>Handwritten Digit Recognition with a Back-Propagation Network - 1990 - Advances in Neural Information Processing Systems - Early AT&amp;T Bell Labs application work</li> <li>ACM Turing Award Winners - 2019 - ACM Official Page - Official announcement of 2018 Turing Award to LeCun, Bengio, and Hinton</li> <li>NYU Center for Data Science Launch - 2012 - NYU Official Page - Founding of center with LeCun as inaugural director</li> <li>ICLR Conference History - 2013 - ICLR Official Website - Co-founded by LeCun and Bengio as premier ML conference</li> <li>LeCun's NYU Profile - Current - NYU CS Department - Official position at NYU and CV</li> <li>LeCun on AI and World Models - 2022 - MIT Technology Review - Recent interview on LLM limitations and AGI</li> <li>Facebook AI Research Appointment - 2013 - Meta Research Blog - Announcement of LeCun as FAIR director at Meta</li> <li>LeCun at Bell Labs Research - 1988-2003 - Bell Labs Archives - Documentation of LeCun's tenure and achievements at AT&amp;T Bell Labs</li> </ol>"}]}